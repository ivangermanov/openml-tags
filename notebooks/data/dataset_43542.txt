Do We Really Need Another Dataset of Books?

My initial plan was to build a toy example for a recommender system article I was writing. After conducting a thorough search online, I found a few datasets. Unfortunately, most of them had some issues that made them unusable for me, such as missing descriptions of the books, a mix of different languages but no column to specify the language per row, or weird delimiters. Therefore, I decided to create a dataset that would match my purposes.

First, I obtained International Standard Book Numbers (ISBNs) from the Goodreads-books dataset. Using those identifiers, I crawled the Google Books Application Programming Interface (API) to extract the books' information. Then, I merged those results with some of the original columns from the dataset and, after some cleaning, I got the dataset you see here.

This dataset can be used for various purposes, including different exploratory data analysis, clustering of books by topics or categories, and content-based recommendation engines using different fields from the book's description.

Why is this dataset smaller than the Goodreads-books dataset? Many of the ISBNs from that dataset did not return valid results from the Google Books API. I plan to update this in the future, using more fields, such as title and author, in the API requests, to have a bigger dataset.

To build this dataset, I used a Google Books crawler, which is available in a repository.

This dataset relied heavily on the Goodreads-books dataset.

The dataset features include book title, subtitle, authors, categories, thumbnail, description, publication year, average rating, number of pages, and ratings count.

Keywords: book dataset, recommender system, Google Books API, data analysis, clustering, content-based recommendation.