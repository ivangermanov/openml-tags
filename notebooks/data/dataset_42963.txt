The Sgemm Gpu Kernel Performance Dataset 

This dataset measures the running time of a matrix-matrix product A multiplied by B equals C, where all matrices have a size of two thousand forty-eight by two thousand forty-eight, using a parameterizable Sgemm Gpu kernel with a vast number of possible parameter combinations. For each tested combination, four runs were performed and their results are reported as the four last columns. All times are measured in milliseconds. There are fourteen parameters, the first ten being ordinal and can only take up to four different powers of two values, and the four last variables being binary. Out of a total of one million three hundred twenty-seven thousand one hundred four parameter combinations, only two hundred forty-one thousand six hundred are feasible due to various kernel constraints. This dataset contains the results for all these feasible combinations.

The experiment was run on a desktop workstation running Ubuntu Linux with an Intel Core processor, sixteen gigabytes of Ram, and an Nvidia Geforce graphics card. The gemm fast kernel from the automatic Opencl kernel tuning library CLTune was used.

Note that for this kind of datasets, it is usually better to work with the logarithm of the running times, as seen in similar research studies.

Independent Variables 

Per-matrix Two-Dimensional Tiling at Workgroup Level 
Inner Dimension of Two-Dimensional Tiling at Workgroup Level 
Local Workgroup Size 
Local Memory Shape 
Kernel Loop Unrolling Factor 
Per-Matrix Vector Widths for Loading and Storing 
Enable Stride for Accessing Off-Chip Memory Within a Single Thread 
Per-Matrix Manual Caching of the Two-Dimensional Workgroup Tile 

Output 

Performance Times in Milliseconds for Four Independent Runs Using the Same Parameters 

Keywords: gpu kernel, matrix multiplication, performance optimization, opencl, kernel tuning