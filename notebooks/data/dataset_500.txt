The Friedman Datasets are eighty artificially generated datasets originating from stochastic gradient boosting. The dataset names are coded according to their characteristics, specifically the degree of colinearity and the number of samples and features.

These datasets were designed to test the performance of algorithms in handling complex relationships between variables, including both linear and nonlinear relationships. A normalized noise is added to the output to simulate real-world scenarios.

The Friedman function, which generates these datasets, includes five primary features that influence the output, as well as additional features that are unrelated to the output. The function is formulated as follows: output equals ten times the sine of pi times the product of the first two features, plus twenty times the square of the third feature minus point five, plus ten times the fourth feature, plus five times the fifth feature, plus a normalized noise term.

To examine the impact of irrelevant features, additional features are added to the datasets. These extraneous features are independent of the output. Furthermore, to assess an algorithm's robustness to colinearity, the datasets are generated with varying degrees of colinearity, ranging from zero to four. Colinearity refers to the number of features that depend on other features.

The generated Friedman datasets vary in terms of their parameters, specifically the number of features, samples, and colinearity degree. The number of features ranges from five to one hundred, although only the first five features are related to the output. The remaining features are randomly generated and do not influence the output. The number of samples varies from one hundred to one thousand. The colinearity degree, which measures the interdependence of features, ranges from zero to four.

As a result, eighty unique artificial datasets are generated by combining different feature numbers, sample sizes, and colinearity degrees.

Keywords: Friedman Datasets, stochastic gradient boosting, colinearity, nonlinear relationships, feature selection, algorithm performance.