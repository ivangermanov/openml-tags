{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438bd6927ac98607",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploratory Data Analysis (EDA) of OpenML Datasets\n",
    "\n",
    "This notebook presents an exploratory data analysis of datasets from OpenML. We will examine dataset descriptions, analyze word and sentence lengths, perform named entity recognition, POS tagging, and explore text similarity among dataset descriptions. We will also investigate the complexity of dataset descriptions, the distribution of tags, and the number of features in datasets. Finally, we will analyze the presence of URLs in dataset descriptions and explore the domains of these URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:39:12.295215Z",
     "start_time": "2024-04-15T15:39:06.855021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openml\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import seaborn as sns\n",
    "import textstat\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "from urlextract import URLExtract\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Enable GPU support for spaCy\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# spacy must be imported after setting the environment variable\n",
    "import spacy\n",
    "\n",
    "activated = spacy.prefer_gpu()\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except OSError:\n",
    "    !python -m spacy download en_core_web_trf\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Disable truncation in pandas display of columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Disable truncation in pandas display of rows \n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f571867197afc73",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Retrieval\n",
    "Retrieving and listing all datasets from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785019db5b567c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:39:28.411392Z",
     "start_time": "2024-04-15T15:39:12.295959Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = openml.datasets.list_datasets()\n",
    "print(openml.datasets.list_datasets()[531])\n",
    "\n",
    "ids = list(datasets.keys())\n",
    "print(\"Number of datasets: \", len(ids))\n",
    "\n",
    "# Removing non-dataset entries \n",
    "ids.remove(4537)\n",
    "ids.remove(4546)\n",
    "ids.remove(4562)\n",
    "\n",
    "# Fetching dataset\n",
    "datasets = openml.datasets.get_datasets(ids, download_data=False, download_qualities=False)\n",
    "datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f72f4920f0eb7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initial Dataframe Creation\n",
    "Creation of an initial dataframe for analysis, including URLs and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed9131b9c5bf59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:39:28.466288Z",
     "start_time": "2024-04-15T15:39:28.412315Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([dataset.__dict__ for dataset in datasets])\n",
    "# Add the OpenML URL to the dataframe\n",
    "df['openml_url'] = [dataset.openml_url for dataset in datasets]\n",
    "# All datasets which have None description, make it an empty string \"\n",
    "df['description'] = df['description'].fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b01d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:39:28.468655Z",
     "start_time": "2024-04-15T15:39:28.466874Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the project root directory (go up one level from 'notebooks')\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac4d697f14a3dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T17:20:02.806863Z",
     "start_time": "2024-04-15T17:19:55.911656Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "from src import tag_augmenter, feature_augmenter, dataset_augmenter, similarity_augmenter, name_augmenter, \\\n",
    "    scrapy_augmenter\n",
    "\n",
    "importlib.reload(tag_augmenter)\n",
    "importlib.reload(feature_augmenter)\n",
    "importlib.reload(dataset_augmenter)\n",
    "importlib.reload(similarity_augmenter)\n",
    "importlib.reload(name_augmenter)\n",
    "importlib.reload(scrapy_augmenter)\n",
    "\n",
    "from src import TagAugmenter, FeatureAugmenter, DatasetAugmenter, SimilarityAugmenter, NameAugmenter, ScrapyAugmenter\n",
    "\n",
    "description_column = 'description'\n",
    "tag_column = 'tag'\n",
    "features_column = '_features'\n",
    "name_column = 'name'\n",
    "augmented_column = 'augmented_description'\n",
    "similar_datasets_column = 'similar_datasets'\n",
    "scraped_column = 'scraped_data'\n",
    "\n",
    "augmenters = [\n",
    "    TagAugmenter(description_column, tag_column, augmented_column),\n",
    "    NameAugmenter(description_column, name_column, augmented_column),\n",
    "    FeatureAugmenter(description_column, features_column, augmented_column, max_features=100, reduce_features=True),\n",
    "    ScrapyAugmenter(description_column, scraped_column, augmented_column),\n",
    "    SimilarityAugmenter(description_column, augmented_column, similar_datasets_column)\n",
    "]\n",
    "\n",
    "dataset_augmenter = DatasetAugmenter(augmenters=augmenters)\n",
    "augmented_df = dataset_augmenter.augment(df.copy())\n",
    "augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dce97b6e7fb6d78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T17:20:08.873758Z",
     "start_time": "2024-04-15T17:20:08.862993Z"
    }
   },
   "outputs": [],
   "source": [
    "# show augmented_df where scraped_data is not empty string\n",
    "augmented_df[augmented_df[scraped_column] != ''][['dataset_id', 'name', 'original_data_url', 'description', 'scraped_data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1a126bf947d9b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Description vs Augmented Description lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a032d631b17321d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T13:13:40.308046Z",
     "start_time": "2024-04-09T13:13:40.227433Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot length of combined 'description' column vs 'augmented_description' column\n",
    "desc_len = df['description'].map(len)\n",
    "aug_desc_len = augmented_df['augmented_description'].map(len)\n",
    "\n",
    "# plot sns barplot one vs the other\n",
    "sns.barplot(x=['description', 'augmented_description'],\n",
    "            y=[desc_len.mean(), aug_desc_len.mean()],\n",
    "            hue=['description', 'augmented_description'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3935d34e49965972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T13:21:49.878446Z",
     "start_time": "2024-04-09T13:21:49.870259Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the augmented descriptions for the rest of the notebook\n",
    "augmented_df = augmented_df.drop(columns=['description'])\n",
    "augmented_df = augmented_df.rename(columns={'augmented_description': 'description'})\n",
    "df = augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb6611475f4b982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:05.466359Z",
     "start_time": "2024-04-08T21:05:05.440985Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show df where similar datasets are not empty list\n",
    "df[df[similar_datasets_column].map(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d3ff87226286e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Description Length Analysis\n",
    "\n",
    "Analyzing the length of dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1bc8bd54005d94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:05.519713Z",
     "start_time": "2024-04-08T21:05:05.468021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=['empty description', 'non-empty description'],\n",
    "            y=[len(df[df['description'] == '']), len(df[df['description'] != ''])],\n",
    "            hue=['empty description', 'non-empty description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958eb9c019b2ecc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:05.641562Z",
     "start_time": "2024-04-08T21:05:05.520356Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist([len(d) for d in df['description']], bins=100, color='C0')\n",
    "plt.xlabel('Length of description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot([len(d) for d in df['description']], color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba0022233da141e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filtering Long Descriptions\n",
    "\n",
    "Filtering out datasets with unusually long descriptions for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d516328c8fa1aa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:05.662723Z",
     "start_time": "2024-04-08T21:05:05.643896Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['description'].map(len) > 9000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e289d16410ff026",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Word and Sentence Analysis\n",
    "\n",
    "Analyzing the word count and average sentence length in dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4dbb747107fc112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:05.979575Z",
     "start_time": "2024-04-08T21:05:05.663395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word count analysis\n",
    "plt.hist([len(d.split()) for d in df['description']], bins=100, color='C0')\n",
    "plt.xlabel('Word count of description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot([len(d.split()) for d in df['description']], color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325d67c65952b13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T21:05:07.931901Z",
     "start_time": "2024-04-08T21:05:05.980177Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "plt.hist([len(tokenize.sent_tokenize(d)) for d in df['description']], bins=100, color='C0')\n",
    "plt.xlabel('Sentence count of description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot([len(tokenize.sent_tokenize(d)) for d in df['description']], color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d2cc2f84ae6ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mean Word Length Calculation\n",
    "\n",
    "Computing the mean word length in dataset descriptions using spaCy for more accurate tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb11461d2f15b4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T13:20:16.720356Z",
     "start_time": "2024-04-09T13:20:13.369318Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to calculate mean word length\n",
    "def mean_word_length(description):\n",
    "    doc = nlp(description)\n",
    "    words = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    if words:  # Check if there are words to avoid ZeroDivisionError\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "mean_word_lengths = df['description'].apply(mean_word_length)\n",
    "print(\"Mean word lengths mean and median: \", mean_word_lengths.mean(), mean_word_lengths.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add2c5c12422da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Average word length in English texts is 5.1 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790ed9617fa50ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualization of Mean Word Lengths\n",
    "\n",
    "Displaying the distribution of mean word lengths across dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b147e00c8c83599b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mean_word_lengths, bins=100, color='C0', range=(0, 12))\n",
    "plt.xlabel('Mean word length of description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbcf7d282c19b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Longest Word in Descriptions\n",
    "\n",
    "Identifying the longest words within dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8be4cb8f06ee4f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def longest_word(description):\n",
    "    doc = nlp(description)\n",
    "    words = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    if words:  # Check if there are words to avoid ZeroDivisionError\n",
    "        return max(words, key=len)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "longest_words = df['description'].apply(longest_word)\n",
    "longest_words.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349eeaa605f633d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Term Frequency Analysis\n",
    "\n",
    "Performing term frequency analysis to identify the most common words and phrases in dataset descriptions, excluding common stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c55a317161af131c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def clean(text, remove_stopwords=True, remove_numbers=True):\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-z A-Z 0-9-]+', '', text)\n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = ' '.join([word for word in word_tokenize(text) if word not in stopwords.words('english')])\n",
    "    # Remove numbers too\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19b54306566e35e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Term frequency analysis for single words\n",
    "mostCommon = Counter(word_tokenize(\" \".join(df['description'].map(clean)))).most_common(15)\n",
    "words = [word[0] for word in mostCommon]\n",
    "frequency = [word[1] for word in mostCommon]\n",
    "\n",
    "sns.barplot(x=frequency, y=words, hue=words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baeef0d34f35cde",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Topics Bar Chart\n",
    "From before. Not dynamic\n",
    "![Topics Bar Chart](../latex/proposal/images/topics_barchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e8ad1fbb31d0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bigram Frequency Analysis\n",
    "\n",
    "Analyzing the frequency of bigrams (pairs of words) in dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff5d46287da8b03e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "bigrams = cv.fit_transform(df['description'].map(clean))\n",
    "\n",
    "count_values = bigrams.toarray().sum(axis=0)\n",
    "bigram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse=True))\n",
    "bigram_freq.columns = [\"frequency\", \"bigram\"]\n",
    "\n",
    "sns.barplot(x=bigram_freq['frequency'][:15], y=bigram_freq['bigram'][:15], hue=bigram_freq['bigram'][:15])\n",
    "plt.title('Top 10 Most Frequently Occuring Bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bec702e6c00a15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Datasets Containing Specific Keywords\n",
    "\n",
    "Filtering datasets that contain specific keywords such as 'TCP' within their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ea15c3ff5a7501a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['description'].map(lambda x: 'tcp' in x)].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc180dcddcdbb9cf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trigram Frequency Analysis\n",
    "\n",
    "Similar to bigram analysis, examining the frequency of trigrams (three-word sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b29bbfd7b042cf2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "trigrams = cv.fit_transform(df['description'].map(clean))\n",
    "\n",
    "count_values = trigrams.toarray().sum(axis=0)\n",
    "trigram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse=True))\n",
    "trigram_freq.columns = [\"frequency\", \"trigram\"]\n",
    "\n",
    "sns.barplot(x=trigram_freq['frequency'][:15], y=trigram_freq['trigram'][:15], hue=trigram_freq['trigram'][:15])\n",
    "plt.title('Top 10 Most Frequently Occuring Trigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5c4290ede52fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exploring Original Data URLs\n",
    "\n",
    "Investigating the presence and domains of original data URLs in the dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82e33e2eeac7ee8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Counting datasets with original data URLs\n",
    "len(df[df['original_data_url'].notnull()])\n",
    "\n",
    "# Visualization of datasets with and without original data URLs\n",
    "sns.barplot(x=['original_data_url', 'no original_data_url'],\n",
    "            y=[len(df[df['original_data_url'].notnull()]), len(df[df['original_data_url'].isnull()])],\n",
    "            hue=['original_data_url', 'no original_data_url'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9833b57b8372",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Domain Analysis of URLs\n",
    "\n",
    "Analyzing the domains of URLs found in the descriptions and visualizing the top domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f54343e10d1fed86",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domains = (df['original_data_url'].dropna().map(lambda x: urlparse(x).netloc))\n",
    "\n",
    "k = 15\n",
    "sns.barplot(x=domains.value_counts().head(k), y=domains.value_counts().head(k).index,\n",
    "            hue=domains.value_counts().head(k).index)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee9baabea4e492",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filtering Datasets by Domain\n",
    "\n",
    "Filtering datasets with URLs from specific domains such as 'ebi.ac.uk' and 'github'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3363c1576625d401",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'ebi.ac.uk' in x if x else False)][\n",
    "    ['name', 'original_data_url', 'openml_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92617ee5430b8964",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'lib.stat.cmu' in x if x else False)][\n",
    "    ['name', 'original_data_url', 'openml_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "300c479decf8928c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'github' in x if x else False)][['name', 'original_data_url', 'openml_url']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "304566727ee442c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'bnlearn' in x if x else False)][['name', 'original_data_url', 'openml_url']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15ff6f559b260db7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'archive.ics' in x if x else False)][\n",
    "    ['name', 'original_data_url', 'openml_url']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4b607c0b288af02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'kaggle' in x if x else False)][['name', 'original_data_url', 'openml_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5ed67ada6cd319",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'stern' in x if x else False)][['name', 'original_data_url', 'openml_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b672c0ed6f538154",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['original_data_url'].map(lambda x: 'csie' in x if x else False)][['name', 'original_data_url', 'openml_url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5c0e311313038",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA) for Topic Modeling\n",
    "\n",
    "Using LDA to model topics in dataset descriptions and visualizing with pyLDAvis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dea3ed981995caa2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    corpus = []\n",
    "    lem = WordNetLemmatizer()\n",
    "    for description in df['description']:\n",
    "        description = description.lower()\n",
    "        words = [w for w in word_tokenize(description) if (w not in stopwords.words('english'))]\n",
    "        words = [lem.lemmatize(w) for w in words if len(w) > 2]\n",
    "\n",
    "        corpus.append(words)\n",
    "\n",
    "    dic = gensim.corpora.Dictionary(corpus)\n",
    "    bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "    return corpus, dic, bow_corpus\n",
    "\n",
    "\n",
    "corpus, dic, bow_corpus = preprocess(df)\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                       num_topics=50,\n",
    "                                       id2word=dic,\n",
    "                                       passes=10,\n",
    "                                       workers=2)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, bow_corpus, dic)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb25a1fcb4c4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Wordcloud\n",
    "\n",
    "Creating a word cloud to visualize the most common words in dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25ad53d6833e850d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=set(STOPWORDS),\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        collocations=False,\n",
    "        random_state=1)\n",
    "\n",
    "    wordcloud = wordcloud.generate(str(data))\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_wordcloud(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc0faabe51dfd9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Named Entity Recognition (NER) with spaCy\n",
    "\n",
    "Analyzing named entities in dataset descriptions to identify common entities like organizations, locations, dates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b53a333905d74d80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the EntityRecognizer component from the pipeline\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# List the available entity types\n",
    "entity_labels = ner.labels\n",
    "\n",
    "print(entity_labels)\n",
    "\n",
    "# Description for each entity type\n",
    "for entity in entity_labels:\n",
    "    print(f\"{entity}: {spacy.explain(entity)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87c2484dbbbf20",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualization of Named Entities\n",
    "\n",
    "Visualizing named entities in a sample dataset description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f40a5b221c139d1d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example visualization of entities in the description of the 500th dataset\n",
    "doc = nlp(df['description'][500])\n",
    "\n",
    "# Print detected entities\n",
    "print(doc.ents)\n",
    "\n",
    "# Visualize the entities\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb820d5d2df3a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Named Entity Frequency Analysis\n",
    "\n",
    "Counting the frequency of each entity type across all dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f97d794fd276d2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform NER on all descriptions\n",
    "docs = [nlp(description) for description in tqdm(df['description'])]\n",
    "\n",
    "# Count all named entities\n",
    "entity_freq_counter = Counter([X.label_ for doc in docs for X in doc.ents])\n",
    "entity_freq = entity_freq_counter.most_common()\n",
    "\n",
    "# Plotting entity frequencies using Plotly\n",
    "labels, values = zip(*entity_freq[::-1])  # Reverse for descending order in plot\n",
    "entity_explanations = [spacy.explain(label) for label in labels]\n",
    "\n",
    "# Generate the Plotly bar chart\n",
    "fig = px.bar(x=values, y=labels, orientation='h', labels={'x': 'Count', 'y': 'Entity'},\n",
    "             hover_data={'Entity': labels, 'Count': values, 'Description': entity_explanations})\n",
    "fig.update_traces(hovertemplate='<b>Entity:</b> %{y}<br><b>Count:</b> %{x}<br><b>Description:</b> %{customdata[2]}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621a54394c9fe20",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exploration of Most Common Entities\n",
    "\n",
    "Investigating the most common entities for each entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc0f754dc69b3655",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to get the most common entities for a given label\n",
    "def get_entities(label, most_common=10):\n",
    "    entities = [X.text for doc in docs for X in doc.ents if X.label_ == label]\n",
    "    return Counter(entities).most_common(most_common)\n",
    "\n",
    "\n",
    "# Function to plot the most common entities for a given label\n",
    "def plot_entities(label, k=15):\n",
    "    entities = get_entities(label, k)\n",
    "    entity_labels, entity_counts = zip(*entities)\n",
    "    sns.barplot(x=list(entity_counts), y=list(entity_labels))\n",
    "    plt.title(f'Top {k} most common entities for {label}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the most common entities for each label\n",
    "for label in entity_labels:\n",
    "    plot_entities(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb64f55b58c2d76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part of Speech (POS) Tagging with spaCy\n",
    "\n",
    "Analyzing the parts of speech in dataset descriptions to understand their grammatical composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff54cfa7f5913015",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example visualization of POS tags in the description of the 500th dataset \n",
    "doc = nlp(df['description'][500][:400])\n",
    "\n",
    "options = {'distance': 75, 'compact': True, 'color': 'black', 'bg': '#f9e79f'}\n",
    "\n",
    "# Visualize the entities\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "643dee5e1388950e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example POS tagging with explanations\n",
    "for token in nlp('Can you give me the can, please?'):\n",
    "    print(token.text, token.pos_, token.tag_, spacy.explain(token.tag_))\n",
    "\n",
    "# Aggregate POS tagging for all descriptions\n",
    "pos_freq_counter = Counter([X.pos_ for doc in docs for X in doc])\n",
    "pos_freq = pos_freq_counter.most_common()\n",
    "\n",
    "# Plotting POS frequencies using Plotly\n",
    "labels, values = zip(*pos_freq[::-1])  # Reverse for descending order in plot\n",
    "pos_explanations = [spacy.explain(label) for label in labels]\n",
    "\n",
    "# Generate the Plotly bar chart\n",
    "fig = px.bar(x=values, y=labels, orientation='h', labels={'x': 'Count', 'y': 'POS'},\n",
    "             hover_data={'POS': labels, 'Count': values, 'Description': pos_explanations})\n",
    "fig.update_traces(hovertemplate='<b>POS:</b> %{y}<br><b>Count:</b> %{x}<br><b>Description:</b> %{customdata[2]}')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57661d92ecd7caf9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assessing Dataset Description Complexity\n",
    "\n",
    "Evaluating the readability of dataset descriptions using the Flesch reading ease score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbe0c793003ba6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Readability Score Interpretation\n",
    "\n",
    "The Flesch reading ease score is interpreted as follows:\n",
    "- 90-100: Very easy to read, understood by an average 11-year-old student\n",
    "- 80-90: Easy to read, conversational English for consumers\n",
    "- 70-80: Fairly easy to read\n",
    "- 60-70: Plain English, easily understood by 13- to 15-year-old students\n",
    "- 50-60: Fairly difficult to read\n",
    "- 30-50: Difficult to read\n",
    "- 0-30: Very difficult to read, best understood by university graduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "635bb3dba68a7684",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the Flesch reading ease test to each description, filter the ones with score below 0\n",
    "stat = df['description'].apply(\n",
    "    lambda x: textstat.textstat.flesch_reading_ease(x) if 0 < textstat.textstat.flesch_reading_ease(x) <= 100 else None)\n",
    "stat.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb48e26d387db2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exploratory Data Analysis on Tags\n",
    "\n",
    "We perform EDA on the 'tag' feature of datasets to better understand their categorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bafe86af985f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tag Presence Analysis\n",
    "\n",
    "Comparing the number of datasets with and without tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d456495e54f4f14a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count datasets with and without tags\n",
    "datasets_with_tags = df[df['tag'].notnull()]\n",
    "datasets_without_tags = df[df['tag'].isnull()]\n",
    "\n",
    "sns.barplot(x=['datasets with tags', 'datasets without tags'],\n",
    "            y=[len(datasets_with_tags), len(datasets_without_tags)],\n",
    "            hue=['datasets with tags', 'datasets without tags'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6fa5339270a472",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tag Distribution Analysis\n",
    "\n",
    "Analyzing the distribution of tags across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9ee01707f258af9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Average number of tags per dataset: {df['tag'].map(lambda x: len(x) if x else 0).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e04f79cac48c67a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df['tag'].map(lambda x: len(x) if x else 0), bins=np.arange(0, 10) - 0.5)\n",
    "plt.xlabel('Number of Tags')\n",
    "plt.ylabel('Number of Datasets')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22348c1ba868b7f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Most Common Tags\n",
    "\n",
    "Identifying the most common tags across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "812cdcb856819265",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = df['tag'].dropna().map(lambda x: x if isinstance(x, list) else [x])\n",
    "tags = [tag for sublist in tags for tag in sublist]\n",
    "tag_freq = Counter(tags)\n",
    "tag_freq = pd.DataFrame(sorted([(count, tag) for tag, count in tag_freq.items()], reverse=True))\n",
    "tag_freq.columns = ['frequency', 'tag']\n",
    "\n",
    "k = 30\n",
    "sns.barplot(x=tag_freq['frequency'][:k], y=tag_freq['tag'][:k], hue=tag_freq['tag'][:k])\n",
    "plt.title(f'Top {k} most common tags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7235823bc7df8a",
   "metadata": {},
   "source": [
    "## Dataset Tags Analysis\n",
    "\n",
    "Exploring the tags associated with the datasets to understand their categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324410439c1f9334",
   "metadata": {},
   "source": [
    "### POS Tagging and NER on Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8e5749a2e84b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6ac5cadeea8d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tags into one list and then into a text string for processing\n",
    "tags = df['tag'].dropna().tolist()\n",
    "tags_combined = ' '.join([' '.join(tag_list) for tag_list in tags if isinstance(tag_list, list)])\n",
    "\n",
    "doc_tags = nlp(tags_combined)\n",
    "\n",
    "# POS Tagging\n",
    "pos_counts = Counter([token.pos_ for token in doc_tags])\n",
    "\n",
    "# NER\n",
    "ner_counts = Counter([ent.label_ for ent in doc_tags.ents])\n",
    "\n",
    "# Convert the counts to DataFrames for easy plotting\n",
    "df_pos_counts = pd.DataFrame(pos_counts.items(), columns=['POS', 'Frequency']).sort_values(by='Frequency',\n",
    "                                                                                           ascending=True)\n",
    "df_ner_counts = pd.DataFrame(ner_counts.items(), columns=['Entity', 'Frequency']).sort_values(by='Frequency',\n",
    "                                                                                              ascending=True)\n",
    "\n",
    "# Plotting POS tags using Plotly\n",
    "fig_pos = px.bar(df_pos_counts, x='Frequency', y='POS', orientation='h', title='Frequency of POS Tags in Dataset Tags')\n",
    "fig_pos.update_layout(xaxis_title='Frequency', yaxis_title='POS')\n",
    "fig_pos.show()\n",
    "\n",
    "# Plotting NER tags using Plotly\n",
    "fig_ner = px.bar(df_ner_counts, x='Frequency', y='Entity', orientation='h',\n",
    "                 title='Frequency of Named Entities in Dataset Tags')\n",
    "fig_ner.update_layout(xaxis_title='Frequency', yaxis_title='Entity')\n",
    "fig_ner.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e107268ccab825",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text Similarity Analysis\n",
    "\n",
    "We analyze the similarity between dataset descriptions using TF-IDF and cosine similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fcfec402214e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cosine Similarity Matrix\n",
    "\n",
    "Generating a cosine similarity matrix for the dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a958dd2d6002cdbc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the descriptions\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['description'].map(clean))\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(X, X)\n",
    "\n",
    "# Plot the similarity matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cosine_sim, cmap='coolwarm', square=True, xticklabels=False, yticklabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f246123ece8f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Distribution of Cosine Similarities\n",
    "\n",
    "Examining the distribution of similarity scores between different dataset descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fafb3161584f52e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the upper triangle of the matrix, excluding the diagonal\n",
    "upper_tri_idx = np.triu_indices_from(cosine_sim, k=1)\n",
    "unique_similarities = cosine_sim[upper_tri_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(unique_similarities, bins=100)\n",
    "plt.title('Distribution of Cosine Similarities')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94274be123521f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Similar Dataset Pairs\n",
    "\n",
    "Filtering dataset pairs with a high level of similarity (cosine similarity > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98d15fb764f7010b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=['below 0.9', 'above 0.9'],\n",
    "            y=[len(np.where(np.triu(cosine_sim, k=1) < 0.9)[0]), len(np.where(np.triu(cosine_sim, k=1) > 0.9)[0])],\n",
    "            hue=['below 0.9', 'above 0.9'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bcf2f58af62f0bb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "above_threshold_indices = np.where(np.triu(cosine_sim, k=1) > 0.9)\n",
    "\n",
    "# Extract the dataset pairs based on the indices\n",
    "similar_pairs = [(i, j, cosine_sim[i, j]) for i, j in zip(*above_threshold_indices)]\n",
    "\n",
    "# Create a new DataFrame to store the similar dataset pairs with their similarity score\n",
    "similar_df = pd.DataFrame(similar_pairs, columns=['Dataset_ID_1', 'Dataset_ID_2', 'Similarity'])\n",
    "\n",
    "# Add dataset names, descriptions, openml_url, and version to the DataFrame for easier interpretation\n",
    "similar_df['Dataset_Name_1'] = similar_df['Dataset_ID_1'].apply(lambda x: df.iloc[x]['name'])\n",
    "similar_df['Dataset_Description_1'] = similar_df['Dataset_ID_1'].apply(lambda x: df.iloc[x]['description'])\n",
    "similar_df['OpenML_URL_1'] = similar_df['Dataset_ID_1'].apply(lambda x: df.iloc[x]['openml_url'])\n",
    "similar_df['Version_1'] = similar_df['Dataset_ID_1'].apply(lambda x: df.iloc[x]['version'])\n",
    "\n",
    "similar_df['Dataset_Name_2'] = similar_df['Dataset_ID_2'].apply(lambda x: df.iloc[x]['name'])\n",
    "similar_df['Dataset_Description_2'] = similar_df['Dataset_ID_2'].apply(lambda x: df.iloc[x]['description'])\n",
    "similar_df['OpenML_URL_2'] = similar_df['Dataset_ID_2'].apply(lambda x: df.iloc[x]['openml_url'])\n",
    "similar_df['Version_2'] = similar_df['Dataset_ID_2'].apply(lambda x: df.iloc[x]['version'])\n",
    "\n",
    "similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfae00e8e1f74ba6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show an example of dataset pairs with high similarity \n",
    "similar_df[similar_df['Dataset_ID_1'] == 1287]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda6a7bc6b40794",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Comparing Versions of Datasets\n",
    "\n",
    "Investigating how dataset descriptions vary across different versions of the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19582e0a3bec8aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Analysis of Similarity Between Different Dataset Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60ea0d19d37d12be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter the similar_df for entries with the same name and different version (as before)\n",
    "same_name_different_version = similar_df[\n",
    "    (similar_df['Dataset_Name_1'] == similar_df['Dataset_Name_2']) &\n",
    "    (similar_df['Version_1'] != similar_df['Version_2'])\n",
    "    ]\n",
    "\n",
    "# Filter the similar_df for entries that are not the same dataset (the complement)\n",
    "not_same_dataset = similar_df[\n",
    "    (similar_df['Dataset_Name_1'] != similar_df['Dataset_Name_2'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7a59474d5b0f9e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "same_name_different_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0c87761a6650440",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_same_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d026be3fa32656",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualizing Similarity Distribution for Different Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "878bc1030de5c036",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_to_indices = defaultdict(list)\n",
    "for i, row in df.iterrows():\n",
    "    name_to_indices[row['name']].append(df.index.get_loc(i))\n",
    "\n",
    "version_similarity = []\n",
    "\n",
    "for name, indices in name_to_indices.items():\n",
    "    # Only consider names with more than one version\n",
    "    if len(indices) > 1:\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                idx1, idx2 = indices[i], indices[j]\n",
    "                similarity = cosine_sim[idx1, idx2]\n",
    "                version_similarity.append({\n",
    "                    'Dataset_Name': name,\n",
    "                    'Version_1': df.iloc[idx1]['version'],\n",
    "                    'OpenML_URL_1': df.iloc[idx1]['openml_url'],\n",
    "                    'Version_2': df.iloc[idx2]['version'],\n",
    "                    'OpenML_URL_2': df.iloc[idx2]['openml_url'],\n",
    "                    'Similarity': similarity\n",
    "                })\n",
    "\n",
    "version_similarity_df = pd.DataFrame(version_similarity)\n",
    "\n",
    "version_similarity_sorted = version_similarity_df.sort_values(by=['Dataset_Name', 'Similarity'],\n",
    "                                                              ascending=[True, False])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(version_similarity_sorted['Similarity'], bins=100)\n",
    "\n",
    "plt.title('Distribution of Cosine Similarities for Different Versions of the Same Dataset')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed36a513a18078",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Most Similar Dataset Versions\n",
    "\n",
    "Identifying the most similar versions of datasets based on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd673b15b1edfa1a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Group by the 'name' column and count the number of versions for each dataset\n",
    "version_counts = df.groupby('name').size()\n",
    "\n",
    "# We're interested in datasets that have more than one version\n",
    "multiple_versions = version_counts[version_counts > 1]\n",
    "\n",
    "num_bins = min(multiple_versions.nunique(), 50)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(multiple_versions, bins=np.arange(1, num_bins + 1) - 0.5)\n",
    "\n",
    "plt.title('Distribution of Number of Versions per Dataset')\n",
    "plt.xlabel('Number of Versions')\n",
    "plt.ylabel('Number of Datasets')\n",
    "\n",
    "plt.xticks(range(1, num_bins + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b787e0910b303",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset Features\n",
    "\n",
    "Analyzing the features of the datasets such as the number of features and their types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b2a3f854f92c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### POS Tagging and NER on Features\n",
    "This doesn't make sense to include, as the number of features is very large, and they are not connected in any meaningful sense in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cb0c51162bfa6ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:29.854637Z",
     "start_time": "2024-04-09T13:21:55.743270Z"
    }
   },
   "outputs": [],
   "source": [
    "features = df['_features']\n",
    "\n",
    "# Wrap the outer loop with tqdm for a progress bar\n",
    "docs = [nlp(feature.name) for feature_list in tqdm(features, desc='Processing features') for feature in\n",
    "        feature_list.values()]\n",
    "\n",
    "# Count all named entities\n",
    "overall_pos_counts = Counter([X.pos_ for doc in docs for X in doc])\n",
    "overall_ner_counts = Counter([X.label_ for doc in docs for X in doc.ents])\n",
    "\n",
    "# Convert the overall counts to DataFrames for easy plotting\n",
    "df_pos_counts = pd.DataFrame(overall_pos_counts.items(), columns=['POS', 'Frequency']).sort_values(by='Frequency',\n",
    "                                                                                                   ascending=True)\n",
    "df_ner_counts = pd.DataFrame(overall_ner_counts.items(), columns=['Entity', 'Frequency']).sort_values(by='Frequency',\n",
    "                                                                                                      ascending=True)\n",
    "\n",
    "# Plotting POS tags using Plotly\n",
    "fig_pos = px.bar(df_pos_counts, x='Frequency', y='POS', orientation='h',\n",
    "                 title='Frequency of POS Tags in Dataset Features')\n",
    "fig_pos.update_layout(xaxis_title='Frequency', yaxis_title='POS')\n",
    "fig_pos.show()\n",
    "\n",
    "# Plotting NER using Plotly\n",
    "fig_ner = px.bar(df_ner_counts, x='Frequency', y='Entity', orientation='h',\n",
    "                 title='Frequency of Named Entities in Dataset Features')\n",
    "fig_ner.update_layout(xaxis_title='Frequency', yaxis_title='Entity')\n",
    "fig_ner.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd136ba9eda5762",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Distribution of Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec9f82affafadd2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:29.911128Z",
     "start_time": "2024-04-09T14:06:29.855528Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df._features.map(lambda feature_list: [feature.name for feature in feature_list.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c5d413214a7d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:30.020387Z",
     "start_time": "2024-04-09T14:06:29.912215Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = df._features.map(len)\n",
    "plt.hist(num_features[num_features < 1000], bins=100, color='C0')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fa24d5b88d7dc",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64fc7ede240fc961",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Distribution of Number of Features with feature count > 1000 and > 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24a4d6befdecaef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:30.147573Z",
     "start_time": "2024-04-09T14:06:30.022064Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(num_features[(num_features > 1000) & (num_features < 2000)], bins=10, color='C0')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(num_features[(num_features > 2000)], bins=10, color='C0')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71acdaf6a0690f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## URL Analysis in Dataset Descriptions\n",
    "\n",
    "Identifying and analyzing the presence of URLs in the dataset descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035aaed7aad0779",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Datasets with URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a59f12adb1f2920f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:06:30.432613Z",
     "start_time": "2024-04-09T14:06:30.148519Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to check if a description contains a URL\n",
    "def contains_url(description):\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(description)\n",
    "    return bool(urls)\n",
    "\n",
    "\n",
    "datasets_with_urls = df[df['description'].apply(contains_url)]\n",
    "datasets_with_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f23a5f3a82a84c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualization of Datasets with URLs vs Datasets without URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d6c11ccf13214",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=['datasets with urls', 'datasets without urls'],\n",
    "            y=[len(datasets_with_urls), len(df) - len(datasets_with_urls)],\n",
    "            hue=['datasets with urls', 'datasets without urls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4cf69702f08605",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Number of URLs in Dataset Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b1b23ca3ad0e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the number of URLs in dataset descriptions for all datasets\n",
    "plt.hist(df['description'].apply(lambda x: len(URLExtract().find_urls(x))), bins=25, color='C0')\n",
    "plt.xlabel('Number of URLs in description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(\n",
    "    df['description'].apply(lambda x: len(URLExtract().find_urls(x)) if len(URLExtract().find_urls(x)) < 100 else 100),\n",
    "    bins=25, color='C0')\n",
    "plt.xlabel('Number of URLs in description')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89979420c91dcc40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each dataset, extract their urls, and put it in a column \"description_urls\"\n",
    "def extract_urls(description):\n",
    "    # Find all URLs in the description\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(description)\n",
    "    return urls\n",
    "\n",
    "\n",
    "datasets_with_urls['description_urls'] = datasets_with_urls['description'].apply(extract_urls)\n",
    "datasets_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c44aeea60ebd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_domain(description):\n",
    "    # Find all URLs in the description\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(description)\n",
    "    domains = []\n",
    "    for url in urls:\n",
    "        # Prepend a scheme if missing\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "        try:\n",
    "            domain = urlparse(url).netloc\n",
    "            if domain:\n",
    "                domains.append(domain)\n",
    "            else:\n",
    "                print(f\"Error: No domain found in URL: {url}\")\n",
    "        except ValueError as e:\n",
    "            # If an invalid URL is found, log an error message\n",
    "            print(f\"ValueError: {str(e)} in URL: {url}\")\n",
    "    return domains\n",
    "\n",
    "\n",
    "domains = datasets_with_urls['description'].apply(extract_domain)\n",
    "domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ffa3d315ee701",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Distribution of Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b2e3a21cf104",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sublists = [domain for sublist in domains for domain in sublist]\n",
    "domain_freq = Counter(sublists)\n",
    "domain_freq = pd.DataFrame(sorted([(count, domain) for domain, count in domain_freq.items()], reverse=True))\n",
    "domain_freq.columns = ['frequency', 'domain']\n",
    "\n",
    "k = 15\n",
    "sns.barplot(x=domain_freq['frequency'][:k], y=domain_freq['domain'][:k], hue=domain_freq['domain'][:k])\n",
    "plt.title(f'Top {k} most common domains in dataset descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2372f60b29ad7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_urls(description):\n",
    "    # Find all URLs in the description\n",
    "    extractor = URLExtract()\n",
    "    urls = extractor.find_urls(description)\n",
    "    return urls\n",
    "\n",
    "\n",
    "urls = datasets_with_urls['description'].apply(extract_urls)\n",
    "urls_df = pd.DataFrame(urls)\n",
    "urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77770c9d138fc15e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecd3fa3ae57c55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sublists = [url for sublist in urls for url in sublist]\n",
    "url_freq = Counter(sublists)\n",
    "url_freq = pd.DataFrame(sorted([(count, url) for url, count in url_freq.items()], reverse=True))\n",
    "url_freq.columns = ['frequency', 'url']\n",
    "\n",
    "k = 30\n",
    "sns.barplot(x=url_freq['frequency'][:k], y=url_freq['url'][:k], hue=url_freq['url'][:k])\n",
    "plt.title(f'Top {k} most common URLs in dataset descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7ef9ecdaa8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 30 least common URLs\n",
    "k = 30\n",
    "sns.barplot(x=url_freq['frequency'][-k:], y=url_freq['url'][-k:], hue=url_freq['url'][-k:])\n",
    "plt.title(f'Top {k} least common URLs in dataset descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f6cab3dc46b3c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Number of unique URLs: {len(url_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8561568dbe7263",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See that the URLs in the descriptions that are meaningful are actually contained in the original_data_url-s\n",
    "# therefore, inspecting the data from the URLs in the descriptions is not going to be that useful. Also check plot above.\n",
    "df[df['original_data_url'].map(lambda x: 'IPUMS' in x if x else False)][['name', 'original_data_url', 'openml_url']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
