{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivang/miniconda3/envs/mvp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import hdbscan\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import PartOfSpeech\n",
    "import os\n",
    "import json\n",
    "import openml\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_list = openml.datasets.list_datasets()\n",
    "# ids = list(datasets_list.keys())\n",
    "# print(\"Number of datasets: \", len(ids))\n",
    "\n",
    "# datasets = []\n",
    "# # # Removing non-dataset entries \n",
    "# # ids.remove(4537)\n",
    "# # ids.remove(4546)\n",
    "# # ids.remove(4562)\n",
    "\n",
    "# for dataset_id in tqdm(ids, desc=\"Downloading datasets\"):\n",
    "#     try:\n",
    "#         dataset = openml.datasets.get_dataset(dataset_id, download_features_meta_data=True)\n",
    "#     except openml.exceptions.OpenMLServerException:\n",
    "#         # If the first attempt fails, try again without features metadata\n",
    "#         dataset = openml.datasets.get_dataset(dataset_id, download_features_meta_data=False)\n",
    "#     datasets.append(dataset)\n",
    "\n",
    "# datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame([dataset.__dict__ for dataset in datasets])\n",
    "# # Add the OpenML URL to the dataframe\n",
    "# df['openml_url'] = [dataset.openml_url for dataset in datasets]\n",
    "# df['upload_date'] = [dataset.upload_date for dataset in datasets]\n",
    "# # All datasets which have None description, make it an empty string \"\"\n",
    "# df['description'] = df['description'].fillna('')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe, so datasets aren't refetched every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"datasets_checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets from the saved CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ignore_attribute</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>description</th>\n",
       "      <th>cache_format</th>\n",
       "      <th>format</th>\n",
       "      <th>creator</th>\n",
       "      <th>contributor</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>...</th>\n",
       "      <th>parquet_file</th>\n",
       "      <th>_dataset</th>\n",
       "      <th>_parquet_url</th>\n",
       "      <th>_features</th>\n",
       "      <th>_qualities</th>\n",
       "      <th>_no_qualities_found</th>\n",
       "      <th>data_pickle_file</th>\n",
       "      <th>data_feather_file</th>\n",
       "      <th>feather_attribute_file</th>\n",
       "      <th>openml_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>anneal</td>\n",
       "      <td>1</td>\n",
       "      <td>**Author**: Unknown. Donated by David Sterling...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>[David Sterling, Wray Buntine]</td>\n",
       "      <td>David Sterling and Wray Buntine</td>\n",
       "      <td>1990</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://openml1.win.tue.nl/dataset2/dataset_2.pq</td>\n",
       "      <td>{0: [0 - family (nominal)], 1: [1 - product-ty...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>kr-vs-kp</td>\n",
       "      <td>1</td>\n",
       "      <td>Author: Alen Shapiro\\nSource: [UCI](https://ar...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>Alen Shapiro</td>\n",
       "      <td>Rob Holte</td>\n",
       "      <td>1989-08-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://openml1.win.tue.nl/dataset3/dataset_3.pq</td>\n",
       "      <td>{0: [0 - bkblk (nominal)], 1: [1 - bknwy (nomi...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>labor</td>\n",
       "      <td>1</td>\n",
       "      <td>**Author**: Unknown\\n**Source**: Collective Ba...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>Collective Bargaining Review of Labour Canada</td>\n",
       "      <td>Stan Matwin</td>\n",
       "      <td>1988-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://openml1.win.tue.nl/dataset4/dataset_4.pq</td>\n",
       "      <td>{0: [0 - duration (numeric)], 1: [1 - wage-inc...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>1</td>\n",
       "      <td>**Author**: H. Altay Guvenir, Burak Acar, Hald...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>[H. Altay Guvenir, Burak Acar, Haldun Muderris...</td>\n",
       "      <td>None</td>\n",
       "      <td>1998-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://openml1.win.tue.nl/dataset5/dataset_5.pq</td>\n",
       "      <td>{0: [0 - age (numeric)], 1: [1 - sex (nominal)...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>letter</td>\n",
       "      <td>1</td>\n",
       "      <td>**Author**: David J. Slate  \\n**Source**: [UCI...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>David J. Slate</td>\n",
       "      <td>None</td>\n",
       "      <td>1991-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>http://openml1.win.tue.nl/dataset6/dataset_6.pq</td>\n",
       "      <td>{0: [0 - x-box (numeric)], 1: [1 - y-box (nume...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5809</th>\n",
       "      <td>None</td>\n",
       "      <td>46462</td>\n",
       "      <td>pollution</td>\n",
       "      <td>5</td>\n",
       "      <td>air quality</td>\n",
       "      <td>pickle</td>\n",
       "      <td>arff</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://openml1.win.tue.nl/datasets/0004/46462...</td>\n",
       "      <td>{0: [0 - ID (numeric)], 1: [1 - log_pSat_Pa (n...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/46462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>None</td>\n",
       "      <td>46463</td>\n",
       "      <td>pollution</td>\n",
       "      <td>6</td>\n",
       "      <td>air quality</td>\n",
       "      <td>pickle</td>\n",
       "      <td>arff</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://openml1.win.tue.nl/datasets/0004/46463...</td>\n",
       "      <td>{0: [0 - ID (numeric)], 1: [1 - log_pSat_Pa (n...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/46463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>None</td>\n",
       "      <td>46467</td>\n",
       "      <td>IBRD_Loans_Classification</td>\n",
       "      <td>4</td>\n",
       "      <td>The International Bank for Reconstruction and ...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>arff</td>\n",
       "      <td>Anna Wiewer</td>\n",
       "      <td>OpenML Community</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://openml1.win.tue.nl/datasets/0004/46467...</td>\n",
       "      <td>{0: [0 - Loan_Number (string)], 1: [1 - Loan_T...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/46467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>None</td>\n",
       "      <td>46468</td>\n",
       "      <td>Give-Me-Some-Credit-Sampled</td>\n",
       "      <td>1</td>\n",
       "      <td>Originial Dataset Give-Me-Some-Credit 45577</td>\n",
       "      <td>pickle</td>\n",
       "      <td>arff</td>\n",
       "      <td>M. Freurer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://openml1.win.tue.nl/datasets/0004/46468...</td>\n",
       "      <td>{0: [0 - SeriousDlqin2yrs (nominal)], 1: [1 - ...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/46468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>None</td>\n",
       "      <td>46469</td>\n",
       "      <td>Corporate_Credit_Rating</td>\n",
       "      <td>1</td>\n",
       "      <td># Credit Ratings of Big US Firms and their Fin...</td>\n",
       "      <td>pickle</td>\n",
       "      <td>arff</td>\n",
       "      <td>Alan Gewerc</td>\n",
       "      <td>Yayun Li</td>\n",
       "      <td>2024-12-04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://openml1.win.tue.nl/datasets/0004/46469...</td>\n",
       "      <td>{0: [0 - rating (string)], 1: [1 - name (strin...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.openml.org/d/46469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5814 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ignore_attribute  dataset_id                         name  version  \\\n",
       "0                None           2                       anneal        1   \n",
       "1                None           3                     kr-vs-kp        1   \n",
       "2                None           4                        labor        1   \n",
       "3                None           5                   arrhythmia        1   \n",
       "4                None           6                       letter        1   \n",
       "...               ...         ...                          ...      ...   \n",
       "5809             None       46462                    pollution        5   \n",
       "5810             None       46463                    pollution        6   \n",
       "5811             None       46467    IBRD_Loans_Classification        4   \n",
       "5812             None       46468  Give-Me-Some-Credit-Sampled        1   \n",
       "5813             None       46469      Corporate_Credit_Rating        1   \n",
       "\n",
       "                                            description cache_format format  \\\n",
       "0     **Author**: Unknown. Donated by David Sterling...       pickle   ARFF   \n",
       "1     Author: Alen Shapiro\\nSource: [UCI](https://ar...       pickle   ARFF   \n",
       "2     **Author**: Unknown\\n**Source**: Collective Ba...       pickle   ARFF   \n",
       "3     **Author**: H. Altay Guvenir, Burak Acar, Hald...       pickle   ARFF   \n",
       "4     **Author**: David J. Slate  \\n**Source**: [UCI...       pickle   ARFF   \n",
       "...                                                 ...          ...    ...   \n",
       "5809                                        air quality       pickle   arff   \n",
       "5810                                        air quality       pickle   arff   \n",
       "5811  The International Bank for Reconstruction and ...       pickle   arff   \n",
       "5812        Originial Dataset Give-Me-Some-Credit 45577       pickle   arff   \n",
       "5813  # Credit Ratings of Big US Firms and their Fin...       pickle   arff   \n",
       "\n",
       "                                                creator  \\\n",
       "0                        [David Sterling, Wray Buntine]   \n",
       "1                                          Alen Shapiro   \n",
       "2         Collective Bargaining Review of Labour Canada   \n",
       "3     [H. Altay Guvenir, Burak Acar, Haldun Muderris...   \n",
       "4                                        David J. Slate   \n",
       "...                                                 ...   \n",
       "5809                                               None   \n",
       "5810                                               None   \n",
       "5811                                        Anna Wiewer   \n",
       "5812                                         M. Freurer   \n",
       "5813                                        Alan Gewerc   \n",
       "\n",
       "                          contributor collection_date  ... parquet_file  \\\n",
       "0     David Sterling and Wray Buntine            1990  ...         None   \n",
       "1                           Rob Holte      1989-08-01  ...         None   \n",
       "2                         Stan Matwin      1988-11-01  ...         None   \n",
       "3                                None      1998-01-01  ...         None   \n",
       "4                                None      1991-01-01  ...         None   \n",
       "...                               ...             ...  ...          ...   \n",
       "5809                             None            None  ...         None   \n",
       "5810                             None            None  ...         None   \n",
       "5811                 OpenML Community      2024-12-01  ...         None   \n",
       "5812                             None            None  ...         None   \n",
       "5813                         Yayun Li      2024-12-04  ...         None   \n",
       "\n",
       "     _dataset                                       _parquet_url  \\\n",
       "0        None    http://openml1.win.tue.nl/dataset2/dataset_2.pq   \n",
       "1        None    http://openml1.win.tue.nl/dataset3/dataset_3.pq   \n",
       "2        None    http://openml1.win.tue.nl/dataset4/dataset_4.pq   \n",
       "3        None    http://openml1.win.tue.nl/dataset5/dataset_5.pq   \n",
       "4        None    http://openml1.win.tue.nl/dataset6/dataset_6.pq   \n",
       "...       ...                                                ...   \n",
       "5809     None  https://openml1.win.tue.nl/datasets/0004/46462...   \n",
       "5810     None  https://openml1.win.tue.nl/datasets/0004/46463...   \n",
       "5811     None  https://openml1.win.tue.nl/datasets/0004/46467...   \n",
       "5812     None  https://openml1.win.tue.nl/datasets/0004/46468...   \n",
       "5813     None  https://openml1.win.tue.nl/datasets/0004/46469...   \n",
       "\n",
       "                                              _features _qualities  \\\n",
       "0     {0: [0 - family (nominal)], 1: [1 - product-ty...       None   \n",
       "1     {0: [0 - bkblk (nominal)], 1: [1 - bknwy (nomi...       None   \n",
       "2     {0: [0 - duration (numeric)], 1: [1 - wage-inc...       None   \n",
       "3     {0: [0 - age (numeric)], 1: [1 - sex (nominal)...       None   \n",
       "4     {0: [0 - x-box (numeric)], 1: [1 - y-box (nume...       None   \n",
       "...                                                 ...        ...   \n",
       "5809  {0: [0 - ID (numeric)], 1: [1 - log_pSat_Pa (n...       None   \n",
       "5810  {0: [0 - ID (numeric)], 1: [1 - log_pSat_Pa (n...       None   \n",
       "5811  {0: [0 - Loan_Number (string)], 1: [1 - Loan_T...       None   \n",
       "5812  {0: [0 - SeriousDlqin2yrs (nominal)], 1: [1 - ...       None   \n",
       "5813  {0: [0 - rating (string)], 1: [1 - name (strin...       None   \n",
       "\n",
       "     _no_qualities_found data_pickle_file data_feather_file  \\\n",
       "0                  False             None              None   \n",
       "1                  False             None              None   \n",
       "2                  False             None              None   \n",
       "3                  False             None              None   \n",
       "4                  False             None              None   \n",
       "...                  ...              ...               ...   \n",
       "5809               False             None              None   \n",
       "5810               False             None              None   \n",
       "5811               False             None              None   \n",
       "5812               False             None              None   \n",
       "5813               False             None              None   \n",
       "\n",
       "     feather_attribute_file                      openml_url  \n",
       "0                      None      https://www.openml.org/d/2  \n",
       "1                      None      https://www.openml.org/d/3  \n",
       "2                      None      https://www.openml.org/d/4  \n",
       "3                      None      https://www.openml.org/d/5  \n",
       "4                      None      https://www.openml.org/d/6  \n",
       "...                     ...                             ...  \n",
       "5809                   None  https://www.openml.org/d/46462  \n",
       "5810                   None  https://www.openml.org/d/46463  \n",
       "5811                   None  https://www.openml.org/d/46467  \n",
       "5812                   None  https://www.openml.org/d/46468  \n",
       "5813                   None  https://www.openml.org/d/46469  \n",
       "\n",
       "[5814 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"datasets_checkpoint.pkl\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from augmenters import (\n",
    "#     TagAugmenter,\n",
    "#     FeatureAugmenter, \n",
    "#     DatasetAugmenter,\n",
    "#     SimilarityAugmenter,\n",
    "#     NameAugmenter,\n",
    "#     ScrapyAugmenter\n",
    "# )\n",
    "\n",
    "# description_column = 'description'\n",
    "# dataset_id_column = 'dataset_id'\n",
    "# tag_column = 'tag'\n",
    "# features_column = '_features'\n",
    "# name_column = 'name'\n",
    "augmented_column = 'augmented_description'\n",
    "# similar_datasets_column = 'similar_datasets'\n",
    "# scraped_column = 'scraped_data'\n",
    "# prompt_description_column = 'prompt_description'\n",
    "\n",
    "# augmenters = [\n",
    "#     TagAugmenter(description_column, tag_column, augmented_column, dataset_id_column, json_file_path=\"./GPT_semantic_tags.json\"),\n",
    "#     NameAugmenter(description_column, name_column, augmented_column),\n",
    "#     FeatureAugmenter(description_column, features_column, augmented_column, max_features=100, reduce_features=True),\n",
    "#     ScrapyAugmenter(description_column, scraped_column, augmented_column),\n",
    "#     SimilarityAugmenter(description_column, augmented_column, similar_datasets_column),\n",
    "# ]\n",
    "\n",
    "# dataset_augmenter = DatasetAugmenter(augmenters=augmenters)\n",
    "# augmented_df = dataset_augmenter.augment(df.copy())\n",
    "# augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save augmented descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_df[augmented_column].to_csv(\"augmented_description_checkpoint.csv\", index=False)\n",
    "# data = augmented_df[augmented_column].tolist()\n",
    "data = pd.read_csv(\"augmented_description_checkpoint.csv\")[augmented_column].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# embedding_model = SentenceTransformer(\"Salesforce/SFR-Embedding-2_R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5332/5332 [7:14:30<00:00,  4.89s/it]   \n"
     ]
    }
   ],
   "source": [
    "# embeddings = embedding_model.encode(data, show_progress_bar=True, batch_size=1)\n",
    "# np.save('embeddings_checkpoint.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "embeddings = np.load('embeddings_checkpoint.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the below information, extract and synthesize human-readable tags/keywords/themes from the text, capitalized first letters of words. What is the main human-readable theme or subject matter discussed in the provided texts? What is the overarching, high-level theme of the texts, e.g. \"Music\", \"Sports\", \"Environment\", etc.? Please provide overarching themes that tie the different pieces of information together. What is/are the overarching, highest level theme(s) that you could use as a keyword(s)? Prefer single word tags/keywords, e.g. \"Tennis\" rather than \"Tennis Match\", \"Prison\" rather than \"Prison Time\", etc., however, if the term makes sense only as a whole, e.g. \"Republic of the Congo\", \"COVID-19\", then use it! Consider adding synonyms as well, e.g. for \"Influenza\", add \"Flu\", for \"Car\", add \"Automobile\", etc.! Some examples of human-readable themes are   \"Agriculture\", \"Astronomy\", \"Chemistry\", \"Computational Universe\", \"Computer Systems\", \"Climate and Environment\", \"Culture\", \"Demographics\", \"Earth Science\", \"Economics\", \"Education\", \"Engineering\", \"Finance\", \"Geography\", \"Government\", \"Games\", \"Health\", \"History\", \"Human Activities\", \"Images\", \"Language\", \"Law\", \"Life Science\", \"Machine Learning\", \"Manufacturing\", \"Mathematics\", \"Medicine\", \"Meteorology\", \"Physical Sciences\", \"Politics\", \"Social Media\", \"Sociology\", \"Statistics\", \"Text & Literature\",  \"Transportation\". Avoid tags/keywords that are too specific, e.g. \"Serine Threonine Protein Kinase\". Good theme examples are: \"Birds\", \"Species Migration\", \"Air Pollution\", or \"War\", \"Government\", \"International Relations\", \"Politics\". And be concise in theme generation, e.g. instead of \"Income Prediction\", say \"Income\", instead of \"Demographic Information\", say \"Demographics\"! Another important rule to obey - place more focus on the dataset names for theme extraction, if they exist. Say {\"topic\": {\"themes\": [\"example1\", \"example2\", ...], \"overarching_themes\": [\"example13\", \"...\", ...]}, \"texts\": [{\"example1\": 0.93, \"example2\": 0.68, \"...\": ..., ..., \"example13\": ..., ... (strings corresponding to themes and overarching themes for text 1, all of them)}], {\"example1\": 0.87, \"example2\": 0.34, \"...\": ..., ..., \"example13\": ..., ... (strings corresponding to themes and overarching themes for text 2, all of them)}]} and give your answer in JSON format, where under \"topic\" you put all themes and overarching themes, and in \"texts\", you give a confidence score in each theme and overarching theme for each text. \"themes\" and \"overarching_themes\" shouldn't overlap. If a theme is overarching and common to all texts, it should be present in all texts with a high score. Give only the ones with highest scores.\n",
    "# For example, for this text:\n",
    "# Text 1: Name: Biden Administration\n",
    "\n",
    "# The Biden administration is preparing to roll out a sweeping border executive action as early as Tuesday, according to two sources familiar with the discussions, who cautioned that timing is fluid.\n",
    "\n",
    "# White House officials have begun reaching out to mayors who represent cities along the US southern border to potentially join President Joe Biden when he announces the order, two other sources familiar with those conversations said.\n",
    "\n",
    "# For weeks, administration officials have been working through an executive action that would dramatically limit migrants’ ability to seek asylum at the US southern border — part of a strategy to try to give Biden the upper hand on one of his Republican rival’s key campaign issues. The action is designed to potentially blunt Republican attacks on border security and preempt former President Donald Trump ahead of the first presidential debate, which will be held on June 27 on CNN.\n",
    "# ---\n",
    "# Text 2: Name: Trump conviction\n",
    "\n",
    "# Now that a New York jury has convicted former President Donald Trump of all 34 felony charges of falsifying business records, the next obvious question is: Can a convicted felon run for president?\n",
    "\n",
    "# Definitely.\n",
    "\n",
    "# Trump meets all three requirements. There is, arguably, another criterion laid out in the 14th Amendment, where it states that no one who has previously taken an oath of office who engages in insurrection can be an officer of the US. But the US Supreme Court ruled earlier this year that Congress would have to pass a special law invoking this prohibition. That’s not happening any time soon.\n",
    "\n",
    "# Judge Juan Merchan has scheduled Trump’s sentencing for July 11, which happens to be four days before the start of the Republican National Convention that is scheduled to take place in Milwaukee.\n",
    "\n",
    "# It is technically possible, although perhaps unlikely for a first-time offender, that Trump could be sentenced to prison time.\n",
    "# ---\n",
    "# Text 3: Trump has vowed to give green cards to college grads. Could that actually happen?\n",
    "\n",
    "# The candidate known for touting immigration crackdowns told a group of tech investors that he wanted to help foreign students stay in the US.\n",
    "\n",
    "# “What I want to do, and what I will do, is — you graduate from a college, I think you should get automatically, as part of your diploma, a green card to be able to stay in this country,” Trump said during a June interview with “The All-In Podcast.”\n",
    "\n",
    "# If the president-elect pursues this proposal after he takes office, and if Congress passes the legislation that would be required to enact it, the policy could pave the way for potentially millions of international students to become legal permanent residents.\n",
    "# ---\n",
    "# This would be your answer:\n",
    "# {\n",
    "#   \"topic\": {\n",
    "#     \"themes\": [\n",
    "#       \"Biden Administration\",\n",
    "#       \"Border\",\n",
    "#       \"Executive Action\",\n",
    "#       \"Asylum\",\n",
    "#       \"Immigration\",\n",
    "#       \"Trump\",\n",
    "#       \"Felony\",\n",
    "#       \"Business Records\",\n",
    "#       \"Presidential Campaign\",\n",
    "#       \"Republican\",\n",
    "#       \"Debate\",\n",
    "#       \"Former President\",\n",
    "#       \"Conviction\",\n",
    "#       \"Sentencing\",\n",
    "#       \"Prison\",\n",
    "#       \"14th Amendment\",\n",
    "#       \"Insurrection\",\n",
    "#       \"Supreme Court\",\n",
    "#       \"Republican National Convention\",\n",
    "#       \"College\",\n",
    "#       \"Green Card\",\n",
    "#       \"Legislation\",\n",
    "#       \"Student\"\n",
    "#     ],\n",
    "#     \"overarching_themes\": [\n",
    "#       \"Politics\",\n",
    "#       \"Government\",\n",
    "#       \"Law\",\n",
    "#       \"Justice\",\n",
    "#       \"Elections\",\n",
    "#       \"Education\"\n",
    "#     ]\n",
    "#   },\n",
    "#   \"texts\": { \n",
    "#     \"text1\": {\n",
    "#       \"Biden Administration\": 0.96,\n",
    "#       \"Border\": 0.92,\n",
    "#       \"Executive Action\": 0.91,\n",
    "#       \"Asylum\": 0.88,\n",
    "#       \"Immigration\": 0.84,\n",
    "#       \"Presidential Campaign\": 0.82,\n",
    "#       \"Republican\": 0.82,\n",
    "#       \"Debate\": 0.78,\n",
    "#       \"Politics\": 0.99,\n",
    "#       \"Government\": 0.93,\n",
    "#       \"Law\": 0.85,\n",
    "#       \"Elections\": 0.72,\n",
    "#     },\n",
    "#     \"text2\": {\n",
    "#       \"Trump\": 0.95,\n",
    "#       \"Felony\": 0.92,\n",
    "#       \"Business Records\": 0.97,\n",
    "#       \"Presidential Campaign\": 0.84,\n",
    "#       \"Republican\": 0.82,\n",
    "#       \"Former President\": 0.98,\n",
    "#       \"Conviction\": 0.92,\n",
    "#       \"Sentencing\": 0.91,\n",
    "#       \"Prison\": 0.85,\n",
    "#       \"14th Amendment\": 0.82,\n",
    "#       \"Insurrection\": 0.80,\n",
    "#       \"Supreme Court\": 0.78,\n",
    "#       \"Republican National Convention\": 0.76,\n",
    "#       \"Politics\": 0.92,\n",
    "#       \"Government\": 0.92,\n",
    "#       \"Law\": 0.90,\n",
    "#       \"Justice\": 0.88,\n",
    "#       \"Elections\": 0.85,\n",
    "#     },\n",
    "#     \"text3\": {\n",
    "#       \"Immigration\": 0.67,\n",
    "#       \"Trump\": 0.98,\n",
    "#       \"Republican\": 0.59,\n",
    "#       \"College\": 0.98,\n",
    "#       \"Green Card\": 0.93,\n",
    "#       \"Legislation\": 0.89,\n",
    "#       \"Student\": 0.89,\n",
    "#       \"Politics\": 0.82,\n",
    "#       \"Government\": 0.81,\n",
    "#       \"Law\": 0.69,\n",
    "#       \"Education\": 0.97\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "# ---\n",
    "# Now, the above was just an example. Now, do it for all the following text(s), generate many themes:\n",
    "# Text 1: Name: COVID-19 Rio de Janeiro (City)\\n\\nTags: Context - World Health Organization (WHO) Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus.\\n\\nThis dataset provides information on the number of confirmed cases, deaths, and recoveries by neighborhood in the city of Rio de Janeiro, Brazil. It is essential to note that this is a time-series data, and therefore, the number of cases on any given day represents a cumulative total. To obtain the number of new cases, one can calculate the difference between the current and previous days' data.\\n\\nThe data is available from April to December, providing a comprehensive overview of the pandemic's progression in the city. The dataset is a valuable resource for analyzing changes in the number of confirmed cases, deaths, and recoveries by neighborhood over time, as well as at the city level. It also enables the study of the spread of the disease in the city.\\n\\nDate, Hour, Neighborhood, Cases, Deaths, Recovered\n",
    "# ---\n",
    "# Text 2: Name: Covid-19 Turkey Daily Details Dataset\\nTags: Context Turkey Covid 19 Dataset\\n\\nThis dataset has been created in accordance with the data shared by the Ministry of Health of the Republic of Turkey. The data is collected from the website of the Ministry of Health on a daily basis using data mining methods. The dataset is updated daily and shared on GitHub.\\n\\nThe dataset contains the following columns:\\n\\nTotal Tests: the cumulative number of tests carried out up to a specific date\\nTotal Cases: the cumulative number of cases announced up to a specific date\\nTotal Deaths: the cumulative number of deaths announced up to a specific date\\nTotal Intensive Care: the cumulative number of people in intensive care announced until a specific date\\nTotal Intubated: the cumulative number of intubated people announced until a specific date\\nTotal Recovered: the cumulative number of people who have recovered\\nDaily Tests: the number of people tested on a specific day\\nDaily Cases: the number of new cases announced on a specific day\\nDaily Deaths: the number of new deaths announced on a specific day\\nDaily Recovered: the number of people who have recovered on a specific day\\n\\nFeatures: date, total tests, total cases, total deaths, total intensive care, total intubated, total recovered, daily tests, daily cases, daily deaths\n",
    "# ---\n",
    "# Text 3: Name: COVID-19 World Vaccination Progress\\nTags: Context\\n\\nData is collected daily from Our World in Data GitHub repository for COVID-19, merged and uploaded.\\n\\nThe data contains the following information: Country, which is the country for which the vaccination information is provided; Country ISO Code, which is the ISO code for the country; Date, which is the date for the data entry; for some of the dates, we have only the daily vaccinations, while for others, only the cumulative total; Total Number of Vaccinations, which is the absolute number of total immunizations in the country; Total Number of People Vaccinated, which is a person, depending on the immunization scheme, will receive one or more, typically two, vaccines; at a certain moment, the number of vaccinations might be larger than the number of people; Total Number of People Fully Vaccinated, which is the number of people that received the entire set of immunization according to the immunization scheme, typically two; at a certain moment in time, there might be a certain number of people that received one vaccine and another number, smaller, of people that received all vaccines in the scheme; Daily Vaccinations, which is for a certain data entry, the number of vaccination for that date and country; Total Vaccinations per Hundred, which is the ratio, in percent, between vaccination number and total population up to the date in the country; Total Number of People Vaccinated per Hundred, which is the ratio, in percent, between population immunized and total population up to the date in the country; Total Number of People Fully Vaccinated per Hundred, which is the ratio, in percent, between population fully immunized and total population up to the date in the country; Number of Vaccinations per Day, which is the number of daily vaccination for that day and country; Daily Vaccinations per Million, which is the ratio, in parts per million, between vaccination number and total population for the current date in the country; Vaccines Used in the Country, which is the total number of vaccines used in the country up to date; Source Name, which is the source of the information, such as national authority, international organization, local organization, etc.; Source Website, which is the website of the source of information.\\n\\nThis dataset allows users to track COVID-19 vaccination progress in the world, answering questions such as which country is using what vaccine, in which country the vaccination program is more advanced, where are more people vaccinated per day, and where are more people vaccinated in terms of percentage from the entire population\n",
    "# ---\n",
    "# Text 4: Name: COVID-19 U.S. Daily Cases\n",
    "# Tags: Context - U.S. COVID-19 Data\n",
    "# This dataset provides daily updates on confirmed COVID-19 cases, hospitalizations, and deaths across the United States. It includes state-by-state breakdowns and national trends, allowing for a comprehensive analysis of how COVID-19 is affecting different regions. The data is updated regularly and helps public health officials and researchers to track the virus's spread, assess the effectiveness of interventions, and inform policy decisions.\n",
    "# ---\n",
    "# Text 5: Name: COVID-19 Global Testing Rates\n",
    "# Tags: Context - Global Testing Data\n",
    "# This dataset tracks the number of COVID-19 tests conducted worldwide, highlighting the importance of testing in managing the pandemic. It includes data on daily testing rates, cumulative tests by country, and comparisons of testing capacities among nations. The information is crucial for understanding how different countries are responding to the pandemic and for evaluating the effectiveness of their public health strategies.\n",
    "# ---\n",
    "# Text 6: Name: COVID-19 Variants Tracker\n",
    "# Tags: Context - Virus Variants\n",
    "# This dataset includes information on different COVID-19 variants identified globally, tracking their prevalence and mutations. It provides insights into how these variants affect transmissibility, severity, and vaccine efficacy. The data is essential for researchers and public health officials to monitor the evolution of the virus and make informed decisions regarding vaccination strategies and public health measures.\n",
    "# ---\n",
    "# Text 7: Name: COVID-19 Economic Impact Survey\n",
    "# Tags: Context - Economic Effects of COVID-19\n",
    "# This dataset captures the economic effects of the pandemic through surveys assessing job loss, business closures, and changes in income across various demographics. It provides a detailed look at how different sectors are affected and the long-term implications for the economy. This information is vital for policymakers and economists to design effective recovery strategies and support measures for affected individuals and businesses.\n",
    "# ---\n",
    "# Text 8: Name: COVID-19 Mental Health Impact Study\n",
    "# Tags: Context - Mental Health\n",
    "# This dataset examines the mental health effects of the COVID-19 pandemic, focusing on anxiety, depression, and overall well-being across different age groups and communities. It includes survey responses collected during various stages of the pandemic, providing valuable insights into the psychological toll of the crisis. Understanding these impacts is crucial for developing mental health resources and support systems in the wake of the pandemic.\n",
    "# ---\n",
    "# Text 9: Name: COVID-19 Vaccine Adverse Events Reporting\n",
    "# Tags: Context - Vaccine Safety\n",
    "# This dataset contains reports of adverse events following COVID-19 vaccinations, compiled from various health agencies and reporting systems. It includes information on the types of side effects reported, demographic data of affected individuals, and follow-up outcomes. This data is essential for monitoring vaccine safety and ensuring public confidence in vaccination programs as they play a crucial role in controlling the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:   9%|▉         | 1/11 [00:00<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document   ID  Topic\n",
      "0    Name: anneal\\n\\nTags: study_1, study_14, study...    0    133\n",
      "1    Name: kr-vs-kp\\n\\nTags: mythbusting_1, OpenML-...    1    219\n",
      "2    Name: labor\\n\\nTags: mythbusting_1, study_1, s...    2    184\n",
      "3    Name: arrhythmia\\n\\nTags: sport, study_1, stud...    3    247\n",
      "4    Name: letter\\n\\nTags: AzurePilot, AzurePilot1,...    4    197\n",
      "..                                                 ...  ...    ...\n",
      "495  Name: fri_c4_250_25\\n\\nTags: artificial\\n\\n**A...  495    462\n",
      "496  Name: fri_c3_500_50\\n\\nTags: artificial\\n\\n**A...  496    471\n",
      "497  Name: fri_c3_500_10\\n\\nTags: artificial\\n\\n**A...  497    450\n",
      "498  Name: fri_c1_250_10\\n\\nTags: artificial\\n\\n**A...  498    459\n",
      "499  Name: fri_c1_250_50\\n\\nTags: artificial\\n\\n**A...  499    355\n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  18%|█▊        | 2/11 [00:01<00:06,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: fri_c0_500_5\\n\\nTags: \\n\\n**Author**:   ...    0.0  248.0\n",
      "1    Name: fri_c0_500_50\\n\\nTags: artificial\\n\\n**A...    1.0  248.0\n",
      "2    Name: fri_c0_100_25\\n\\nTags: artificial\\n\\n**A...    2.0  248.0\n",
      "3    Name: fri_c4_100_100\\n\\nTags: artificial\\n\\n**...    3.0  248.0\n",
      "4    Name: fri_c0_250_25\\n\\nTags: artificial\\n\\n**A...    4.0  248.0\n",
      "..                                                 ...    ...    ...\n",
      "891                                                     891.0  475.0\n",
      "892                                                     892.0  485.0\n",
      "893                                                     893.0  488.0\n",
      "894                                                     894.0  493.0\n",
      "895                                                     895.0  496.0\n",
      "\n",
      "[896 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  27%|██▋       | 3/11 [00:02<00:06,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: BNG(spect_test)\\n\\nTags: artificial, BNG...    0.0  248.0\n",
      "1    Name: BNG(spectf_test)\\n\\nTags: artificial, BN...    1.0  248.0\n",
      "2    Name: BNG(adult)\\n\\nTags: artificial, BNG, stu...    2.0  483.0\n",
      "3    Name: BNG(satimage)\\n\\nTags: artificial, BNG, ...    3.0  248.0\n",
      "4    Name: BNG(baseball)\\n\\nTags: artificial, BNG\\n...    4.0  483.0\n",
      "..                                                 ...    ...    ...\n",
      "972                                                     972.0  493.0\n",
      "973                                                     973.0  494.0\n",
      "974                                                     974.0  496.0\n",
      "975                                                     975.0  497.0\n",
      "976                                                     976.0  499.0\n",
      "\n",
      "[977 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: QSAR-TID-11811\\n\\nTags: MTLQSAR, study_3...    0.0   13.0\n",
      "1    Name: QSAR-TID-30015\\n\\nTags: MTLQSAR, study_3...    1.0   13.0\n",
      "2    Name: QSAR-TID-101299\\n\\nTags: MTLQSAR, study_...    2.0   13.0\n",
      "3    Name: QSAR-TID-20126\\n\\nTags: MTLQSAR, study_3...    3.0   13.0\n",
      "4    Name: QSAR-TID-12887\\n\\nTags: MTLQSAR, study_3...    4.0   13.0\n",
      "..                                                 ...    ...    ...\n",
      "994                                                     994.0  495.0\n",
      "995                                                     995.0  496.0\n",
      "996                                                     996.0  497.0\n",
      "997                                                     997.0  498.0\n",
      "998                                                     998.0  499.0\n",
      "\n",
      "[999 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  45%|████▌     | 5/11 [00:04<00:05,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: QSAR-TID-10778\\n\\nTags: MTLQSAR, study_3...    0.0   13.0\n",
      "1    Name: QSAR-TID-14073\\n\\nTags: MTLQSAR, study_3...    1.0   13.0\n",
      "2    Name: QSAR-TID-11589\\n\\nTags: MTLQSAR, study_3...    2.0   13.0\n",
      "3    Name: QSAR-TID-10486\\n\\nTags: MTLQSAR, study_3...    3.0   13.0\n",
      "4    Name: QSAR-TID-10939\\n\\nTags: MTLQSAR, study_3...    4.0   13.0\n",
      "..                                                 ...    ...    ...\n",
      "976                                                     976.0  494.0\n",
      "977                                                     977.0  496.0\n",
      "978                                                     978.0  497.0\n",
      "979                                                     979.0  498.0\n",
      "980                                                     980.0  499.0\n",
      "\n",
      "[981 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  55%|█████▍    | 6/11 [00:05<00:04,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: Moneyball\\n\\nTags: sport, study_130, stu...    0.0  425.0\n",
      "1    Name: Short_Track_Speed_Skating\\n\\nTags: Sport...    1.0  425.0\n",
      "2    Name: gisette\\n\\nTags: \\n\\n**Author**: Isabell...    2.0  429.0\n",
      "3    Name: jungle_chess_2pcs_raw_endgame_complete\\n...    3.0  491.0\n",
      "4    Name: EMNIST_Balanced\\n\\nTags: vision\\n\\nEMNIS...    4.0  487.0\n",
      "..                                                 ...    ...    ...\n",
      "980                                                     980.0  493.0\n",
      "981                                                     981.0  494.0\n",
      "982                                                     982.0  496.0\n",
      "983                                                     983.0  497.0\n",
      "984                                                     984.0  499.0\n",
      "\n",
      "[985 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  64%|██████▎   | 7/11 [00:05<00:03,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: Test_3vectors_num\\n\\nTags: \\n\\nf fr\\n\\nF...    0.0  483.0\n",
      "1    Name: Test_3vectors_num_let\\n\\nTags: \\n\\nsqs e...    1.0  483.0\n",
      "2    Name: Test_5vectors_num\\n\\nTags: \\n\\nb gtrg\\n\\...    2.0  483.0\n",
      "3    Name: airlines\\n\\nTags: \\n\\n**Author**: Albert...    3.0  487.0\n",
      "4    Name: delays_zurich_transport\\n\\nTags: \\n\\nZur...    4.0  487.0\n",
      "..                                                 ...    ...    ...\n",
      "970                                                     970.0  493.0\n",
      "971                                                     971.0  494.0\n",
      "972                                                     972.0  496.0\n",
      "973                                                     973.0  497.0\n",
      "974                                                     974.0  499.0\n",
      "\n",
      "[975 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  73%|███████▎  | 8/11 [00:06<00:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: USA-Airport-Dataset\\n\\nTags: \\n\\nWhat is...    0.0  465.0\n",
      "1    Name: Toronto-COVID-19-Cases\\n\\nTags: \\n\\nThis...    1.0  463.0\n",
      "2    Name: Cosmetics-datasets\\n\\nTags: \\n\\nContext\\...    2.0  446.0\n",
      "3    Name: 2019-Ironman-World-Championship-Results\\...    3.0  458.0\n",
      "4    Name: Pima-Indians-Diabetes-Dataset\\n\\nTags: \\...    4.0  482.0\n",
      "..                                                 ...    ...    ...\n",
      "970                                                     970.0  494.0\n",
      "971                                                     971.0  496.0\n",
      "972                                                     972.0  497.0\n",
      "973                                                     973.0  498.0\n",
      "974                                                     974.0  499.0\n",
      "\n",
      "[975 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  82%|████████▏ | 9/11 [00:07<00:01,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: Intersectional-Bias-Assessment-(Testing-...    0.0  458.0\n",
      "1    Name: Reading_Hydro\\n\\nTags: \\n\\nData from the...    1.0  405.0\n",
      "2    Name: Reading_Hydro_upstream\\n\\nTags: \\n\\nUpst...    2.0  405.0\n",
      "3    Name: Reading_Hydro_upstream\\n\\nTags: \\n\\nUpst...    3.0  405.0\n",
      "4    Name: Reading_Hydro_downstream\\n\\nTags: \\n\\nUp...    4.0  405.0\n",
      "..                                                 ...    ...    ...\n",
      "985                                                     985.0  495.0\n",
      "986                                                     986.0  496.0\n",
      "987                                                     987.0  497.0\n",
      "988                                                     988.0  498.0\n",
      "989                                                     989.0  499.0\n",
      "\n",
      "[990 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data:  91%|█████████ | 10/11 [00:08<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: guillermo_seed_2_nrows_2000_nclasses_10_...    0.0  101.0\n",
      "1    Name: guillermo_seed_3_nrows_2000_nclasses_10_...    1.0  101.0\n",
      "2    Name: guillermo_seed_4_nrows_2000_nclasses_10_...    2.0  101.0\n",
      "3    Name: riccardo_seed_0_nrows_2000_nclasses_10_n...    3.0  101.0\n",
      "4    Name: riccardo_seed_1_nrows_2000_nclasses_10_n...    4.0  101.0\n",
      "..                                                 ...    ...    ...\n",
      "975                                                     975.0  494.0\n",
      "976                                                     976.0  496.0\n",
      "977                                                     977.0  497.0\n",
      "978                                                     978.0  498.0\n",
      "979                                                     979.0  499.0\n",
      "\n",
      "[980 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data: 100%|██████████| 11/11 [00:09<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Document     ID  Topic\n",
      "0    Name: test\\n\\nTags: \\n\\ntest\\n\\nFeatures: Prod...    0.0  290.0\n",
      "1    Name: StudentsPerformance\\n\\nTags: \\n\\nThis da...    1.0  429.0\n",
      "2    Name: DrinkDataset\\n\\nTags: \\n\\nfake dataset w...    2.0  429.0\n",
      "3    Name: algae\\n\\nTags: \\n\\nThis dataset contains...    3.0  443.0\n",
      "4    Name: Amphibian\\n\\nTags: \\n\\namphibians\\n\\nFea...    4.0  284.0\n",
      "..                                                 ...    ...    ...\n",
      "650                                                     650.0  492.0\n",
      "651                                                     651.0  494.0\n",
      "652                                                     652.0  497.0\n",
      "653                                                     653.0  498.0\n",
      "654                                                     654.0  499.0\n",
      "\n",
      "[655 rows x 3 columns]\n",
      "                                              Document     ID  Topic\n",
      "0    Name: anneal\\n\\nTags: study_1, study_14, study...    0.0  133.0\n",
      "1    Name: kr-vs-kp\\n\\nTags: mythbusting_1, OpenML-...    1.0  219.0\n",
      "2    Name: labor\\n\\nTags: mythbusting_1, study_1, s...    2.0  184.0\n",
      "3    Name: arrhythmia\\n\\nTags: sport, study_1, stud...    3.0  247.0\n",
      "4    Name: letter\\n\\nTags: AzurePilot, AzurePilot1,...    4.0  197.0\n",
      "..                                                 ...    ...    ...\n",
      "327  Name: Credit_Score_Classification_downsampled\\...  327.0  212.0\n",
      "328  Name: pollution\\n\\nTags: \\n\\nair quality\\n\\nFe...  328.0  405.0\n",
      "329  Name: IBRD_Loans_Classification\\n\\nTags: \\n\\nT...  329.0  420.0\n",
      "330  Name: Give-Me-Some-Credit-Sampled\\n\\nTags: \\n\\...  330.0  472.0\n",
      "331  Name: Corporate_Credit_Rating\\n\\nTags: \\n\\n# C...  331.0  391.0\n",
      "\n",
      "[5332 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from river import stream, cluster\n",
    "from bertopic.vectorizers import OnlineCountVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from bertopic import BERTopic\n",
    "\n",
    "class River:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def partial_fit(self, embeddings):\n",
    "        for embedding, _ in stream.iter_array(embeddings):\n",
    "            self.model.learn_one(embedding)\n",
    "\n",
    "        labels = []\n",
    "        for embedding, _ in stream.iter_array(embeddings):\n",
    "            label = self.model.predict_one(embedding)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "\n",
    "umap_model = IncrementalPCA()\n",
    "# cluster_model = River(cluster.DBSTREAM())\n",
    "cluster_model = MiniBatchKMeans(n_clusters=500, random_state=0)\n",
    "\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\")\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True, bm25_weighting=True)\n",
    "\n",
    "model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=cluster_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# documents is empty dataframe to which to append the documents\n",
    "documents = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(0, len(data), 500), desc=\"Fitting data\"):\n",
    "    _, docs = model.partial_fit(data[i:i+500], embeddings[i:i+500].astype(np.float64))\n",
    "    documents = pd.concat([documents, docs])\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document    Name: quake\\n\\nTags: \\n\\n**Author**:   \\n**Sou...\n",
       "ID                                                      402.0\n",
       "Topic                                                    68.0\n",
       "Name: 402, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.iloc[402]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m doc_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)) \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m([]))\n\u001b[0;32m----> 2\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m repr_docs_mappings, _, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_extract_representative_docs(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mc_tf_idf_, documents, model\u001b[38;5;241m.\u001b[39mtopics_, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mvp/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda3/envs/mvp/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mvp/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/mvp/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "doc_ids = range(len(data)) if data is not None else range(len([]))\n",
    "documents = pd.DataFrame({\"Document\": data, \"ID\": doc_ids, \"Topic\": model.topics_, \"Image\": []})\n",
    "\n",
    "repr_docs_mappings, _, _, _ = model._extract_representative_docs(\n",
    "    model.c_tf_idf_, documents, model.topics_, 500, 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_umap_model = IncrementalPCA()\n",
    "test_umap_model = UMAP(\n",
    "    n_neighbors=3,\n",
    "    n_components=5,\n",
    "    min_dist=0.008,\n",
    ")\n",
    "test_embeddings = test_umap_model.fit_transform(embeddings)\n",
    "# test_embeddings = test_umap_model.transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster_model = River(cluster.DBSTREAM(\n",
    "    clustering_threshold=0.1,  # Try small values like 0.1-0.3\n",
    "    fading_factor=0.01,       # Small value for more stable clusters\n",
    "    cleanup_interval=20,      # Larger value to prevent frequent cleanup\n",
    "    intersection_factor=0.3,  # Default is good for separation\n",
    "    minimum_weight=1.0        # Keep default unless you want to filter noise\n",
    ")\n",
    ")\n",
    "# test_cluster_model.partial_fit(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting data: 100%|██████████| 1067/1067 [04:29<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(data), 5), desc=\"Fitting data\"):\n",
    "    test_cluster_model.partial_fit(embeddings[i:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 31])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_cluster_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0_stylizedmetaalbumpltdocstyextended_phen_chem...</td>\n",
       "      <td>[stylizedmetaalbumpltdocstyextended, phen, che...</td>\n",
       "      <td>[Name: Stylized_Meta_Album_PLT_DOC_STY_Extende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>1_stock_pulsar_dmsnr_week</td>\n",
       "      <td>[stock, pulsar, dmsnr, week, radio, open, cand...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2_futurebusiness_proceduralfairness_analcatdat...</td>\n",
       "      <td>[futurebusiness, proceduralfairness, analcatda...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3_rabexxx_bertram_schattersternnyuedu_samprit</td>\n",
       "      <td>[rabexxx, bertram, schattersternnyuedu, sampri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4_bnglabornominal1000000_costoflivingadjustmen...</td>\n",
       "      <td>[bnglabornominal1000000, costoflivingadjustmen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>495</td>\n",
       "      <td>107</td>\n",
       "      <td>495_default_vaccinations_vaccination_bioconcen...</td>\n",
       "      <td>[default, vaccinations, vaccination, bioconcen...</td>\n",
       "      <td>[Name: AIDS_Virus_Infection_Prediction\\n\\nTags...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>2</td>\n",
       "      <td>496_admin2_covariatex_fips_provincestate</td>\n",
       "      <td>[admin2, covariatex, fips, provincestate, conf...</td>\n",
       "      <td>[Name: Covid19-us\\n\\nTags: \\n\\nDaily values of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>2</td>\n",
       "      <td>497_logpopulation_total2000_silver2000_bronze2000</td>\n",
       "      <td>[logpopulation, total2000, silver2000, bronze2...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>23</td>\n",
       "      <td>498_filannino_michele_dbworld_addresses</td>\n",
       "      <td>[filannino, michele, dbworld, addresses, bagof...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>2</td>\n",
       "      <td>499_mfeatzernike_fric0100050_colinearity_att27</td>\n",
       "      <td>[mfeatzernike, fric0100050, colinearity, att27...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name  \\\n",
       "0        0      2  0_stylizedmetaalbumpltdocstyextended_phen_chem...   \n",
       "1        1     63                          1_stock_pulsar_dmsnr_week   \n",
       "2        2      1  2_futurebusiness_proceduralfairness_analcatdat...   \n",
       "3        3      9      3_rabexxx_bertram_schattersternnyuedu_samprit   \n",
       "4        4      1  4_bnglabornominal1000000_costoflivingadjustmen...   \n",
       "..     ...    ...                                                ...   \n",
       "495    495    107  495_default_vaccinations_vaccination_bioconcen...   \n",
       "496    496      2           496_admin2_covariatex_fips_provincestate   \n",
       "497    497      2  497_logpopulation_total2000_silver2000_bronze2000   \n",
       "498    498     23            498_filannino_michele_dbworld_addresses   \n",
       "499    499      2     499_mfeatzernike_fric0100050_colinearity_att27   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [stylizedmetaalbumpltdocstyextended, phen, che...   \n",
       "1    [stock, pulsar, dmsnr, week, radio, open, cand...   \n",
       "2    [futurebusiness, proceduralfairness, analcatda...   \n",
       "3    [rabexxx, bertram, schattersternnyuedu, sampri...   \n",
       "4    [bnglabornominal1000000, costoflivingadjustmen...   \n",
       "..                                                 ...   \n",
       "495  [default, vaccinations, vaccination, bioconcen...   \n",
       "496  [admin2, covariatex, fips, provincestate, conf...   \n",
       "497  [logpopulation, total2000, silver2000, bronze2...   \n",
       "498  [filannino, michele, dbworld, addresses, bagof...   \n",
       "499  [mfeatzernike, fric0100050, colinearity, att27...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [Name: Stylized_Meta_Album_PLT_DOC_STY_Extende...  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "495  [Name: AIDS_Virus_Infection_Prediction\\n\\nTags...  \n",
       "496  [Name: Covid19-us\\n\\nTags: \\n\\nDaily values of...  \n",
       "497                                                NaN  \n",
       "498                                                NaN  \n",
       "499                                                NaN  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_representative_docs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 13:43:56,280 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2024-12-06 13:44:07,573 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-12-06 13:44:07,574 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-12-06 13:46:31,604 - BERTopic - Cluster - Completed ✓\n",
      "2024-12-06 13:46:31,623 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-12-06 13:48:31,629 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Only run once\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 1))\n",
    "\n",
    "try:\n",
    "    representation_model = PartOfSpeech(model=\"en_core_web_lg\")\n",
    "    print(\"Model already installed\")\n",
    "except OSError:\n",
    "    # If model isn't installed, download it\n",
    "    print(\"Model not found, downloading...\")\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_lg\")\n",
    "\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=3,\n",
    "    n_components=5,\n",
    "    min_dist=0.008,\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=3,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    min_topic_size=2,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    calculate_probabilities=True,\n",
    "    umap_model=umap_model,\n",
    "    # embedding_model=embedding_model, # don't pass embedding model, just pass embeddings in the next step (pre-computed)\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    ")\n",
    "\n",
    "topics, probs = model.fit_transform(data, embeddings)\n",
    "np.save('topics_checkpoint.npy', topics)\n",
    "np.save('probs_checkpoint.npy', probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model\", serialization=\"safetensors\", save_ctfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTopic.load(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = np.load('topics.npy')\n",
    "# probs = np.load('probs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Based on the below information, extract and synthesize human-readable tags/keywords/themes from the text, capitalized first letters of words. What is the main human-readable theme or subject matter discussed in the provided texts? What is the overarching, high-level theme of the texts, e.g. \"Music\", \"Sports\", \"Environment\", etc.? Please provide overarching themes that tie the different pieces of information together. What is/are the overarching, highest level theme(s) that you could use as a keyword(s)? Prefer single word tags/keywords, e.g. \"Tennis\" rather than \"Tennis Match\", \"Prison\" rather than \"Prison Time\", etc.! Some examples of human-readable themes are   \"Agriculture\", \"Astronomy\", \"Chemistry\", \"Computational Universe\", \"Computer Systems\", \"Climate and Environment\", \"Culture\", \"Demographics\", \"Earth Science\", \"Economics\", \"Education\", \"Engineering\", \"Finance\", \"Geography\", \"Government\", \"Games\", \"Health\", \"History\", \"Human Activities\", \"Images\", \"Language\", \"Law\", \"Life Science\", \"Machine Learning\", \"Manufacturing\", \"Mathematics\", \"Medicine\", \"Meteorology\", \"Physical Sciences\", \"Politics\", \"Social Media\", \"Sociology\", \"Statistics\", \"Text & Literature\",  \"Transportation\". Also, don't give very similar tags/keywords, e.g. \"Wine\" and \"Red Wine\", just give one or the other in these cases. Avoid tags/keywords that are too specific, e.g. \"Serine Threonine Protein Kinase\". Good theme examples are: \"Birds\", \"Species Migration\", \"Air Pollution\", or \"War\", \"Government\", \"International Relations\", \"Politics\". Another important rule to obey - place more focus on the dataset names for theme extraction. And be concise in theme generation, e.g. instead of \"Income Prediction\", say \"Income\", instead of \"Demographic Information\", say \"Demographics\"! Also, extract the theme of the text, what it is about, instead of the type of problem it is, for instance we don't care about \"Regression\", \"Numerical Features\", \"Data Analysis\", \"Data\", \"Outliers\", \"Subsampling\" or things of that sort, but we care about the ESSENCE of the text! Say {\"themes\": [...], \"overarching_themes\": [...]} and give your answer in JSON format.\n",
    "For example, for this text:\n",
    "Text 1: The Biden administration is preparing to roll out a sweeping border executive action as early as Tuesday, according to two sources familiar with the discussions, who cautioned that timing is fluid.\n",
    "\n",
    "White House officials have begun reaching out to mayors who represent cities along the US southern border to potentially join President Joe Biden when he announces the order, two other sources familiar with those conversations said.\n",
    "\n",
    "For weeks, administration officials have been working through an executive action that would dramatically limit migrants’ ability to seek asylum at the US southern border — part of a strategy to try to give Biden the upper hand on one of his Republican rival’s key campaign issues. The action is designed to potentially blunt Republican attacks on border security and preempt former President Donald Trump ahead of the first presidential debate, which will be held on June 27 on CNN.\n",
    "---\n",
    "Text 2: Now that a New York jury has convicted former President Donald Trump of all 34 felony charges of falsifying business records, the next obvious question is: Can a convicted felon run for president?\n",
    "\n",
    "Definitely.\n",
    "\n",
    "Trump meets all three requirements. There is, arguably, another criterion laid out in the 14th Amendment, where it states that no one who has previously taken an oath of office who engages in insurrection can be an officer of the US. But the US Supreme Court ruled earlier this year that Congress would have to pass a special law invoking this prohibition. That’s not happening any time soon.\n",
    "\n",
    "Judge Juan Merchan has scheduled Trump’s sentencing for July 11, which happens to be four days before the start of the Republican National Convention that is scheduled to take place in Milwaukee.\n",
    "a\n",
    "It is technically possible, although perhaps unlikely for a first-time offender, that Trump could be sentenced to prison time.\n",
    "---\n",
    "This would be your answer:\n",
    "{\"themes\": [\"Biden Administration\", \"Border\", \"Executive Action\", \"Asylum\", \"Immigration\", \"Trump\", \"Felony\", \"Business Records\", \"Presidential Campaign\", \"Republican\", \"Debate\", \"Former President\", \"Conviction\", \"Sentencing\", \"Prison\", \"14th Amendment\", \"Insurrection\", \"Supreme Court\", \"Republican National Convention\"], \"overarching_themes\": [\"Politics\", \"Government\", \"Law\", \"Justice\", \"Elections\"]}\n",
    "---\n",
    "Now, the above was just an example. Now, do it for the following text(s), be concise!:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 01:26:50,709 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Only run once\n",
    "new_topics = model.reduce_outliers(data, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "model.update_topics(data, topics=new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "# from extensions import MistralAI\n",
    "from src.bertopic_extensions import MistralAI\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "llm = MistralAI(\n",
    "    client,\n",
    "    prompt=prompt,\n",
    "    model=\"mistral-large-latest\",\n",
    "    nr_docs=100,\n",
    "    max_tokens = 8192,\n",
    "    cache=False,\n",
    "    generator_kwargs={\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_object\"\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "model.update_topics(data, topics=new_topics, representation_model=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model_fine_tuned\", serialization=\"safetensors\", save_ctfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only run this step if the topic_representations aren't updated for some reason (error, etc.)\n",
    "# import ast\n",
    "# import json\n",
    "\n",
    "# with open(\"../src/bertopic_extensions/_cache/mistral.json\") as f:\n",
    "#     mistral = json.load(f)\n",
    "\n",
    "# for i, topic_data in enumerate(mistral):\n",
    "#     themes = [(theme, 1) for theme in topic_data['Themes']]\n",
    "#     overarching_themes = [(theme, 2) for theme in topic_data['Overarching themes']]\n",
    "#     model.topic_representations_[i] = themes + overarching_themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "top_k = 2\n",
    "top_k_topics = []\n",
    "for i in range(len(data)):\n",
    "    top_k_indices = np.argsort(probs[i])[::-1][:top_k]\n",
    "    top_k_indices = [index for index in top_k_indices]\n",
    "    top_k_indices = [index for index in top_k_indices if index != -1]\n",
    "    top_k_topics.append([str(topic) for topic in top_k_indices])\n",
    "\n",
    "model.get_topic_info(int(top_k_topics[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "import json\n",
    "\n",
    "with open('model_fine_tuned/topics.json') as f:\n",
    "    topics_json = json.load(f)\n",
    "\n",
    "topics_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Themes Dictionary:\n",
      "0: Hockey, Teams, Players, Games, Goals, Playoffs, Penalties, Shots, Periods, Season, Stanley Cup, Fans, Trades, Injuries, Captains, Coaches, Standings, Polls\n",
      "1: Car, Cars, V6, Ford, Engine, Mustang, Probe, GT, V8, Gear, Auto, Nissan, Automatic, Airbag, Automobile, Maxima, Autos, Rotor, Opel, Strut, Manta, Buy, Manual, Safety\n",
      "2: Guns, Militia, Firearms, Weapons, Nuclear, Government, Law, Politics, Crime, Safety, Constitution, Rights, Legislation\n",
      "3: Cryptography, Privacy, Security, Encryption, Government, Policy, Data, Freedom, Rights, Organization, NSA, EFF, ACLU, NRA, TEMPEST, RSA, PGP, Export, Law, Constitution, Assembly, Arms, Symposium, Submissions, Systems, Files, Keys\n",
      "4: Electronics, Circuits, Power Supply, Voltage, Current, Digital, Analog, Signal Processing, Measurement, Amplifiers, Components, Electrical Engineering, Data Acquisition, Sound, Noise\n",
      "5: Clipper, Encryption, Algorithm, Government, Chip, Privacy, EXPN, NSA, Proposal, Sternlight, Denning, Csrcncslnistgov, Frog, Crypto, Secure, VRIFY, David, ATT, Secret, Apple, Illegal, Trust, DES, Will, Not, Attacks, Alias, Firstorg, Strnlghtnetcomcom, That\n",
      "6: Bible, God, Jesus, Christian, Life, Hell, Christ, Gehenna, Satan, Hades, Lunatic, Liar, Saved, Believe, Fire, Valley, Prophet, Kendigianism, Scripture, Faith, Belief, Religion, Theology, Spirituality, Afterlife, Salvation\n",
      "7: Morality, Ethics, Values, Philosophy, Debate, Religion, Belief, Culture, Evolution, Society, Atheism, Moral Systems, Objectivity, Subjectivity, Humanism, Social Norms, Authority, Conscience, Moral Driver, Freedom, Terrorism\n",
      "8: Cubs, Mailing List, Box Scores, Twins, Schedule, Habs, Montreal Canadiens, John Franco, Mets, Mel Hall, Yankees, Vegas Odds, World Series, Lynx, Texas Rangers, Tickets, Radio, Playoffs, Stadium, WFAN, John Wetteland, Derek Lilliquist, Indians, All-Star, Fenway Park, J.T. Snow, Angeles, Baseball, Images, Rookies\n",
      "9: Homosexuality, Sexuality, Gay Rights, Discrimination, LGBTQ, Sexual Orientation, Sexual Behavior, Morality, Government, Politics, Society, Culture, Mental Health, Abuse, Surveys, Swimming Pools, Sexual Transmission, Boy Scouts, Tattoos\n",
      "10: ESPN, Baseball, Hockey, Game, Coverage, Overtime, Playoffs, Sports, Teams, Broadcast, NHL\n",
      "11: Space, Moon, SSRT, SSTO, Cost, Lunar, Titan, Launch, Program, Vehicle, Prize, Reward, Billion, Clementine, Ti, Economy, Market, Competition, Industry, Technology, Budget, Engineering, NASA\n",
      "12: FBI, Branch Davidians, Fire, Tear Gas, Asylum, Government, Law Enforcement, Waco, Investigation, Autopsy, Explosion, Gas, Stove, Tank, Electricity, Suicide, Defense\n",
      "13: X11, Gcc, Compilation, Libraries, Linking, Errors, Patches, SunOS, IPX, AIX, Xterm, HP-UX, Motif\n",
      "14: MPEG, Multimedia, Software, Images, TIFF, DXF, HDF, Conversion, Graphics, Utilities, File Formats, Processing, Storage\n",
      "15: Hockey, Equipment, Goalies, Injury, Mask, Tradition, Fans, Players, Teams, Games, NHL\n",
      "16: MenuButton, Widget, Label, Bitmap, XWall, XVersion, XOpenDisplay, Xdm, XView, WindowServer, OpenWindows, XWindows, DrawnButtons, Motif, Knob, X11, SPARCstation, Xserver, XBell, QuicKeys, Macintosh, Keystrokes, MouseActions, Applications, Xlib, Xt, Toolkit, MailingList, Protocol, Programming\n",
      "17: Baseball, Player Performance, Statistics, Clutch Hitting, Defensive Average, RBIs, Injury, Prediction, Positioning, Batting Average, Bo Jackson, Keith Mitchell, Mark Whiten, Gary Gaetti\n",
      "18: Pneumonia, Phimosis, Asthma, Hepatitis, Shingles, Heart Valves, Kidney Stones, Taxes, Morphine, Scarring, ANP, Syringe, Mood Swings\n",
      "19: Encryption, Clipper Chip, Key Escrow, Law Enforcement, Cellular Phones, Algorithm, Security, Government\n",
      "20: Motorcycle, Wheelie, Shaft-Drive, Countersteering, Steering, Braking, Traction, Efficiency, Chain-Drive, Bike, Horse\n",
      "21: Upgrades, CPUs, Computers, Hardware, Speed, Cooling, Modifications, Motherboards, Clock, Oscillators, Temperature, Heatsinks, Performance, Benchmarks\n",
      "22: Government, Military, Law Enforcement, Civil Law, Waco, Branch Davidians, ATF, FBI, Justice, Racism, Kent State, National Guard, President, Politics, Law, Religion, Discrimination, Investigation, Congress, Assassination, JFK\n",
      "23: Driver, Quadra, Centris, Iomega, BBS, Conner, Quantum, IDE, Controller, DOS, BIOS, Floppy, Hard Disk, Boot, Partition, Format, Cache, SmartDrive, HyperDisk, FDISK, Speedstor, DiskManager, Commodore, Compaq, Magneto-Optical, Seagate, XT, AT, MFM, RLL, Debug, Drive Geometry, Low Level Format, Norton, SpeedDisk, Gateways, Promise, Align\n",
      "24: Bible, New Testament, Gospels, Qumran, Scrolls, Ezekiel, Tyre, Prophecy, Historical Documents, Translation, Scholars, Dispensationalism, Plymouth Brethren, Lexicon, Concordance, Scofield, Darby, Young\n",
      "25: Bible, Jesus, God, Law, Christians, Jews, Easter, Deuteronomy, Resurrection, Pharisees, Christ, Grace, Salvation, Faith, Commandments, Prophets\n",
      "26: Drivers, Windows, Printers, Canon, FTP, Graphics, Cards, Chips, Cirrus, Micron, Diamond, Stealth, Viper, Panasonic, BIOS, Benchmarks, Video, RAM, VLB, Winbench, NCR, Trident, TSENG\n",
      "27: FBI, BATF, Koresh, Compound, Fire, Children, Government, Law Enforcement, Investigation, Authority, Religion, Cult, Hostage, Surveillance, Media, Negotiation, Trial, Death, Tragedy, Responsibility, Evidence, Witness, Justice\n",
      "28: God, Religion, Christianity, Atheism, Universe, Existence, Meaning, Philosophy, Revelation, Omnipotence, Pride, Sacrifice, Truth\n",
      "29: Computer Hardware, CPU, Harddisk, Headphones, Printer, Software, Handgun, Synthesizer, Monitor, Modem, Keyboard, Voice Processing\n",
      "30: Ray Tracing, Graphics, Computers, Composites, Seminar, Workstations, Manufacturing, Design, Visualization, Modeling, NASA, Kubota Kenai, SGI Indigo, FTP, 3D Images, Face Animation\n",
      "31: Motorcycles, Riding, Waving, Passengers, Helmets, Routes, Rallies, Travel\n",
      "32: Port, Modem, IRQ, Interrupts, COM, Mouse, Serial, PC, UART, Handshaking, Hardware\n",
      "33: Motorcycle, Riding, Helmet, Biker, Insurance, Drunk, DWI, DoD, Batman, Safety, Law, Journalism, Movie, Transportation, Education, Costs\n",
      "34: Astronomy, Comets, Jupiter, Oort Cloud, Bursts, Energy, Gamma Rays, Observatory, Parallax, Planets, Space\n",
      "35: Polygon, Bezier, Mathematica, Maple, Convex, Concave, Delaunay, Voronoi, Hexagons, 3D, Algorithms, Geometry\n",
      "36: Interviews, Andrew, DesqviewX, Applications, Tools, Audio, VMS, Xavier, Objects, Motif, Consortium, Conference, Toolkits, Papers, DVX, Fresco, PC, Display, Theme, June, Interface, Code, Video, Construction, Sun, Release, Machine, Tutorial, Classes, Demo\n",
      "37: Quotes, Documentary, Chomsky, NPR, Propaganda, Freedom, Speech, University, Politics, History, War, Annexation, Hawaii\n",
      "38: Images, File Formats, Edge Detection, JPEG, Decompression, Software, Graphics, Sub-bands, MPG, IGES, Arc/Info, Filters, Specifications, Graphics Cards, Algorithms, Code\n",
      "39: Baseball, Managers, Players, Injuries, Pitchers, Teams, Trades, Records, Stadiums, Owners, Divisions, Season\n",
      "40: Mary, Conception, Bernadette, Immaculate, Blessed, Priest, Grace, Gifts, Lily, Beautiful, Purity, Garden, Sin, Tree, Rose, Flowers, Lord, Angel\n",
      "41: Computer Systems, Software, Games, Hardware, Sales, Prices, Shipping, Manuals, Technology, DOS, MPC, Apple, Sega, TurboGrafx-16, IBM, Microsoft, AIX-PS/2, Arcade\n",
      "42: Phillies, Baseball, Teams, Games, Scores, Players, Season, Hit, Runs, Inning, Pitching, Pittsburgh, Cincinnati, Dodgers, Cardinals, Braves\n",
      "43: Israel, Palestine, Peace Talks, Jerusalem, Clinton, Mubarak, Arafat, PLO, Occupation, Lebanon, Syria, Golan Heights, War, Terrorism, Settlements, Borders, United Nations, Negotiations\n",
      "44: Communication, Email, Mailing List, Book Review, System Crash, Information Exchange, Subscription\n",
      "45: Clipper Chip, Clinton Administration, Encryption, NSA, FBI, Surveillance, Privacy, Constitution, Rights, Democracy, Technology, Government, Law, Politics\n",
      "46: Israel, Palestinian, Jews, Arab, Occupation, Human Rights, Zionism, PLO, Hamas, Intifada, Gaza, Negev, Bedouin, Citizenship, Israeli, Palestine, Assistance, United Nations, Development, Economics, Judaism, Islam, Refugees, History, Territory, State, International Law\n",
      "47: Furniture, Sale, Window Shades, Dining Table, Books, Chemistry, Bicycle, Microwave, Beekeeping, Ring, Star Trek, Ornaments\n",
      "48: SCSI, Drive, IDE, CD-ROM, Controller, Partition, BIOS, Floptical, Hardware, Performance, Swap File, Tech Support, BBS\n",
      "49: Baseball, Games, Players, Umpires, Rules, Hall of Fame, Fans, Gender\n",
      "50: Monitor, Transmitter, GPS, Wiring, Distance, Receiver, House, Tempest, Power, Radiation, Equipment, Interference, Plate, GHz, FCC, Signal, Filters, Engine, Problem, Laptop\n",
      "51: Hockey, NHL, Canadians, Europeans, Teams, Players, Fans, Expansion, Broadcasting\n",
      "52: Button, Focus, Window, Dialog, Event, Keyboard, Pointer, Shell, Colormap, Application, Popup, Transient, Cursor, Mask, Flags, Display\n",
      "53: Flamebait, Asshole, Scooter, Bike, Spelling, Stereotype, Geekism, Tattoo, Razzing, Picture, Education, Israeli, Pilot, Lebanon, School, Children, Bombing, Humor\n",
      "54: MSG, Food, Glutamate, Additives, Causes, Effects, Restaurant, Cook, Studies, Amounts, Amino, Steak, Olneys, Intraventricular, Brain, Literature, Chen, Milk, Mice, Humans, Infant, Natural, People\n",
      "55: Radar, Detector, Traffic, Police, Law, Court, Sensors, CalTrans, Cars, Freeways, RFI, Audio, Receivers, Manitoba\n",
      "56: Apple, PowerBook, Duo, Pricing, Hardware, Memory, Portable, Education, Discount, Sales, Upgrade\n",
      "57: Evolution, Science, Theory, Gravity, Atoms, Glass, Existence, God, Faith, Universe, Spacetime, Quarks, Plates, Life, Creation, Question, Answer, Reason, Fact, Nothing, Why\n",
      "58: Faith, God, Jesus, Love, Salvation, Scripture, Belief, Creed, Truth, Purification, Family, Heaven, Sin\n",
      "59: Muslims, Bosnian, Serbs, Croats, Croatia, Bosnia, Secession, Yugoslavia, Rally, War, Tibetans, Motivated, Genocide, Islam, Political, Empire, Regime, Chinese, Ethnic, Egypt, Religion, Conflict, Protest\n",
      "60: Icons, Windows, BMP, SPSS, Files, RLE, WINCOM, Font, Wallpaper, Program, DOS\n",
      "61: Networks, Windows, Novell, DOS, Drivers, Hardware, Upgrade, NT, Stacker, Workgroups, ODI, NDIS, Workplace, LAN\n",
      "62: Harvest, Disposal, Palms, Ocean, Airport, Power Lines, Airplanes, Embassy, Film, Animation, Colors, Flags, Reading, Quakes, History\n",
      "63: Programming, X Windows, Chip Design, Computer Architecture, Frequency Data, Operations, X-Windows Server, Windows, Ctl-Alt-Del, Xterm, X Window Servers, MS-Windows, Cursor, Keyboard, XView, Text Rotation, Pixel Value, Drawable, Network Traffic, X Apps, Curses, Telnet, Keystrokes\n",
      "64: Senators, Oilers, Move, Northlands, Pocklington, Montreal, Team, Pick, Franchise, Lindros, Daigle, Roy, Draft, Ottawa, Philadelphia\n",
      "65: Abortion, Rape, Viability, Christianity, Drugs, Responsibility, Law, Politics, Media, Ethics, Health, Justice, Religion, Society, Culture\n",
      "66: Mormons, Mountain Meadows Massacre, Brigham Young, Discovery Channel, Kenny Rogers, Television, Coptic Church, Monophysitism, Council of Chalcedon, Orthodox Church, Religion, Philosophy, Atheism, Christianity, Protestant, Catholic, Church, Doctrine, Faith, Bible, Theology, Eucharist, Chalcedon, Denominations, Jesus, God, Scripture, Worship, Belief, History, Culture\n",
      "67: Kirlian, Photography, Energy, Biology, Transmutations, Minerals, Paranormal, Auras, Science, Experiment, Physics\n",
      "68: Art, GIF, Algorithm, Globe, Map Projections, Smithsonian, Cartography, Graphics, PC, Earth, Aorta, Heart, Medicine, Clinical Trial, Press Conference, GeoSphere, Sunrise, Sunset, Astronomy, Blood Flow\n",
      "69: GNU, COPYLEFT, SIMULOG, Xv, ImageMagick, Shell-Script, Newsgroups, Comp.text.interleaf, Alt, Netiquette, Taoism, Misc.taoism, Philosophy, Metaphysics, Graphics, Computer Systems, Newsgroup Split, Forum\n",
      "70: Sound Blaster, DMAs, Monitor, VGA, RGB, Sync, Printer, Driver, Cable, Microswitch, Mouse, BBS, Gateway2000, NeXT, Apple, Okidata, Atari, LCII, MacClassic\n",
      "71: Atheism, Religion, Christianity, Debate, Belief, Fanaticism, Motto, Propaganda, Court, Abortion, Racism, Psychology, Education, Silence, Prayer, School\n",
      "72: Medicine, Health, Pediatrics, Osteopathy, Doctors, Patients, Alternative Medicine, Psychology, Science, Research, Methodology, Treatment, Clinical Studies, Placebo Effect\n",
      "73: Judaism, Christianity, Islam, Fundamentalism, Politics, Racism, Holocaust, Nazi, Hitler, Antisemitism, Religion, Israel, Palestine, Zionism, Government, History, Cabinet, Proselytism, WWII\n",
      "74: Car, Vehicle, Corvette, Camaro, Alarm, Sensor, Shock, Viper, Oil, Seat, Cigarette, Tank, Convertible, Pressure, Aftermarket, Syclone, Coolant, Thief, Alpine, Hood, AC, Feature, Cup, Ashtray\n",
      "75: Color, Colormap, RGB, XLib, VGA, Mode, Image, Monitor, Depth, XView, HP, Workstation, Pixel\n",
      "76: Space, Astronomy, NASA, Usenet, Satellite, Shuttle, Mission, Solar, Telescope, Astrophysics, Education, Research, Technology, Network, Communication, Science, Inca, Greenhouse, Agriculture, Alaska, Magazine, Publishing, Discussion, E-mail, Listserv, BITNET, FTP, Digest, Subscription, Archive, Mailing List\n",
      "77: Armenians, Russian, Turkey, Turks, Genocide, Massacre, War, History, Population, Muslim, Soldiers, Villages, Armenia\n",
      "78: Convertible, Vinyl, Roof, Trim, Hailstorm, Integra, Insurance, Companies, Zip, Charge, AAA, Allstate, State Farm\n",
      "79: Electronics, Circuits, Radio, Transistors, Microcontroller, Universities, Engineering, Television, Frequency, Modulation, Remote Control, Spread Spectrum, Amplifier, Demodulator, Receiver, Mixer, Catalog, Education\n",
      "80: Computer, Monitor, Software, Hardware, Electronics, Multimedia, Graphics, Gaming, Technology, Hardware Manufacturers, System Software\n",
      "81: DES, Encryption, Cryptography, Cryptanalysis, Keys, Cipher, Plaintext, Ciphertext, Brute-Force, Authentication, One-Time Pad, Diffie-Hellman, Error Correction, Data Link, Key Exchange, Security, Cryptosystems\n",
      "82: Drugs, LSD, Alcohol, Marijuana, Smuggling, War On Drugs, Birth Defects, Statistics, DEA\n",
      "83: Baseball, Predictions, Players, Teams, Statistics, Trades, Contracts, Performance, Season, Division, League, Giants, Yankees, Orioles, Mets, Braves, Bonds, Batting Order, OPS, ERA, Pitching, Fielding, Home Runs, Injuries, Cy Young, MVP, World Series\n",
      "84: Windows, NT, Microsoft, Sun, Graphics, Images, Programming, Tutorials, X Windows, Accessibility\n",
      "85: Computer, Battery, BIOS, CMOS, Keyboard, Duo, LC IIs, LaserWriter, Quadra, Mac Plus, Compaq, DeskPro, System, Reboot, Password, Settings\n",
      "86: Saudi, Iraq, Arabia, Saddam, Kuwait, Hussein, Arms, Bombing, Gulf, Casualties, Malaysia, Sanctions, Colonial, Invade, Yemen, Nehru, Citizens, Smart\n",
      "87: Space, Shuttle, Software, Engineering, Solar Arrays, HST, Mars Observer, Spacecraft, Design, Mission, Telemetry, Antenna, Orbit, Thruster\n",
      "88: Law, Judiciary, Trials, Evidence, Witnesses, Jury, Verdict, Civil Rights, Human Rights, Constitution, Police, Crime, Government\n",
      "89: Printer, PostScript, PCX, TIFF, Windows, Emacs, Latex, Fonts, Casady & Greene, Word, PrintScreen, Image, Photoshop, Quark, Laser, ASCII, EPS, VGA\n",
      "90: Equalizer, Receiver, Speakers, Tape Deck, CarDiscman, Subwoofers, VCR, Amp, Stereo, Warranty, Sale, Condition, Features\n",
      "91: Allergy, Rhinitis, Vasomotor, Nose, Skin, Dryness, Treatment, Medication, Nosebleeds, Muscle, Neurons, Biopsy, Mucus, Health, Disease\n",
      "92: Bikes, Motorcycles, BMW, Honda, Kawasaki, Insurance, Maintenance, Purchasing, Models, Parts, Dealerships, Registration\n",
      "93: Scientology, Rosicrucian, OTO, Freemasonry, Baptist, Aryan, Mormonism, Mysticism, Metaphysical, Toleration, Christianity, Religion, Ancient, Culture, History\n",
      "94: Jews, Israel, Jerusalem, Zionism, Arabs, Religion, Antisemitism, History, Politics, Culture, Conflict\n",
      "95: Oil, Fuel, Injector, Drain, Plug, Miles, Cleaning, Products, Leak, Suction, Remove, Cleaner, Maintenance\n",
      "96: Baseball, Players, Fielding, Batting, Statistics, Home Runs, Pitching, Defense, Offense, Mattingly, Alomar, Baerga\n",
      "97: Rockets, Propulsion, Aerospace, Engineering, Regulations, Safety\n",
      "98: X, Window, Manager, Twm, Icon, Directory, File, Position, Title, Xterm, Escape, Sequence, Shell, Command, Prompt\n",
      "99: Graphics, Software, Tools, Animator, Widgets, Hypertext, Data, Images, Music, Film\n",
      "100: Network, Email, TCP, DDE, Application, Windows, Internet, Client, Server, Macro, Beta, Connect, Character, Transfer, Remote\n",
      "101: Honda, Mailing List, VW, Corrado, VR6, T-Bird, Club, Pontiac, Bonneville, Used Car, Integra, Torque, Volvo, Scandanavian, Tour, Reflector, BMW, Luggage, Fenders, Car\n",
      "102: Modem, Organizer, SIMMS, Macintosh, ROM, Arcade, Joysticks, Typewriter, Computer, Hardware, Software, Electronics, Technology, Sale\n",
      "103: SIMMs, Apple, Mac, Selling, Deals, Computer Equipment, Newsgroups, Dana Weick\n",
      "104: Sky, Night, Billboards, Astronomy, Light, Dark, Commercials, Rights, Law, Zoning, Aesthetic\n",
      "105: Engineering, Programming, Computer, Course, Exams, Degree, Education\n",
      "106: ATF, FBI, Warrant, Agents, Surprise, Gunfire, Responsibility, Standoff, Raid, Federal, Outcome, Agency, Missile, CNN, Baghdad, Networks, Waco, Tragedy\n",
      "107: Mouse, Driver, Windows, Graphics, Video, FileManager, Display, Cursor, XFree86, Network\n",
      "108: Graphics Card, Monitor, Resolution, Driver, ATI, Cirrus Logic, Windows, Gateway, AutoCAD, GIFs, Excel, VL-Bus, Customer Support\n",
      "109: Hardware, Connections, Cables, Interfaces, Keyboards, Adapters, Floppy Drives, Motherboards, Pinouts, Serial, RS232, Disk, Expansion Chassis, Tape Drives, Mac, PC\n",
      "110: Yamaha, Seca, Turbo, Motorcycle, Honda, Clutch, Prelude, Accord, Civic, Kawasaki, VW, Hyundai, Audi, 5000S, Chevrolet, Nova, MG, Bronco, Rust, Paint, Axle, Offroad, Car, Buy\n",
      "111: FBI, Branch Davidians, Fire, Waco, Tanks, Suicide, BATF, Gas, Kerosene, 911, Investigation, Media, Negligence, Survivors, Coordination, Evidence, Electricity, Lamps, Russotto, Mitchell, ABC, Accelerant\n",
      "112: Religion, Christianity, Islam, Blasphemy, Rushdie, Fatwa, East Timor, Democracy, Persecution, Neo-Nazis, White Supremacists, Judaism, Belief, Atrocities, Terrorism\n",
      "113: Hockey, Expansion, Franchise, NHL, Division, Playoff, Milwaukee, Miami, Hispanic, Community, Broadcast\n",
      "114: Accessbus, Protocol, I2C, Bus, Devices, Host, EISA, Macplus, Peripheral, PC, DMA, PDT, Motherboard, Locators, VLB, Hint, Chipset, Technology, Device, Layer, Physical, Quit, Defines\n",
      "115: Wiretapping, Clipper, FBI, EFF, ACLU, CPSR, FOIA, Surveillance, Privacy, Encryption, Telephony, Law Enforcement, Court Orders, Pen Registers, Trap and Trace, Emergency, Investigation\n",
      "116: DOS, DoubleSpace, Stacker, Windows, MSDOS, DRDOS, SuperStor, Defrag, Norton, Speedisk, Backup, Memory, Help, Sector, Corruption, Errors, Reboot, Directory, Research, Paper, Article, Problems, Date, Time, CMOS, Battery, Menu, System, Hardware\n",
      "117: Tickets, Travel, Airlines, Certificates, Vouchers, Round-trip, Transferable, Expiration, Destinations, Offers\n",
      "118: Hockey, Bruins, Boston, NHL, Playoffs, Games, Coverage, Television, Radio, Stations, ESPN, ABC, TSN, Teams, Expansion, Scores, Season\n",
      "119: Communication, Debate, Books, Environment, Logic, Behavior, Climate\n",
      "120: Tax, Income, Government, Constitution, Education, Economy, Power, Country, Congress, Money, State, Federal, Republican, Democratic, Libertarians, Veto, Majority\n",
      "121: BATF, FBI, Waco, Branch Davidians, Assault, Raid, Weapons, Compound, Warrant, Investigation, Government, Law Enforcement, Congress, Janet Reno, David Koresh, Propaganda, Justice, Legislation\n",
      "122: FTP, Download, Telnet, X-Face, CompuServe, Shareware, Newsgroup, RenderMan, Photoshop, Games\n",
      "123: Monitor, MX15F, ViewSonic, Trinitron, T560i, F550i, F550iW, Quadra, SCSI, Resolution, VRAM, IO, Processor, Display\n",
      "124: Copy Protection, Encryption, Software, Security, Hardware, Piracy, Backup, Documentation, Support\n",
      "125: Newsgroup, Algorithm, PRK, RK, Contacts, Glasses, Vision, Cornea, Surgery, Laser, Eye, Hacking, Computer Science, Papers\n",
      "126: Baseball, Injury, Rangers, Orioles, Braves, Maddux, Cubs, Underdogs, Fans, World Series, Players, Pitching, Winning, Games, Talent\n",
      "127: Mac, Drives, Multisession, Disks, Sony, SCSI, Disc, MDs, CD300, Flopticals, Optical, Floppy, CDs, Timing, Devices, Apple, Handshake, Blind, EtherLAN, PhotoCD, Firmware\n",
      "128: Government, Libertarian, Society, Freedom, Slavery, Capitalism, Regulation, Economy, Labor, South, North\n",
      "129: Magellan, Venus, Spacecraft, Gravity, Aerobraking, Galileo, Command, Timer, No-Op, Antenna, Flyby, Sun, Sunshade, Manuevers\n",
      "130: Health, Disease, Medication, Lyme, Hypoglycemia, Thyroid, Hormone, Tinnitus, Doctor, Diagnosis, Haldol, Drugs\n",
      "131: Tunnel Diode, SPICE, Astronautics, Shuttle, Orbit, Cross-Sectional Area, SRBs, External Tank, Sounding Rockets, Thruster, Aircraft, Pilot, Acceleration, Maneuvering, Helmets\n",
      "132: Atheism, Theism, Arrogance, Stalin, Faith, Hard Atheism, Evidence, Dogma, Irrationality, Philosophers, Religion, Belief, Theory, Assumption, Condescension, Mass Suicides\n",
      "133: Armenia, Turkey, Karabakh, Cyprus, Azerbaijan, Greece, Massacre, Conflict, Refugees, War, History, Politics, Ethnicity, Human Rights\n",
      "134: Dogs, Bikers, Weapons, Louisiana, Chase, Bike, Helmet, Paint, Street, Party, Stereotype\n",
      "135: Baptism, Sin, Faith, Children, Parents, Ezekiel, Catholics, LDS, Adam, Grace, Bible, Heaven, Guilt, Soul, Teach, Responsible\n",
      "136: Mac, Powerbook, RAM, Memory, Upgrade, Modem, CPU, FFT, Processor, Accelerator, SIMM, IDE, AC, Current, PowerPC, Warranty, Apple, Hardware\n",
      "137: Tariffs, Subsidies, Economy, Trade, Protectionism, Japan, Tax, Deficit, GST, VAT, Immunization, Health, Bailout, Manufacturing, Jobs, Politics, Government\n",
      "138: Religion, History, Politics, Conflict, Islam, Judaism, Christianity, Nazism, Fundamentalism, Middle East, Europe, Law, Society, Culture\n",
      "139: VGA, CRTC, EGA, XGA, VESA, IBM, DOS, Windows, Unix, Tseng Labs, ET4000, Quadra, PowerBook, Mac, Stereo, Sound, Games, Registers, Video, Sync, Mouse\n",
      "140: Car, Vehicle, Sale, Used, Maintenance, Miles, Price, Model, Year, Manufacturer, Features, Condition\n",
      "141: Solvents, Adhesives, Glass, Stickers, Corrosion, Carcinogens, Benzene, Acetone, Goof-off, Duct Tape, Carpet, Battery Terminals, Appliances, Lampposts\n",
      "142: Newspaper, Copyright, Dream, Vehicle, Money, Opinion, Humor, Politics, Transaction, Trust, Fraud, Commerce\n",
      "143: Garage Door, GRE, Test Preparation, Economics, Playboy, Magazines, Nintendo, Games, Records, Music, Ultima, Game Boy\n",
      "144: BMW, Clutch, MR2, Engine, Oil, Noise, Performance, Mercury Capri, Gas Mileage, Idling, Carburetor, Cylinder, Pistons, Wear, Bike, Honda, Smoke, Maintenance\n",
      "145: Health, Insurance, Canada, Private, Care, Doctors, Treatment, Services, System, American, Spending, Population, Money, Grants, Clinic\n",
      "146: Monitor, Card, EGA, VGA, 8-bit, SS2, 1024x748, 8514/A, PS2/70, Micron, Cobra, CGA, DayStar, PowerCache, SE/30, 040, Accelerator, CPU, Radius, Precision Color Pivot\n",
      "147: WordPerfect, Windows, Memory, Disk Space, Installation, Mathcad, Swap File, Applications, Resources\n",
      "148: DWI, BATF, Waco, Prestige, DoD, Ad, USA Today, Cartoon\n",
      "149: Universe, Motion, Light, Physics, Astrophysics, Galaxies, Stars, Quasars, Antimatter, Parallel Universes, Mind, Consciousness, Particles, Waves, Quantum Electrodynamics, Feynman, Larson, Theory, Book, Wolf, Bottles, Beer, Hole Currents, Semiconductors, Electronics, Astronomers, Supernova, Gamma Ray Bursts\n",
      "150: Christianity, Homosexuality, Interpretation, Bible, Communion, Ritual, Leviticus, Paul, Faith, Gospel, Tradition, Ethics\n",
      "151: Braves, Boston Red Sox, Mariners, Yankees, Baseball, Pitcher, Record, Injury, Fans, Season, Win, Run, Game\n",
      "152: Jesus, Resurrection, Disciples, Gospel, Matthew, Mark, Luke, John, Apostles, Eliyah, Christianity, Bible, Faith, Belief\n",
      "153: Motorcycles, Helmets, Safety, Switches, Wiring, Painting, Wheelchairs, Insurance, No-Fault, Accidents\n",
      "154: Ethernet, Card, Mac, LC, PDS, Connector, Fan, Volt, Watt, Switch, Vram, Chip, Board, Accelerator\n",
      "155: SIGKids, Research, Showcase, SIGGRAPH, Comic, B.C., Johnny Hart, Mitsumi, Toxoplasmosis, Cats, Pregnancy, Health, Literature, Addresses\n",
      "156: Medical, Card, Info, Hospital, Security, Emergency, Notification, Criminal, Records, Public, Access, Policy\n",
      "157: Bike, Suspension, Corner, Turn, Engine, Carb, Screws, Jacket, Aerostich, Leather, Protection, Layers\n",
      "158: Astronomy, Solar System, Planets, Pluto, Charon, Atmosphere, Oxygen, Earth, Poles, Sunlight, Orbit, Missions, Monitoring, Shadow, Photosynthesis, Sediments, Weathering, Habitability\n",
      "159: Politics, Elections, Vice Presidency, Candidates, Parties, Booing, Ceremonial Events, Sports, Sports Fans, Libertarianism, Philosophy, Dissent, Law Reform\n",
      "160: Crime, Surveillance, Clearance, Government, Politics, Organizations, Elections, Military, Internet, Communication, Legislation, Activism\n",
      "161: Jesus, Son, God, Trinity, Suffering, Atonement, Salvation, Persecution, Resurrection, Faith, Conspiracy, Tradition, Incarnation, Logos, Virgin Birth, Disciples, Martyrdom\n",
      "162: XTerm, Stty, Dither, Pixmaps, Sliders, XView, Wordwrap, XDrawImageString, Bitmap, Graphics\n",
      "163: Taxes, Deficit, Revenue, Investors, Clinton, Reagan, Bush, Government, Politics, Economics\n",
      "164: Space, Surveillance, Satellite, Optical, Radar, Orbital, Debris, Water, Monitoring, Observation, Military, Resources, Geography, Technology\n",
      "165: Sci-Fi Books, Star Wars, Star Trek, Dune, Alien, Comics, Marvel, DC, Spider-Man, Hulk, Wolverine, X-Force\n",
      "166: Fans, Teams, Management, Respect, Loyalty, Ticket Prices, Attendance, Salaries, Contracts, Owners, Revenue, Performance, Sports, Baseball, Hockey\n",
      "167: Religion, Mormon, Polygamy, Prayer, Fatima, Ceremonies, LDS, Contradiction, Scripture, City Hall, Event\n",
      "168: Windows, Network, OS/2, NT, Files, SAS, Batch, IBM, Local, Users, Administrators, File, Microsoft, Installation, Run, Crash, Apps, User, Hard, Job, VC, Sparc, C7, Workstation, Machine, Backup, Icon, Disks, Installed, Application\n",
      "169: Vitamins, Medicine, Disease, Infection, Patients, Treatment, Yeast, Candida, Vaccine, Nutrition, Study, Lyme, Health, Weight, Chromium, Insulin, Fat, Muscle, Metabolism, Colon, Cleansing, Infections, Dilation, Clinic, Yogurt, PVCs\n",
      "170: Windows, MS-DOS, Software, Installation, Compatibility, Error, Memory, Hardware, Driver, Configuration\n",
      "171: Battery, BMW, Motorcycles, Maintenance, Insurance, For Sale, Tires\n",
      "172: Circuit Cellar Inc, ICs, Data Books, CD-ROM, DSP, Vocoder, 8051, Assemblers, BBS, Netlist, Schematic, FTP, Download, Software, Microprocessor, Compiler\n",
      "173: Religion, God, Worship, Hell, Prayers, Judgment, Sinners, Faith, Belief, Christianity, Atheism, Bible, Oath\n",
      "174: Spacecraft, Navigation, Space Station, Budget, NASA, Russia, Redesign, Launch, Astronauts, HST, Thruster, Reboost, Tools, Safety\n",
      "175: Cults, Baptists, Religion, Government, Constitution, Clinton, Koresh, Messiah, Sociopaths, FBI, BATF, Waco, Federal Agencies, Weapons\n",
      "176: Paradox, Database, Access, Borland, File, BinHex, Bin, MacBinary, Approach, Records, Windows, RDBMS, Creation, Newsgroup, RFD\n",
      "177: Firearms, Weapons, Gangs, Sniper, Terrorism, Bombs, Chemical Weapons, Bio-weapons, Military, Civilians, Safety, Protection, Law Enforcement\n",
      "178: Driving, Vehicles, Traffic, Safety, Law, Society, Karma, Battery, Cement, Dirt, Temperature, Speed, Membership, Attorneys, Judicial System, Crime, Violence, Youth\n",
      "179: Friendship, God, Belief, Faith, Religion, Christianity, Proselytizing, Fundamentalism, Spirituality, Truth, Love, Tolerance, Judgment, Sin, Prayer, Holy Spirit, Conversion\n",
      "180: X, XmText, GXxor, GXequiv, Keystroke, Beep, Widget, Date, Code, Application, Callback, Key, Display, Graphics, Interface, Programming\n",
      "181: Sony, TV, Scanner, Amp, Detector, VCR, Cassette Deck, Generator, Tube Tester, Lawn Spreader, Realistic, Electronics, Equipment, Sale, Offer, Shipping, Payment\n",
      "182: Baseball, Players, Teams, Performance, Injuries, Trades, Contracts, Strategies, Season, League, Catchers, Defense, Offense, Roster, Management\n",
      "183: XWindows, Device Driver, DOS, Windows, X Servers, Desqview, XVision, Public Domain, Shareware, Commercial, Pricing, Incompatibilities, Window Managers, Imaging Workstations, High Resolution Graphics, Fax, Network Interface, Mouse, Print Support, Graphs, Bar Charts, Xwd, Xpr, Fax Software, XVideo, X Motif, GUI, SGI, Forms\n",
      "184: Atheism, Bible, Contradictions, Ezekiel, Holiness, Christianity, Priests, Swearing, Book, Second Coming, Harold Camping\n",
      "185: Oscillators, Circuits, LEDs, Flashing, Frequency, Hertz, Chips, Testing, Packaging, Decoders, Demultiplexers, High-Speed, Electronics\n",
      "186: Phenylketonuria, Phenylalanine, Seizures, Neurological Damage, Genetic Disorder, Kartagener's Syndrome, Immotile Cilia Syndrome, Situs Inversus, Bronchiectasis, Infections, Infantile Spasms, Hyperactivity, Diet, Feingold Diet, Artificial Coloring, Artificial Flavoring, Corn, Tryptophan, Aflatoxin\n",
      "187: Multiverse, Software, Virtual Reality, Clients, Servers, Objects, Collision, Detection, Platforms, Compilation, Functionality, Extensions, Users, Events, Rendering, Shading, Support, Portability\n",
      "188: Migraines, Pain, Surgery, Estrogen, Endometriosis, Asthma, Sinusitis, Medication, Headache, Back Pain, Pelvic Pain\n",
      "189: Yamaha, Honda, Seca, Turbo, Virago, Shadow, Motorcycles, Bikes, Exhaust, Tires, Engine, Power, Records, Brooklands, Bert Le Vack, Illustrated Encyclopedia\n",
      "190: LEDs, Blue, Boards, Solder, Mask, Green, Red, Soldering, Electronics, Fiberglass, Phenolic, Bakelite, Circuits, Components, Manufacturing\n",
      "191: CPU, Coprocessor, Motherboard, RAM, Cache, BIOS, UART, ROM, CAD, AutoCAD, 3D Studio, Weitek, Intel, Motorola, Sale, Shipping\n",
      "192: Computer, Programming, Hardware, Software, Finance, Automotive, Education, Writing, Help\n",
      "193: Christianity, Homosexuality, Marriage, Bible, Sin, Church, Interpretation, Scripture, Faith, Morality, Belief, Dogma, Wedding, Religion\n",
      "194: Cars, Pricing, Reliability, Maintenance, Dealers, Honda, Civic, Saturn, SC1, SC2, Japanese Cars\n",
      "195: Fixed-Point Math, C Library, C++ Class, RGB, HLS, HSV, Conversion, Developable Surface, GL Source Code, PALs, GALs, VGA Graphics, Programming, Mathematics, Graphics, Hardware\n",
      "196: Valves, Shims, Clearance, Wear, Engines, Overhaul, Slick 50, Tractor, Lawnmowers, Idle, Temperature\n",
      "197: Homosexuality, Christianity, Ex-Gay, Sexuality, Relationships, Church, Ministry, Bible, Sociology, Culture, LGBTQ\n",
      "198: Space, Russia, Defense, Orbit, Satellite, Advertising, Disease, Research, Technology, Temperature, Blackbody, Microwave, Astronomy, Film, Security, CDC, Radiation, Government\n",
      "199: Motorola, BBS, Telephony, LED, Zener, Wires, Jacks, Phone, Line, Hook, Ring, Caller ID, Synthesized Voice\n",
      "200: Clinton, NCAA, Hockey, Championship, Maine, Sports, Senate, Government, Waco, FBI, Politics, Education, Stimulus, Deficit, Republicans\n",
      "201: Atheism, Religion, Books, Belief, God, Existence, Faith, Agnosticism, Philosophy, Christianity, Catholicism\n",
      "202: Clancy, Nuclear, Bombs, Media, CNN, Advertisements, Guns, Traders, Politics, Stock, Shareholders, Censorship\n",
      "203: Depression, Radio, TV, Fossil Fuels, Air Travel, Processed Bread, X-Ray, Radiation, Chest, Law, Legislator, FDA, Blood, Donation, Malaria, Platelet, Organ, Secretary, State, Meningitis, Contagious, Brain, Infant, Fever, Hospital, Spinal Tap, Carriers, Medication, Throat\n",
      "204: Sports, Hockey, Baseball, Players, Teams, Games, Finals, Wins, Positions, Rings\n",
      "205: Latin, Email, Discussion Group, Language, Classical Latin, Medieval Latin, NeoLatin, Corrections, Procedure, Health, Medicine, Law, Court, Claims\n",
      "206: Iran, Terrorism, USIA, Foreign Policy, International Relations, Sanctions, Claims, Tribunal, Assets, Education, Broadcasting, Government, Politics, Law\n",
      "207: President, Reno, Federal, Attorney, Compound, FBI, Waco, Tragedy, Children, Government, Investigation, Law Enforcement, Clinton, Koresh\n",
      "208: Vehicles, Driving, Speed, Safety, Clothing, Gender, Motorcycles, Manufacturing, Legislation\n",
      "209: Space, Solar Sails, Technology, Medicine, Drugs, Penicillin, Centoxin, Teflon, Calculators, Pacemakers, Velcro\n",
      "210: Religion, Faith, God, Love, Moses, Prayer, Bible, Family, Marriage, Islam, Allah, Muhammad, Prophethood, Guidance, Worship\n",
      "211: Hard Drive, Storage, Upgrade, Sale, Purchase, Mac, IDE, Western Digital\n",
      "212: MacPortable, Hard Drive, Modem, Battery, Adaptec, Controller, ESDI, MFM, RLL, Perstor, Floppy, SE/30, Seagate, IDE, Fault, Batch, Model\n",
      "213: Graphics, Library, C, C++, Xlib, Xt, Xm, Khoros, SGIs, Sun IPX, UNIX, Visual Numerics, QuickDraw, Gouroud, X, PEX, OpenGL, POVRay, XWindows, OpenVMS, DEC Alpha, X256q\n",
      "214: NHL, Minnesota, North Stars, Hockey, Franchise, Sports, Teams, Fans, Business School, Canada, Ontario, Rivalry, Sports Teams, Leafs, Toronto, College, Education, AFC, Champions\n",
      "215: Odometer, Fraud, Car, Dealer, Mileage, Refund, Maintenance, Owner, Court, Fine, Mercedes\n",
      "216: PowerPC, NuBus, FPU, Apple, LCIII, Heat, Coprocessor, Centris, 610, Heat Sink, CPU, IIvx, 040, Slot, 486, Macs, PDS, Ethernet, Board, RocketShare, CPUs, 68LC040, SyQuest, Socket, Station, Chip, SIMM\n",
      "217: Christianity, Koresh, Davidians, Cult, Children, Government, FBI, ATF, Tragedy, Waco, Prophecy, Belief, Movement, Suffering, Assault, Suicide\n",
      "218: Children, Branch Davidians, FBI, BATF, Siege, Custody, Lawsuit\n",
      "219: VHS, Movie, Sale, Playmation, Fitness, Membership, Raquetball\n",
      "220: DOS, DoubleDisk, Stacker, MIRROR, Commands, Backup, CView, Floppy Disks, Files, Directory, FAT, Windows, Norton, WordPerfect, Hard Drive, Smartdrv, Errors, Cache, XCopy, Disk Technician, Sectors\n",
      "221: Quadra, Clock, Acceleration, Graphics, Cards, Benchmarks, Performance, VGA, BNC, Premiere, QuickTime, Playback, Movie, Frames, Scaling, Speed, Tests, Machbanding\n",
      "222: Arbitration, Automotive, Vehicles, Lemon Law, Repurchase, Dealership, Sales, Negotiation, Customer Service, Pricing, Manufacturer, Transmission, Mazda, Nissan, Chevrolet\n",
      "223: Astronomy, Orbits, Gravity, NASA, Probe, Fuel, University, Overhead, Budget, Space, Moon, Earth, Sun, Trajectory, Perturbations\n",
      "224: Graphics, Computer Animation, Printer, Ozone, Fractal Compression, Video, Image Processing, Hardware, Resolution, Computer Vision, Keyboard\n",
      "225: Baseball, Cubs, Expos, Blue Jays, MLB, Players, Defection, Canada, Teams, Record, Hitter, Guzman, Nixion\n",
      "226: Israel, Palestine, Torture, Prison, Interrogation, Human Rights, War, Conflict, Politics, Government, Media, Journalism, Violence, Dispute, Accusation, Law\n",
      "227: Passwords, Memorization, Medical Education, Anatomy, Histology, Pathology, Pharmacology, Conference, Teachers, Paraprofessionals, School, Health, Children, Education, Students, Techniques\n",
      "228: Baseball, Players, History, Jewish, Books, Spy, Anecdotes, Sportscasters, Culture\n",
      "229: Bible, Translation, Septuagint, Hebrew, Greek, Jewish, Samaritan, Temple, History, Interpretation, Tradition, Manuscripts, Value, Context, Neighbors, Inhumane, Enlightened, Symbolically, Originals, Book\n",
      "230: Hard Drive, Laptop, Computer, RAM, HDD, Floppy Drive, SCSI, CGA, MGA, IDE, Packer Bell, Western Digital, Mitsubishi, Caviar, Seagate, Condition, Technology, Storage, Peripherals\n",
      "231: Audio, Electronics, Oscilloscope, Music, Plotter, Tape Drive, Disk Drive, Monitor, Cable, Sale, Price, Shipping, Condition, Paintball\n",
      "232: Apple, Hardware, Printers, Memory, Software, Upgrades, Backups, Inkjet, LaserWriter, SIMMs, Tape, Drives, ROM\n",
      "233: Housing, University, Internship, Summer, Moscow, Apartment, Rent, Roommate, Sublet, Seattle, Evanston, Downtown\n",
      "234: Motorcycles, Insurance, DWI, Harley, Ducati, Beemer, BMW, Bike, Oil, Transmission, Painting, Safety, Drinking, Bus\n",
      "235: Death Penalty, Recidivism, Crime, FBI, Court, Punishment, Property, Damages, Eminent Domain, Settlement, Law, Economics, Justice, Automobile, Supreme Court, Instructors, Appeals\n",
      "236: Cryptography, Encryption, Privacy, Electronic Cash, Security, Key, DES, Algorithm, Protocol, Cybersecurity, Data Protection\n",
      "237: Religion, Authority, Pastor, Denomination, Bible, Deacons, Elders, God, Faith, Scripture\n",
      "238: Label Makers, SMC 270E, ARCNET, Mac, Batteries, Lithium, AudioMedia, DigiDesign, Digital Camera, Xapshot, Composite Output, Sale, Email\n",
      "239: Sabres, Bruins, Playoffs, Hockey, Goals, Fans, Captain, Trades, Games, Teams, Season, Overtime, Powerplay, Crowd\n",
      "240: Performa, Xstones, Clone, Apple, Warranty, Repairs, Mail Order, Retail, Prices, Service, Dealers, Educational\n",
      "241: Substance Abuse, Recovery, Spirituality, 12-Step Programs, Alcoholics Anonymous, Higher Power, Anxiety, Religion, Guilt\n",
      "242: Literature, Science, Philosophy, Spirituality, Cinema, Imagination\n",
      "243: Drivers, SoundBlaster, Cards, BusLogic, VideoBlaster, Device, Adaptec, Beta, Speaker, Manufacturer, Audio, OS2, FTP, SCSI, Accelerator\n",
      "244: Baseball, Hockey, Sports, Teams, Playoffs, Statistics, Uniforms, Coaching\n",
      "245: Atheism, God, Belief, Religion, Theism, Faith, Revelation, Reason, Truth, Epistemology, Argument, Debate, Bible, Christianity, Conscience, Church, Eternity, Fallibility, Arrogance, Hell, Messenger\n",
      "246: Motorcycles, Bikes, Speed, Riding, Police, Birds, Wildlife, Accidents\n",
      "247: Unix, File Manager, Graph, Voice Input, CGM, PCX, Macintosh, Hypercard, Database, Fashion Design, Graphics, Conversion\n",
      "248: Education, Puberty, Sexual Abuse, Pituitary Gland, Tobacco, Health, Smokeless, E. coli, O157H7, Outbreak, Prevalence, Infections, Illness, Diarrhea, Bloody, CDC, Diseases\n",
      "249: Encryption, Clipper Chip, Security, Privacy, Government, Law Enforcement, Technology, Algorithm, Policy, Surveillance, Cryptography, Keys, Wiretap, Inquiry, Chips, Escrow\n",
      "\n",
      "Overarching Themes Dictionary:\n",
      "0: Sports, Hockey\n",
      "1: Transportation, Vehicles, Automotive\n",
      "2: Guns, Law, Politics\n",
      "3: Security, Government, Law, Technology, Cryptography\n",
      "4: Electronics, Engineering, Technology\n",
      "5: Technology, Government, Privacy\n",
      "6: Religion, Theology\n",
      "7: Philosophy, Ethics, Religion, Society\n",
      "8: Sports, Baseball\n",
      "9: Sociology, Culture, Politics, Health\n",
      "10: Sports\n",
      "11: Space, Astronomy, Economics, Engineering\n",
      "12: Government, Law, Justice, Crime, Investigation\n",
      "13: Software, Computer Systems, Engineering\n",
      "14: Images, Computer Systems, Software\n",
      "15: Sports\n",
      "16: Computer Systems, Software, Programming, User Interface\n",
      "17: Sports, Statistics, Baseball, Performance\n",
      "18: Medicine, Health, Diseases, Finance\n",
      "19: Cryptography, Technology, Security, Government, Communication\n",
      "20: Transportation, Engineering, Physics\n",
      "21: Technology, Computing, Engineering\n",
      "22: Government, Law, Justice, Politics\n",
      "23: Computer Systems, Hardware, Software, Technology\n",
      "24: Religion, History, Scholarship, Literature\n",
      "25: Religion, Theology, Biblical Studies, Christianity, Judaism\n",
      "26: Technology, Computer Systems, Hardware, Software\n",
      "27: Government, Law, Society, Religion\n",
      "28: Religion, Philosophy\n",
      "29: Technology, Electronics, Sales\n",
      "30: Computer Systems, Engineering, Manufacturing, Graphics, Technology\n",
      "31: Transportation, Travel\n",
      "32: Computer Systems, Hardware, Communication\n",
      "33: Transportation, Safety, Law, Education\n",
      "34: Astronomy, Physical Sciences, Space\n",
      "35: Mathematics, Computer Systems, Games, Geometry\n",
      "36: Computer Systems, Software, Technology\n",
      "37: Politics, Media, History, Education\n",
      "38: Images, Computer Systems, Software, Graphics\n",
      "39: Sports\n",
      "40: Religion, Christianity, Catholicism\n",
      "41: Technology, Games, Economics\n",
      "42: Sports, Baseball, Games\n",
      "43: Politics, International Relations, Conflict\n",
      "44: Communication, Technology, Information\n",
      "45: Technology, Government, Politics, Law, Privacy\n",
      "46: Politics, Government, Law, Human Rights, International Relations, History\n",
      "47: Commerce, Retail\n",
      "48: Computer Systems, Technology, Hardware\n",
      "49: Sports\n",
      "50: Technology, Engineering, Telecommunications, Electronics\n",
      "51: Sports, Culture, Geography\n",
      "52: Computer Systems, Software, Programming, User Interface\n",
      "53: Communication, Social Interaction, Culture, Conflict, Humor\n",
      "54: Food, Health, Nutrition, Science\n",
      "55: Transportation, Law, Technology\n",
      "56: Computer Systems, Technology, Education, Economics\n",
      "57: Science, Philosophy, Religion\n",
      "58: Religion, Theology, Spirituality\n",
      "59: Politics, War, Ethnic Conflict, International Relations, Protest\n",
      "60: Computer Systems, Software, Operating Systems\n",
      "61: Computer Systems, Technology, Software, Hardware\n",
      "62: Environment, Transportation, Government, Art, History\n",
      "63: Computer Systems, Software, Hardware, Networking\n",
      "64: Sports, Hockey, Teams, Transfers\n",
      "65: Health, Law, Politics, Religion, Society\n",
      "66: Religion, History, Culture\n",
      "67: Physical Sciences, Biology, Photography, Paranormal\n",
      "68: Graphics, Medicine, Earth Science, Astronomy\n",
      "69: Technology, Communication, Philosophy, Computer Systems\n",
      "70: Computer Systems, Hardware, Technology\n",
      "71: Religion, Sociology, Politics, Education, Law\n",
      "72: Health, Medicine, Science\n",
      "73: Religion, Politics, History, Government\n",
      "74: Transportation, Automotive\n",
      "75: Computer Systems, Graphics, Programming\n",
      "76: Astronomy, Space, Communication, Technology, Education, Research, Science\n",
      "77: History, War, Culture, Politics\n",
      "78: Automotive, Insurance, Weather\n",
      "79: Electronics, Engineering, Education\n",
      "80: Computer Systems, Technology\n",
      "81: Cryptography, Computer Systems, Security\n",
      "82: Drugs, Health, Politics, Law\n",
      "83: Sports, Baseball, Predictions, Statistics\n",
      "84: Computer Systems, Software, Technology\n",
      "85: Computer Systems, Hardware, Software\n",
      "86: Politics, Government, War\n",
      "87: Astronomy, Engineering, Space\n",
      "88: Law, Justice, Government\n",
      "89: Computer Systems, Software, Printing, Images\n",
      "90: Electronics, Audio, Equipment, Sales\n",
      "91: Health, Medicine\n",
      "92: Transportation, Vehicles, Commerce\n",
      "93: Religion, Culture, History\n",
      "94: Religion, Politics, History, Culture\n",
      "95: Automotive, Mechanics, Engineering\n",
      "96: Sports\n",
      "97: Aerospace, Engineering, Government, Safety\n",
      "98: Computer Systems, Software, User Interface\n",
      "99: Computer Systems, Technology, Software\n",
      "100: Computer Systems, Networks, Software, Technology\n",
      "101: Automotive, Vehicles, Transportation\n",
      "102: Technology, Computer, Electronics, Sale\n",
      "103: Technology, Commerce, Communication\n",
      "104: Astronomy, Environment, Law, Sociology\n",
      "105: Education, Engineering, Computer Systems\n",
      "106: Law, Government, Politics, Conflict\n",
      "107: Computer Systems, Technology, Software, Hardware\n",
      "108: Computer Systems, Technology, Hardware, Software\n",
      "109: Computer Systems, Technology, Electronics\n",
      "110: Automotive, Vehicles, Maintenance\n",
      "111: Law Enforcement, Government, Crime, Investigation, Media\n",
      "112: Religion, Politics, Sociology, Law, Human Activities\n",
      "113: Sports\n",
      "114: Computer Systems, Technology\n",
      "115: Surveillance, Privacy, Law, Technology, Government\n",
      "116: Computer Systems, Software, Operating Systems, Technology\n",
      "117: Transportation, Travel\n",
      "118: Sports, Media\n",
      "119: Communication, Environment\n",
      "120: Government, Politics, Economics, Law, Education\n",
      "121: Government, Law, Politics, Justice\n",
      "122: Computer Systems, Technology\n",
      "123: Technology, Hardware, Computer Systems\n",
      "124: Computer Systems, Technology, Security\n",
      "125: Computer Systems, Health, Medicine\n",
      "126: Sports, Baseball\n",
      "127: Computer Systems, Technology, Hardware\n",
      "128: Politics, Economics, History\n",
      "129: Astronomy, Space, Engineering\n",
      "130: Health, Medicine\n",
      "131: Engineering, Aerospace, Electronics, Astronomy, Transportation\n",
      "132: Philosophy, Religion\n",
      "133: Politics, War, History, Government, International Relations\n",
      "134: Animals, Transportation, Law, Social Activities\n",
      "135: Religion, Theology, Ethics\n",
      "136: Computer Systems, Technology\n",
      "137: Economics, Politics, Government, Health\n",
      "138: Religion, Politics, History, Conflict, Culture\n",
      "139: Computer Systems, Hardware, Software, Games\n",
      "140: Transportation, Trade\n",
      "141: Chemistry, Cleaning, Maintenance, Household\n",
      "142: Media, Law, Psychology, Transportation, Finance, Politics, Economics\n",
      "143: Products, Sales, Education, Games, Entertainment\n",
      "144: Automotive, Mechanics, Vehicles, Maintenance\n",
      "145: Health, Government, Economics\n",
      "146: Computer Systems, Hardware, Technology\n",
      "147: Computer Systems, Software, Hardware\n",
      "148: Government, Media, Current Events\n",
      "149: Physics, Astronomy, Quantum Mechanics, Philosophy\n",
      "150: Religion, Sexuality, Theology\n",
      "151: Sports, Baseball\n",
      "152: Religion, Theology, Scripture, History\n",
      "153: Transportation, Safety, Maintenance, Insurance\n",
      "154: Computer Systems, Technology, Hardware\n",
      "155: Education, Art, Health\n",
      "156: Health, Government, Technology, Privacy\n",
      "157: Motorcycles, Maintenance, Gear, Driving\n",
      "158: Astronomy, Earth Science\n",
      "159: Politics, Government, Elections, Sports\n",
      "160: Politics, Government, Communication, Activism\n",
      "161: Religion, Christianity, Theology\n",
      "162: Computer Systems, Graphics, Programming\n",
      "163: Economics, Politics, Government\n",
      "164: Astronomy, Earth Science, Technology, Security\n",
      "165: Literature, Entertainment, Science Fiction, Comics, Pop Culture\n",
      "166: Sports, Fan Culture, Management, Economics\n",
      "167: Religion, Culture, Society\n",
      "168: Computer Systems, Software, Networking, Technology\n",
      "169: Health, Medicine, Nutrition\n",
      "170: Computer Systems, Technology, Software, Hardware\n",
      "171: Transportation, Vehicles, Automotive\n",
      "172: Electronics, Hardware, Computer Systems, Technology, Engineering\n",
      "173: Religion, Theology, Belief Systems\n",
      "174: Space, Astronomy, Engineering, Government, Finance\n",
      "175: Religion, Government, Politics, Law\n",
      "176: Computer Systems, Software, Technology\n",
      "177: Security, Crime, Defense, Public Safety\n",
      "178: Transportation, Safety, Law, Society\n",
      "179: Religion, Sociology, Philosophy\n",
      "180: Computer Systems, Programming, Software, Interface\n",
      "181: Electronics, Commerce, Sales\n",
      "182: Sports\n",
      "183: Computer Systems, Software, Technology\n",
      "184: Religion, Theology, Text & Literature\n",
      "185: Electronics, Engineering\n",
      "186: Medicine, Health, Genetics, Nutrition\n",
      "187: Computer Systems, Technology\n",
      "188: Health, Medicine\n",
      "189: Transportation, Engineering, History\n",
      "190: Electronics, Manufacturing\n",
      "191: Computer Systems, Hardware, Technology, Sales\n",
      "192: Technology, Education, Finance, Automotive\n",
      "193: Religion, Sociology, Ethics\n",
      "194: Transportation, Economics\n",
      "195: Computer Systems, Engineering, Mathematics, Graphics, Hardware\n",
      "196: Engineering, Mechanics, Vehicles\n",
      "197: Religion, Sociology, Culture, Sexuality\n",
      "198: Space, Technology, Astronomy, Health, Government\n",
      "199: Technology, Telecommunications, Electronics\n",
      "200: Sports, Government, Politics, Education, Economics\n",
      "201: Religion, Philosophy\n",
      "202: Politics, Media, Weapons\n",
      "203: Health, Medicine, Law, Government\n",
      "204: Sports\n",
      "205: Language, Health, Law\n",
      "206: International Relations, Politics, Law, Education\n",
      "207: Government, Law, Politics\n",
      "208: Transportation, Fashion, Government\n",
      "209: Space, Technology, Medicine\n",
      "210: Religion, Spirituality, Theology\n",
      "211: Computer Systems, Technology, Electronics\n",
      "212: Computer Systems, Hardware, Technology\n",
      "213: Graphics, Computer Systems, Software, Programming\n",
      "214: Sports, Education, Geography\n",
      "215: Transportation, Law, Business\n",
      "216: Computer Systems, Technology\n",
      "217: Religion, Sociology, Government, Tragedy\n",
      "218: Law, Government, Religion\n",
      "219: Entertainment, Commerce, Health\n",
      "220: Computer Systems, Software, Storage, Operating Systems, File Management\n",
      "221: Computer Systems, Technology\n",
      "222: Transportation, Law, Business, Customer Relations\n",
      "223: Astronomy, Economics, Engineering\n",
      "224: Computer Systems, Images, Technology\n",
      "225: Sports, Baseball, Teams\n",
      "226: Politics, Human Rights, Conflict\n",
      "227: Education, Health\n",
      "228: Sports, History, Culture\n",
      "229: Religion, History, Culture, Language\n",
      "230: Computer Systems, Technology, Hardware\n",
      "231: Electronics, Music, Sales, Technology\n",
      "232: Technology, Computer Systems, Hardware\n",
      "233: Housing, Education, Geography\n",
      "234: Transportation, Finance, Safety\n",
      "235: Law, Crime, Justice, Economics\n",
      "236: Cryptography, Security, Technology\n",
      "237: Religion, Theology, Christianity\n",
      "238: Technology, Electronics, Computer Systems, Communication, Commerce\n",
      "239: Sports\n",
      "240: Computer Systems, Economics, Business\n",
      "241: Health, Sociology, Psychology\n",
      "242: Culture, Science, Philosophy\n",
      "243: Computer Systems, Technology, Hardware, Software\n",
      "244: Sports\n",
      "245: Religion, Philosophy, Epistemology\n",
      "246: Transportation, Nature\n",
      "247: Computer Systems, Software, Technology\n",
      "248: Education, Health\n",
      "249: Technology, Government, Security, Law\n"
     ]
    }
   ],
   "source": [
    "# Only run once\n",
    "def split_themes(data):\n",
    "    themes_dict = {}\n",
    "    overarching_themes_dict = {}\n",
    "    count = 0\n",
    "    for topic, representations in data['topic_representations'].items():\n",
    "        themes = []\n",
    "        overarching_themes = []\n",
    "\n",
    "        for rep in representations:\n",
    "            themes.extend([theme for theme in json.loads(rep[0])['Themes']])\n",
    "            overarching_themes.extend([theme for theme in json.loads(rep[0])['Overarching themes']])\n",
    "        \n",
    "        themes = list(dict.fromkeys(themes))\n",
    "        overarching_themes = list(dict.fromkeys(overarching_themes))\n",
    "        \n",
    "        themes_dict[topic] = ', '.join(themes)\n",
    "        overarching_themes_dict[topic] = ', '.join(overarching_themes)\n",
    "        count+=1\n",
    "    \n",
    "    return themes_dict, overarching_themes_dict\n",
    "\n",
    "themes_dict, overarching_themes_dict = split_themes(topics_json)\n",
    "\n",
    "print(\"Themes Dictionary:\")\n",
    "for k, v in themes_dict.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nOverarching Themes Dictionary:\")\n",
    "for k, v in overarching_themes_dict.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "\n",
    "def append_to_json(file_path, new_data):\n",
    "    \"\"\"Append new data to a JSON array stored in a file.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r+') as file:\n",
    "            # Load existing data\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []\n",
    "            # Append new data\n",
    "            data.append(new_data)\n",
    "            # Move the file pointer to the beginning\n",
    "            file.seek(0)\n",
    "            # Write the updated data\n",
    "            json.dump(data, file, indent=2)\n",
    "            # Truncate the file to the current size\n",
    "            file.truncate()\n",
    "    else:\n",
    "        with open(file_path, 'w') as file:\n",
    "            # Write new data in a list\n",
    "            json.dump([new_data], file, indent=2)\n",
    "            \n",
    "import requests\n",
    "\n",
    "API_URL = os.getenv('HUGGINGFACE_DEDICATED_API_URL')\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_DEDICATED_API_KEY')}\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "hypothesis_template = \"This text is about: {}\"\n",
    "\n",
    "for i, topics in tqdm(enumerate(top_k_topics), total=len(top_k_topics), desc=\"Processing Topics\"):\n",
    "    topic_labels_list = []\n",
    "    overarching_labels_list = []\n",
    "    \n",
    "    for topic_index in topics:\n",
    "        # Using themes from the themes_dict\n",
    "        labels = themes_dict[str(topic_index)]\n",
    "        labels = labels.split(\", \")\n",
    "        topic_labels_list.append(labels)\n",
    "\n",
    "        # Using overarching themes from the overarching_themes_dict\n",
    "        overarching_labels = overarching_themes_dict[str(topic_index)]\n",
    "        overarching_labels = overarching_labels.split(\", \")\n",
    "        overarching_labels_list.append(overarching_labels)\n",
    "    \n",
    "    # Flatten the list of lists and remove duplicates\n",
    "    topic_list = [item for sublist in topic_labels_list for item in sublist]\n",
    "    topic_list = list(set(topic_list))\n",
    "    overarching_list = [item for sublist in overarching_labels_list for item in sublist]\n",
    "    overarching_list = list(set(overarching_list))\n",
    "    \n",
    "    # Query the API instead of zeroshot_model\n",
    "    output = query({\n",
    "        \"inputs\": data[i],  # Assuming `data[i]` is the input text for the zeroshot classifier\n",
    "        \"parameters\": {\n",
    "            \"candidate_labels\": topic_list,\n",
    "            \"hypothesis_template\": hypothesis_template,\n",
    "            \"multi_label\": True\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    append_to_json('tags.json', output)\n",
    "\n",
    "    # Query for overarching labels\n",
    "    output_overarching = query({\n",
    "        \"inputs\": data[i],\n",
    "        \"parameters\": {\n",
    "            \"candidate_labels\": overarching_list,\n",
    "            \"hypothesis_template\": hypothesis_template,\n",
    "            \"multi_label\": True\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    append_to_json('overarching_tags.json', output_overarching)\n",
    "    \n",
    "    # Print labels with scores > 0.7\n",
    "    for label, score in zip(output['labels'], output['scores']):\n",
    "        if score > 0.7:\n",
    "            print(f\"Label: {label}, Score: {score}\")\n",
    "    \n",
    "    # Print overarching labels with scores > 0.5\n",
    "    for label, score in zip(output_overarching['labels'], output_overarching['scores']):\n",
    "        if score > 0.5:\n",
    "            print(f\"Overarching Label: {label}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above but model runs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "hypothesis_template = \"This text is about: {}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
    "zeroshot_model = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-large-zeroshot-v2.0\")\n",
    "\n",
    "for i, topics in tqdm(enumerate(top_k_topics), total=len(top_k_topics), desc=\"Processing Topics\"):\n",
    "    topic_labels_list = []\n",
    "    overarching_labels_list = []\n",
    "    for topic_index in topics:\n",
    "        # Using themes from the themes_dict\n",
    "        labels = themes_dict[str(topic_index)]\n",
    "        labels = labels.split(\", \")\n",
    "        topic_labels_list.append(labels)\n",
    "        \n",
    "        # Using overarching themes from the overarching_themes_dict\n",
    "        overarching_labels = overarching_themes_dict[str(topic_index)]\n",
    "        overarching_labels = overarching_labels.split(\", \")\n",
    "        overarching_labels_list.append(overarching_labels)\n",
    "    \n",
    "    # Flatten the list of lists and remove duplicates\n",
    "    topic_list = [item for sublist in topic_labels_list for item in sublist]\n",
    "    topic_list = list(set(topic_list))\n",
    "    overarching_list = [item for sublist in overarching_labels_list for item in sublist]\n",
    "    overarching_list = list(set(overarching_list))\n",
    "    \n",
    "    # Apply the zeroshot model\n",
    "    output = zeroshot_model(data[i], topic_list, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "    append_to_json('results.json', output)\n",
    "    \n",
    "    output_overarching = zeroshot_model(data[i], overarching_list, hypothesis_template=hypothesis_template, multi_label=True)\n",
    "    append_to_json('overarching_results.json', output_overarching)\n",
    "    \n",
    "    # Print labels with scores > 0.7\n",
    "    for label, score in zip(output['labels'], output['scores']):\n",
    "        if score > 0.7:\n",
    "            print(f\"Label: {label}, Score: {score}\")\n",
    "            \n",
    "    for label, score in zip(output_overarching['labels'], output_overarching['scores']):\n",
    "        if score > 0.5:\n",
    "            print(f\"Overarching Label: {label}, Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
