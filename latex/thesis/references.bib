@article{abdelrazek_topic_2022,
  title = {Topic Modeling Algorithms and Applications: A Survey},
  shorttitle = {Topic Modeling Algorithms and Applications},
  author = {Abdelrazek, Aly and Eid, Yomna and Gawish, Eman and Medhat, Walaa and Hassan Yousef, Ahmed},
  year = {2022},
  month = oct,
  journal = {Information Systems},
  volume = {112},
  pages = {102131},
  doi = {10.1016/j.is.2022.102131},
  abstract = {Topic modeling is used in information retrieval to infer the hidden themes in a collection of documents and thus provides an automatic means to organize, understand and summarize large collections of textual information. Topic models also offer an interpretable representation of documents used in several downstream Natural Language Processing (NLP) tasks. Modeling techniques vary from probabilistic graphical models to the more recent neural models. This paper surveys topic models from four aspects. The first aspect categorizes different topic modeling techniques into four categories: algebraic, fuzzy, probabilistic, and neural. We review the wide variety of available models from each category, highlight differences and similarities between models and model categories using a unified perspective, investigate these models' characteristics and limitations, and discuss their proper use cases. The second aspect illustrates six criteria for proper evaluation of topic models, from modeling quality to interpretability, stability, efficiency, and beyond. Topic modeling has found applications in various disciplines, owing to its interpretability. We examine these applications along with some popular software tools which provide an implementation of some models. The fourth aspect reviews available datasets and benchmarks. Using two benchmark datasets, we conducted experiments to compare seven topic models along the proposed metrics. The discussion highlights the differences between the models and their relative suitability for various applications. It notes the relationship between evaluation metrics and proposes four key aspects to help decide which model to use for an application. Our discussion also shows that the research trends move towards developing and tuning neural topic models and leveraging the power of pre-trained language models. Finally, it highlights research gaps in developing unified benchmarks and evaluation metrics.},
  file = {/Users/ivang/Zotero/storage/T9IZLLFG/Abdelrazek et al. - 2022 - Topic modeling algorithms and applications A surv.pdf}
}

@article{abdi_principal_2010,
  title = {Principal Component Analysis},
  author = {Abdi, Herv{\'e} and Williams, Lynne J.},
  year = {2010},
  month = jul,
  journal = {WIREs Computational Statistics},
  volume = {2},
  number = {4},
  pages = {433--459},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1939-5108},
  doi = {10.1002/wics.101},
  urldate = {2024-09-17},
  abstract = {Abstract Principal component analysis (PCA) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (SVD) of rectangular matrices. Copyright ? 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Multivariate Analysis Statistical and Graphical Methods of Data Analysis {$>$} Dimension Reduction},
  keywords = {bilinear decomposition,factor scores and loadings,multiple factor analysis,RESS PRESS,singular and eigen value decomposition}
}

@inproceedings{aggarwal_surprising_2001,
  title = {On the {{Surprising Behavior}} of {{Distance Metrics}} in {{High Dimensional Space}}},
  booktitle = {Database {{Theory}} --- {{ICDT}} 2001},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  editor = {{Van den Bussche}, Jan and Vianu, Victor},
  year = {2001},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {420--434},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44503-X_27},
  abstract = {In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.},
  isbn = {978-3-540-44503-6},
  langid = {english},
  keywords = {Confusion Matrice,Distance Metrics,High Dimensional Space,Manhattan Distance,Query Point},
  file = {/Users/ivang/Zotero/storage/LGQ5LWQG/Aggarwal et al. - 2001 - On the Surprising Behavior of Distance Metrics in .pdf}
}

@inproceedings{aletras_evaluating_2013,
  title = {Evaluating {{Topic Coherence Using Distributional Semantics}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Computational Semantics}} ({{IWCS}} 2013) -- {{Long Papers}}},
  author = {Aletras, Nikolaos and Stevenson, Mark},
  editor = {Koller, Alexander and Erk, Katrin},
  year = {2013},
  month = mar,
  pages = {13--22},
  publisher = {Association for Computational Linguistics},
  address = {Potsdam, Germany},
  urldate = {2024-06-09},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/M3EIIGXG/Aletras and Stevenson - 2013 - Evaluating Topic Coherence Using Distributional Se.pdf}
}

@inproceedings{allaoui_considerably_2020,
  title = {Considerably {{Improving Clustering Algorithms Using UMAP Dimensionality Reduction Technique}}: {{A Comparative Study}}},
  shorttitle = {Considerably {{Improving Clustering Algorithms Using UMAP Dimensionality Reduction Technique}}},
  booktitle = {Image and {{Signal Processing}}},
  author = {Allaoui, Mebarka and Kherfi, Mohammed Lamine and Cheriet, Abdelhakim},
  editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {317--325},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-51935-3_34},
  abstract = {Dimensionality reduction is widely used in machine learning and big data analytics since it helps to analyze and to visualize large, high-dimensional datasets. In particular, it can considerably help to perform tasks like data clustering and classification. Recently, embedding methods have emerged as a promising direction for improving clustering accuracy. They can preserve the local structure and simultaneously reveal the global structure of data, thereby reasonably improving clustering performance. In this paper, we investigate how to improve the performance of several clustering algorithms using one of the most successful embedding techniques: Uniform Manifold Approximation and Projection or UMAP. This technique has recently been proposed as a manifold learning technique for dimensionality reduction. It is based on Riemannian geometry and algebraic topology. Our main hypothesis is that UMAP would permit to find the best clusterable embedding manifold, and therefore, we applied it as a preprocessing step before performing clustering. We compare the results of many well-known clustering algorithms such ask-means, HDBSCAN, GMM and Agglomerative Hierarchical Clustering when they operate on the low-dimension feature space yielded by UMAP. A series of experiments on several image datasets demonstrate that the proposed method allows each of the clustering algorithms studied to improve its performance on each dataset considered. Based on Accuracy measure, the improvement can reach a remarkable rate of 60\%.},
  isbn = {978-3-030-51935-3},
  langid = {english},
  keywords = {Big data analytics,Clustering,Comparative study,Dimensionality reduction,Embedding manifold,Machine learning,UMAP},
  file = {/Users/ivang/Zotero/storage/YDSDNDVC/Allaoui et al. - 2020 - Considerably Improving Clustering Algorithms Using.pdf}
}

@misc{almeida_word_2023,
  title = {Word Embeddings: A Survey},
  shorttitle = {Word Embeddings},
  author = {Almeida, Felipe and Xex{\'e}o, Geraldo},
  year = {2023},
  month = may,
  number = {arXiv:1901.09069},
  eprint = {1901.09069},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.09069},
  urldate = {2024-01-09},
  abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
  archiveprefix = {arXiv},
  keywords = {A.1,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7,Statistics - Machine Learning},
  file = {/Users/ivang/Zotero/storage/SHVTUSBJ/Almeida and Xexéo - 2023 - Word Embeddings A Survey.pdf;/Users/ivang/Zotero/storage/TVF9FC2S/1901.html}
}

@inproceedings{alokaili_re-ranking_2019,
  title = {Re-{{Ranking Words}} to {{Improve Interpretability}} of {{Automatically Generated Topics}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Computational Semantics}} - {{Long Papers}}},
  author = {Alokaili, Areej and Aletras, Nikolaos and Stevenson, Mark},
  editor = {Dobnik, Simon and Chatzikyriakidis, Stergios and Demberg, Vera},
  year = {2019},
  month = may,
  pages = {43--54},
  publisher = {Association for Computational Linguistics},
  address = {Gothenburg, Sweden},
  doi = {10.18653/v1/W19-0404},
  urldate = {2024-06-09},
  abstract = {Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.},
  file = {/Users/ivang/Zotero/storage/SJNVIPTP/Alokaili et al. - 2019 - Re-Ranking Words to Improve Interpretability of Au.pdf}
}

@inproceedings{alsumait_topic_2009,
  title = {Topic {{Significance Ranking}} of {{LDA Generative Models}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {AlSumait, Loulwah and Barbar{\'a}, Daniel and Gentle, James and Domeniconi, Carlotta},
  editor = {Buntine, Wray and Grobelnik, Marko and Mladeni{\'c}, Dunja and {Shawe-Taylor}, John},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {67--82},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04180-8_22},
  abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of ``junk distribution'' is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
  isbn = {978-3-642-04180-8},
  langid = {english},
  keywords = {Distance Measure,Latent Dirichlet Allocation,Topic Distribution,Topic Model,Weighted Linear Combination},
  file = {/Users/ivang/Zotero/storage/L3AWTYGI/AlSumait et al. - 2009 - Topic Significance Ranking of LDA Generative Model.pdf}
}

@misc{angelov_top2vec_2020,
  title = {{{Top2Vec}}: Distributed Representations of Topics},
  shorttitle = {{{Top2Vec}}},
  author = {Angelov, Dimo},
  year = {2020},
  month = aug,
  number = {arXiv:2008.09470},
  eprint = {2008.09470},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2008.09470},
  urldate = {2023-12-17},
  abstract = {Topic modeling is used for discovering latent semantic structure, usually referred to as topics, in a large collection of documents. The most widely used methods are Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis. Despite their popularity they have several weaknesses. In order to achieve optimal results they often require the number of topics to be known, custom stop-word lists, stemming, and lemmatization. Additionally these methods rely on bag-of-words representation of documents which ignore the ordering and semantics of words. Distributed representations of documents and words have gained popularity due to their ability to capture semantics of words and documents. We present \${\textbackslash}texttt\{top2vec\}\$, which leverages joint document and word semantic embedding to find \${\textbackslash}textit\{topic vectors\}\$. This model does not require stop-word lists, stemming or lemmatization, and it automatically finds the number of topics. The resulting topic vectors are jointly embedded with the document and word vectors with distance between them representing semantic similarity. Our experiments demonstrate that \${\textbackslash}texttt\{top2vec\}\$ finds topics which are significantly more informative and representative of the corpus trained on than probabilistic generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ivang/Zotero/storage/TE6TZQY8/Angelov - 2020 - Top2Vec Distributed Representations of Topics.pdf;/Users/ivang/Zotero/storage/K25LP5W9/2008.html}
}

@book{archetti_bayesian_2019,
  title = {Bayesian {{Optimization}} and {{Data Science}}},
  author = {Archetti, Francesco and Candelieri, Antonio},
  year = {2019},
  series = {{{SpringerBriefs}} in {{Optimization}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-24494-1},
  urldate = {2024-02-29},
  isbn = {978-3-030-24493-4 978-3-030-24494-1},
  langid = {english},
  keywords = {acquisition functions,automatic algorithm configuration,Gaussian process,knowledge gradient,marketing,MarkTech},
  file = {/Users/ivang/Zotero/storage/VU3BXZJV/Archetti and Candelieri - 2019 - Bayesian Optimization and Data Science.pdf}
}

@article{asmussen_smart_2019,
  title = {Smart Literature Review: A Practical Topic Modelling Approach to Exploratory Literature Review},
  shorttitle = {Smart Literature Review},
  author = {Asmussen, Claus Boye and M{\o}ller, Charles},
  year = {2019},
  month = oct,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {93},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0255-7},
  urldate = {2024-01-03},
  abstract = {Manual exploratory literature reviews should be a thing of the past, as technology and development of machine learning methods have matured. The learning curve for using machine learning methods is rapidly declining, enabling new possibilities for all researchers. A framework is presented on how to use topic modelling on a large collection of papers for an exploratory literature review and how that can be used for a full literature review. The aim of the paper is to enable the use of topic modelling for researchers by presenting a step-by-step framework on a case and sharing a code template. The framework consists of three steps; pre-processing, topic modelling, and post-processing, where the topic model Latent Dirichlet Allocation is used. The framework enables huge amounts of papers to be reviewed in a transparent, reliable, faster, and reproducible way.},
  langid = {english},
  keywords = {Automatic literature review,Latent Dirichlet Allocation,Supply chain management,Topic modelling},
  file = {/Users/ivang/Zotero/storage/THYG8TG5/Asmussen and Møller - 2019 - Smart literature review a practical topic modelli.pdf}
}

@incollection{banerjee_topic_2007,
  title = {Topic {{Models}} over {{Text Streams}}: {{A Study}} of {{Batch}} and {{Online Unsupervised Learning}}},
  shorttitle = {Topic {{Models}} over {{Text Streams}}},
  booktitle = {Proceedings of the 2007 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Banerjee, Arindam and Basu, Sugato},
  year = {2007},
  month = apr,
  series = {Proceedings},
  pages = {431--436},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972771.40},
  urldate = {2024-01-08},
  abstract = {Topic modeling techniques have widespread use in text data mining applications. Some applications use batch models, which perform clustering on the document collection in aggregate. In this paper, we analyze and compare the performance of three recently-proposed batch topic models---Latent Dirichlet Allocation (LDA), Dirichlet Compound Multinomial (DCM) mixtures and von-Mises Fisher (vMF) mixture models. In cases where offline clustering on complete document collections is infeasible due to resource and response-rate constraints, online unsupervised clustering methods that process incoming data incrementally are necessary. To this end, we propose online variants of vMF, EDCM and LDA. Experiments on large real-world document collections, in both the offline and online settings, demonstrate that though LDA is a good model for finding word-level topics, vMF finds better document-level topic clusters more efficiently, which is often important in text mining applications. Finally, we propose a practical heuristic for hybrid topic modeling, which learns online topic models on streaming text and intermittently runs batch topic models on aggregated documents offline. Such a hybrid model is useful for several applications (e.g., dynamic topic-based aggregation of user-generated content in social networks) that need a good tradeoff between the performance of batch offline algorithms and efficiency of incremental online algorithms.},
  isbn = {978-0-89871-630-6},
  file = {/Users/ivang/Zotero/storage/K2GHNFY9/Banerjee and Basu - 2007 - Topic Models over Text Streams A Study of Batch a.pdf}
}

@inproceedings{bengio_neural_2000,
  title = {A {{Neural Probabilistic Language Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  year = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2024-01-09},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  file = {/Users/ivang/Zotero/storage/E9HXBUVI/Bengio et al. - 2000 - A Neural Probabilistic Language Model.pdf}
}

@inproceedings{beyer_when_1999,
  title = {When {{Is}} ``{{Nearest Neighbor}}'' {{Meaningful}}?},
  booktitle = {Database {{Theory}} --- {{ICDT}}'99},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  editor = {Beeri, Catriel and Buneman, Peter},
  year = {1999},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {217--235},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-49257-7_15},
  abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.},
  isbn = {978-3-540-49257-3},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/Z7KZ9H5L/Beyer et al. - 1999 - When Is “Nearest Neighbor” Meaningful.pdf}
}

@misc{bhatia_automatic_2017,
  title = {An {{Automatic Approach}} for {{Document-level Topic Model Evaluation}}},
  author = {Bhatia, Shraey and Lau, Jey Han and Baldwin, Timothy},
  year = {2017},
  month = jun,
  number = {arXiv:1706.05140},
  eprint = {1706.05140},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-10},
  abstract = {Topic models jointly learn topics and document-level topic distribution. Extrinsic evaluation of topic models tends to focus exclusively on topic-level evaluation, e.g. by assessing the coherence of topics. We demonstrate that there can be large discrepancies between topic- and documentlevel model quality, and that basing model evaluation on topic-level analysis can be highly misleading. We propose a method for automatically predicting topic model quality based on analysis of documentlevel topic allocations, and provide empirical evidence for its robustness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/KKEQ8YKN/Bhatia et al. - 2017 - An Automatic Approach for Document-level Topic Mod.pdf}
}

@inproceedings{bianchi_cross-lingual_2021,
  title = {Cross-Lingual {{Contextualized Topic Models}} with {{Zero-shot Learning}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Bianchi, Federico and Terragni, Silvia and Hovy, Dirk and Nozza, Debora and Fersini, Elisabetta},
  editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
  year = {2021},
  month = apr,
  pages = {1676--1683},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.eacl-main.143},
  urldate = {2024-05-26},
  abstract = {Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions.},
  file = {/Users/ivang/Zotero/storage/HETJS6HX/Bianchi et al. - 2021 - Cross-lingual Contextualized Topic Models with Zer.pdf}
}

@misc{bianchi_pre-training_2021,
  title = {Pre-Training Is a {{Hot Topic}}: {{Contextualized Document Embeddings Improve Topic Coherence}}},
  shorttitle = {Pre-Training Is a {{Hot Topic}}},
  author = {Bianchi, Federico and Terragni, Silvia and Hovy, Dirk},
  year = {2021},
  month = jun,
  number = {arXiv:2004.03974},
  eprint = {2004.03974},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-26},
  abstract = {Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We find that our approach produces more meaningful and coherent topics than traditional bag-of-words topic models and recent neural models. Our results indicate that future improvements in language models will translate into better topic models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/E7KLX9NG/Bianchi et al. - 2021 - Pre-training is a Hot Topic Contextualized Docume.pdf}
}

@article{bicalho_general_2017,
  title = {A General Framework to Expand Short Text for Topic Modeling},
  author = {Bicalho, Paulo and Pita, Marcelo and Pedrosa, Gabriel and Lacerda, Anisio and Pappa, Gisele L.},
  year = {2017},
  month = jul,
  journal = {Information Sciences},
  volume = {393},
  pages = {66--81},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.02.007},
  urldate = {2024-01-11},
  abstract = {Short texts are everywhere in the Web, including messages posted in social media, status messages and blog comments, and uncovering the topics of this type of messages is crucial to a wide range of applications, e.g., context analysis and user characterization. Extracting topics from short text is challenging because of the dependence of conventional methods, such as Latent Dirichlet Allocation, in words co-occurrence, which in short text is rare and make these methods suffer from severe data sparsity. This paper proposes a general framework for topic modeling of short text by creating larger pseudo-document representations from the original documents. In the framework, document components (e.g., words or bigrams) are defined over a metric space, which provides information about the similarity between them. We present two simple, effective and efficient methods that specialize our general framework to create larger pseudo-documents. While the first method considers word co-occurrence to define the metric space, the second relies on distributed word vector representations. The pseudo-documents generated can be given as input to any topic modeling algorithm. Experiments run in seven datasets and compared against state-of-the-art methods for extracting topics by generating pseudo-documents or modifying current topic modeling methods for short text show the methods significantly improve results in terms of normalized pointwise mutual information. A classification task was also used to evaluate the quality of the topics in terms of document representation, where improvements in F1 varied from 1.5 to 15\%.},
  keywords = {Pseudo-documents,Short text,Topic modeling,Word vector representation},
  file = {/Users/ivang/Zotero/storage/HP5VZDGK/Bicalho et al. - 2017 - A general framework to expand short text for topic.pdf}
}

@inproceedings{blei_dynamic_2006,
  title = {Dynamic Topic Models},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Blei, David M. and Lafferty, John D.},
  year = {2006},
  month = jun,
  series = {{{ICML}} '06},
  pages = {113--120},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1143844.1143859},
  urldate = {2024-01-08},
  abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
  isbn = {978-1-59593-383-6}
}

@inproceedings{blei_latent_2001,
  title = {Latent {{Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Blei, David and Ng, Andrew and Jordan, Michael},
  year = {2001},
  volume = {14},
  publisher = {MIT Press},
  urldate = {2024-01-08},
  abstract = {We propose a generative model for text and other collections of dis(cid:173) crete data that generalizes or improves on several previous models  including naive Bayes/unigram, mixture of unigrams  [6],  and Hof(cid:173) mann's  aspect  model,  also  known  as  probabilistic latent  semantic  indexing  (pLSI)  [3].  In  the  context  of text  modeling,  our  model  posits  that  each  document  is  generated  as  a  mixture  of  topics,  where  the  continuous-valued  mixture  proportions  are  distributed  as  a  latent  Dirichlet  random  variable.  Inference  and  learning  are  carried out efficiently  via variational  algorithms.  We  present  em(cid:173) pirical  results  on  applications  of this  model  to  problems  in  text  modeling,  collaborative filtering,  and text classification.},
  file = {/Users/ivang/Zotero/storage/PYDR643N/Blei et al. - 2001 - Latent Dirichlet Allocation.pdf}
}

@article{bouma_normalized_nodate,
  title = {Normalized ({{Pointwise}}) {{Mutual Information}} in {{Collocation Extraction}}},
  author = {Bouma, Gerlof},
  abstract = {In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/87TU76HS/Bouma - Normalized (Pointwise) Mutual Information in Collo.pdf}
}

@misc{briggs_advanced_nodate,
  title = {Advanced {{Topic Modeling}} with {{BERTopic}} {\textbar} {{Pinecone}}},
  author = {Briggs, James},
  urldate = {2023-12-07},
  abstract = {BERTopic takes advantage of the superior language capabilities of (not yet sentient) transformer models and uses some other ML magic like UMAP and HDBSCAN to produce what is one of the most advanced techniques in language topic modeling today.},
  howpublished = {https://www.pinecone.io/learn/bertopic/},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/6XAQK8FV/bertopic.html}
}

@misc{brown_language_2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-13},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/29GBBDTV/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/ivang/Zotero/storage/AXP7RZUB/2005.html}
}

@misc{brown_language_2020-1,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2024-06-07},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/L9MXKS8B/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/Users/ivang/Zotero/storage/K65N898U/2005.html}
}

@inproceedings{bunk_welda_2018,
  title = {{{WELDA}}: {{Enhancing Topic Models}} by {{Incorporating Local Word Context}}},
  shorttitle = {{{WELDA}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE}} on {{Joint Conference}} on {{Digital Libraries}}},
  author = {Bunk, Stefan and Krestel, Ralf},
  year = {2018},
  month = may,
  series = {{{JCDL}} '18},
  pages = {293--302},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3197026.3197043},
  urldate = {2024-01-11},
  abstract = {The distributional hypothesis states that similar words tend to have similar contexts in which they occur. Word embedding models exploit this hypothesis by learning word vectors based on the local context of words. Probabilistic topic models on the other hand utilize word co-occurrences across documents to identify topically related words. Due to their complementary nature, these models define different notions of word similarity, which, when combined, can produce better topical representations. In this paper we propose WELDA, a new type of topic model, which combines word embeddings (WE) with latent Dirichlet allocation (LDA) to improve topic quality. We achieve this by estimating topic distributions in the word embedding space and exchanging selected topic words via Gibbs sampling from this space. We present an extensive evaluation showing that WELDA cuts runtime by at least 30\% while outperforming other combined approaches with respect to topic coherence and for solving word intrusion tasks.},
  isbn = {978-1-4503-5178-2},
  keywords = {document representations,topic models,word embeddings}
}

@inproceedings{byrne_topic_2022,
  title = {Topic {{Modeling With Topological Data Analysis}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Byrne, Ciar{\'a}n and Horak, Danijela and Moilanen, Karo and Mabona, Amandla},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {11514--11533},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.792},
  urldate = {2024-05-29},
  abstract = {Recent unsupervised topic modelling ap-proaches that use clustering techniques onword, token or document embeddings can ex-tract coherent topics. A common limitationof such approaches is that they reveal noth-ing about inter-topic relationships which areessential in many real-world application do-mains. We present an unsupervised topic mod-elling method which harnesses TopologicalData Analysis (TDA) to extract a topologicalskeleton of the manifold upon which contextu-alised word embeddings lie. We demonstratethat our approach, which performs on par witha recent baseline, is able to construct a networkof coherent topics together with meaningfulrelationships between them.},
  keywords = {Skimmed},
  file = {/Users/ivang/Zotero/storage/QY4D333X/Byrne et al. - 2022 - Topic Modeling With Topological Data Analysis.pdf}
}

@inproceedings{campello_density-based_2013,
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {160--172},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37456-2_14},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  isbn = {978-3-642-37456-2},
  langid = {english},
  keywords = {Cluster Tree,Core Object,Density Threshold,Hierarchical Cluster Method,Minimum Span Tree}
}

@article{campello_hierarchical_2015,
  title = {Hierarchical {{Density Estimates}} for {{Data Clustering}}, {{Visualization}}, and {{Outlier Detection}}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Zimek, Arthur and Sander, J{\"o}rg},
  year = {2015},
  month = jul,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {10},
  number = {1},
  pages = {5:1--5:51},
  issn = {1556-4681},
  doi = {10.1145/2733381},
  urldate = {2024-09-12},
  abstract = {An integrated framework for density-based cluster analysis, outlier detection, and data visualization is introduced in this article. The main module consists of an algorithm to compute hierarchical estimates of the level sets of a density, following Hartigan's classic model of density-contour clusters and trees. Such an algorithm generalizes and improves existing density-based clustering techniques with respect to different aspects. It provides as a result a complete clustering hierarchy composed of all possible density-based clusters following the nonparametric model adopted, for an infinite range of density thresholds. The resulting hierarchy can be easily processed so as to provide multiple ways for data visualization and exploration. It can also be further postprocessed so that: (i) a normalized score of ``outlierness'' can be assigned to each data object, which unifies both the global and local perspectives of outliers into a single definition; and (ii) a ``flat'' (i.e., nonhierarchical) clustering solution composed of clusters extracted from local cuts through the cluster tree (possibly corresponding to different density thresholds) can be obtained, either in an unsupervised or in a semisupervised way. In the unsupervised scenario, the algorithm corresponding to this postprocessing module provides a global, optimal solution to the formal problem of maximizing the overall stability of the extracted clusters. If partially labeled objects or instance-level constraints are provided by the user, the algorithm can solve the problem by considering both constraints violations/satisfactions and cluster stability criteria. An asymptotic complexity analysis, both in terms of running time and memory space, is described. Experiments are reported that involve a variety of synthetic and real datasets, including comparisons with state-of-the-art, density-based clustering and (global and local) outlier detection methods.}
}

@inproceedings{card_little_2020,
  title = {With {{Little Power Comes Great Responsibility}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and Jia, Robin and Mahowald, Kyle and Jurafsky, Dan},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {9263--9274},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.745},
  urldate = {2024-06-10},
  abstract = {Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75\% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.},
  file = {/Users/ivang/Zotero/storage/8I5HVGWN/Card et al. - 2020 - With Little Power Comes Great Responsibility.pdf}
}

@inproceedings{cataldi_emerging_2010,
  title = {Emerging Topic Detection on {{Twitter}} Based on Temporal and Social Terms Evaluation},
  booktitle = {Proceedings of the {{Tenth International Workshop}} on {{Multimedia Data Mining}}},
  author = {Cataldi, Mario and Di Caro, Luigi and Schifanella, Claudio},
  year = {2010},
  month = jul,
  series = {{{MDMKDD}} '10},
  pages = {1--10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1814245.1814249},
  urldate = {2024-01-09},
  abstract = {Twitter is a user-generated content system that allows its users to share short text messages, called tweets, for a variety of purposes, including daily conversations, URLs sharing and information news. Considering its world-wide distributed network of users of any age and social condition, it represents a low level news flashes portal that, in its impressive short response time, has the principal advantage. In this paper we recognize this primary role of Twitter and we propose a novel topic detection technique that permits to retrieve in real-time the most emergent topics expressed by the community. First, we extract the contents (set of terms) of the tweets and model the term life cycle according to a novel aging theory intended to mine the emerging ones. A term can be defined as emerging if it frequently occurs in the specified time interval and it was relatively rare in the past. Moreover, considering that the importance of a content also depends on its source, we analyze the social relationships in the network with the well-known Page Rank algorithm in order to determine the authority of the users. Finally, we leverage a navigable topic graph which connects the emerging terms with other semantically related keywords, allowing the detection of the emerging topics, under user-specified time constraints. We provide different case studies which show the validity of the proposed approach.},
  isbn = {978-1-4503-0220-3},
  keywords = {aging theory,text analysis,topic detection}
}

@misc{cer_universal_2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and {Guajardo-Cespedes}, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  year = {2018},
  month = apr,
  number = {arXiv:1803.11175},
  eprint = {1803.11175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.11175},
  urldate = {2024-03-02},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/9MKG3CH8/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/Users/ivang/Zotero/storage/L92PBB92/1803.html}
}

@inproceedings{chang_reading_2009,
  title = {Reading {{Tea Leaves}}: {{How Humans Interpret Topic Models}}},
  shorttitle = {Reading {{Tea Leaves}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and {Boyd-graber}, Jordan and Blei, David},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-29},
  abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
  file = {/Users/ivang/Zotero/storage/KN4AZ42U/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf}
}

@inproceedings{chang_reading_2009-1,
  title = {Reading {{Tea Leaves}}: {{How Humans Interpret Topic Models}}},
  shorttitle = {Reading {{Tea Leaves}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and {Boyd-graber}, Jordan and Blei, David},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-24},
  abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/ZAWIFNH6/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf}
}

@inproceedings{chemudugunta_modeling_2006,
  title = {Modeling {{General}} and {{Specific Aspects}} of {{Documents}} with a {{Probabilistic Topic Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chemudugunta, Chaitanya and Smyth, Padhraic and Steyvers, Mark},
  year = {2006},
  volume = {19},
  publisher = {MIT Press},
  urldate = {2024-01-08},
  abstract = {Techniques such as probabilistic topic models and latent-semantic indexing have been shown to be broadly useful at automatically extracting the topical or seman- tic content of documents, or more generally for dimension-reduction of sparse count data. These types of models and algorithms can be viewed as generating an abstraction from the words in a document to a lower-dimensional latent variable representation that captures what the document is generally about beyond the spe- cific words it contains. In this paper we propose a new probabilistic model that tempers this approach by representing each document as a combination of (a) a background distribution over common words, (b) a mixture distribution over gen- eral topics, and (c) a distribution over words that are treated as being specific to that document. We illustrate how this model can be used for information retrieval by matching documents both at a general topic level and at a specific word level, providing an advantage over techniques that only match documents at a general level (such as topic models or latent-sematic indexing) or that only match docu- ments at the specific word level (such as TF-IDF).},
  file = {/Users/ivang/Zotero/storage/GIHDIW3Q/Chemudugunta et al. - 2006 - Modeling General and Specific Aspects of Documents.pdf}
}

@article{churchill_evolution_2022,
  title = {The {{Evolution}} of {{Topic Modeling}}},
  author = {Churchill, Rob and Singh, Lisa},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3507900},
  urldate = {2023-12-19},
  abstract = {Topic models have been applied to everything from books to newspapers to social media posts in an effort to identify the most prevalent themes of a text corpus. We provide an in-depth analysis of unsupervised topic models from their inception to today. We trace the origins of different types of contemporary topic models, beginning in the 1990s, and we compare their proposed algorithms, as well as their different evaluation approaches. Throughout, we also describe settings in which topic models have worked well and areas where new research is needed, setting the stage for the next generation of topic models.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/DYJDB82K/Churchill and Singh - 2022 - The Evolution of Topic Modeling.pdf}
}

@article{churchill_percolation-based_2020,
  title = {Percolation-Based Topic Modeling for Tweets},
  author = {Churchill, R. and Singh, L.},
  year = {2020},
  month = aug,
  journal = {WISDOM 2020 : The 9th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining},
  urldate = {2024-01-09},
  abstract = {This paper investigates topic modeling within a noisy domain. The goal is to generate topics that maximize topic coherence while introducing only a small amount of noise. The problem is motivated by the practical setting of short, noisy tweets, where it is important to generate topics containing a larger number of content words than noise words. For the most general version of this problem, we propose a new method, {$\lambda$}-CLIQ. It is a simple variant of the kclique percolation algorithm that employs for quasi-cliques during graph decomposition and percolation based on {$\lambda$}, a graph property variant. While the topics generated using our base algorithm are highly coherent, they are often contain too few words. To increase topic size, we add a post processing step that augments identified topic words using locally trained embeddings. We show that both {$\lambda$}-CLIQ and {$\lambda$}-CLIQ+ outperform the state of the art in terms of topic coherence on three distinct Twitter data sets.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/ENY2RJ2Y/Churchill and Singh - 2020 - Percolation-based topic modeling for tweets.pdf}
}

@inproceedings{churchill_temporal_2018,
  title = {A {{Temporal Topic Model}} for {{Noisy Mediums}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Churchill, Rob and Singh, Lisa and Kirov, Christo},
  editor = {Phung, Dinh and Tseng, Vincent S. and Webb, Geoffrey I. and Ho, Bao and Ganji, Mohadeseh and Rashidi, Lida},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {42--53},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93037-4_4},
  abstract = {Social media and online news content are increasing rapidly. The goal of this work is to identify the topics associated with this content and understand the changing dynamics of these topics over time. We propose Topic Flow Model (TFM), a graph theoretic temporal topic model that identifies topics as they emerge, and tracks them through time as they persist, diminish, and re-emerge. TFM identifies topic words by capturing the changing relationship strength of words over time, and offers solutions for dealing with flood words, i.e., domain specific words that pollute topics. An extensive empirical analysis of TFM on Twitter data, newspaper articles, and synthetic data shows that the topic accuracy and SNR of meaningful topic words are better than the existing state.},
  isbn = {978-3-319-93037-4},
  langid = {english},
  keywords = {Domain-specific Words,Ground Truth Topics,Previous Time Window,Temporal Topic Models,Word Flood}
}

@article{curiskis_evaluation_2020,
  title = {An Evaluation of Document Clustering and Topic Modelling in Two Online Social Networks: {{Twitter}} and {{Reddit}}},
  shorttitle = {An Evaluation of Document Clustering and Topic Modelling in Two Online Social Networks},
  author = {Curiskis, Stephan A. and Drake, Barry and Osborn, Thomas R. and Kennedy, Paul J.},
  year = {2020},
  month = mar,
  journal = {Information Processing \& Management},
  volume = {57},
  number = {2},
  pages = {102034},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.04.002},
  urldate = {2024-01-03},
  abstract = {Methods for document clustering and topic modelling in online social networks (OSNs) offer a means of categorising, annotating and making sense of large volumes of user generated content. Many techniques have been developed over the years, ranging from text mining and clustering methods to latent topic models and neural embedding approaches. However, many of these methods deliver poor results when applied to OSN data as such text is notoriously short and noisy, and often results are not comparable across studies. In this study we evaluate several techniques for document clustering and topic modelling on three datasets from Twitter and Reddit. We benchmark four different feature representations derived from term-frequency inverse-document-frequency (tf-idf) matrices and word embedding models combined with four clustering methods, and we include a Latent Dirichlet Allocation topic model for comparison. Several different evaluation measures are used in the literature, so we provide a discussion and recommendation for the most appropriate extrinsic measures for this task. We also demonstrate the performance of the methods over data sets with different document lengths. Our results show that clustering techniques applied to neural embedding feature representations delivered the best performance over all data sets using appropriate extrinsic evaluation measures. We also demonstrate a method for interpreting the clusters with a top-words based approach using tf-idf weights combined with embedding distance measures.},
  keywords = {Document clustering,Embedding models,Online social networks,Topic discovery,Topic modelling},
  file = {/Users/ivang/Zotero/storage/6F9BJT29/S0306457318307805.html}
}

@misc{dai_semi-supervised_2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  number = {arXiv:1511.01432},
  eprint = {1511.01432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1511.01432},
  urldate = {2024-02-14},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ivang/Zotero/storage/47GQWEXN/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf;/Users/ivang/Zotero/storage/Q3I58NAV/1511.html}
}

@misc{das_openmlscripts_nodate,
  title = {Openml/Scripts},
  author = {Das, Taniya},
  urldate = {2024-02-08},
  howpublished = {https://github.com/openml/scripts/tree/main},
  file = {/Users/ivang/Zotero/storage/TSVFG743/main.html}
}

@article{de_arruda_topic_2016,
  title = {Topic Segmentation via Community Detection in Complex Networks},
  author = {{de Arruda}, Henrique F. and Costa, Luciano da F. and Amancio, Diego R.},
  year = {2016},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {26},
  number = {6},
  eprint = {1512.01384},
  primaryclass = {cs},
  pages = {063120},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4954215},
  urldate = {2024-01-09},
  abstract = {Many real systems have been modelled in terms of network concepts, and written texts are a particular example of information networks. In recent years, the use of network methods to analyze language has allowed the discovery of several interesting findings, including the proposition of novel models to explain the emergence of fundamental universal patterns. While syntactical networks, one of the most prevalent networked models of written texts, display both scale-free and small-world properties, such representation fails in capturing other textual features, such as the organization in topics or subjects. In this context, we propose a novel network representation whose main purpose is to capture the semantical relationships of words in a simple way. To do so, we link all words co-occurring in the same semantic context, which is defined in a threefold way. We show that the proposed representations favours the emergence of communities of semantically related words, and this feature may be used to identify relevant topics. The proposed methodology to detect topics was applied to segment selected Wikipedia articles. We have found that, in general, our methods outperform traditional bag-of-words representations, which suggests that a high-level textual representation may be useful to study semantical features of texts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/Users/ivang/Zotero/storage/ZEZQ7V9Q/de Arruda et al. - 2016 - Topic segmentation via community detection in comp.pdf;/Users/ivang/Zotero/storage/LYTTEAIS/1512.html}
}

@book{de_theorie_1814,
  title = {{Th{\'e}orie analytique des probabilit{\'e}s}},
  author = {{de)}, Pierre Simon Laplace (marqu{\'i}s},
  year = {1814},
  publisher = {Courcier},
  googlebooks = {PUEXYu1LlAsC},
  langid = {french}
}

@article{deerwester_indexing_1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, {\relax LANDAUE{\'R}T} and Harshman, Richard},
  year = {1990},
  journal = {Indexing by latent semantic analysis},
  volume = {41},
  number = {6},
  pages = {391--407},
  publisher = {John Wiley \& Sons},
  address = {New York, NY},
  issn = {0002-8231},
  file = {/Users/ivang/Zotero/storage/P6TQNW3B/index.html}
}

@misc{devlin_bert_2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/Q5M2EI6G/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/ivang/Zotero/storage/DL9HJCPQ/1810.html}
}

@misc{dieng_dynamic_2019,
  title = {The {{Dynamic Embedded Topic Model}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
  year = {2019},
  month = oct,
  number = {arXiv:1907.05545},
  eprint = {1907.05545},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.05545},
  urldate = {2024-01-11},
  abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. For documents collected in sequence, dynamic topic models capture how these patterns vary over time. We develop the dynamic embedded topic model (D-ETM), a generative model of documents that combines dynamic latent Dirichlet allocation (D-LDA) and word embeddings. The D-ETM models each word with a categorical distribution parameterized by the inner product between the word embedding and a per-time-step embedding representation of its assigned topic. The D-ETM learns smooth topic trajectories by defining a random walk prior over the embedding representations of the topics. We fit the D-ETM using structured amortized variational inference with a recurrent neural network. On three different corpora---a collection of United Nations debates, a set of ACL abstracts, and a dataset of Science Magazine articles---we found that the D-ETM outperforms D-LDA on a document completion task. We further found that the D-ETM learns more diverse and coherent topics than D-LDA while requiring significantly less time to fit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/ivang/Zotero/storage/DTMKA4HY/Dieng et al. - 2019 - The Dynamic Embedded Topic Model.pdf;/Users/ivang/Zotero/storage/ZJMKFG3G/1907.html}
}

@article{dieng_topic_2020,
  title = {Topic {{Modeling}} in {{Embedding Spaces}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
  year = {2020},
  month = jul,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {439--453},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00325},
  urldate = {2024-01-11},
  abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded topic model (etm), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the etm models each word with a categorical distribution whose natural parameter is the inner product between the word's embedding and an embedding of its assigned topic. To fit the etm, we develop an efficient amortized variational inference algorithm. The etm discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.},
  file = {/Users/ivang/Zotero/storage/JDBEAI4D/Dieng et al. - 2020 - Topic Modeling in Embedding Spaces.pdf}
}

@article{ding_equivalence_2008,
  title = {On the Equivalence between {{Non-negative Matrix Factorization}} and {{Probabilistic Latent Semantic Indexing}}},
  author = {Ding, Chris and Li, Tao and Peng, Wei},
  year = {2008},
  month = apr,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {8},
  pages = {3913--3927},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2008.01.011},
  urldate = {2024-01-08},
  abstract = {Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF (with the I-divergence objective function) optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of the local minima of the other method successively, thus achieving a better final solution. Extensive experiments on five real-life datasets show relations between NMF and PLSI, and indicate that the hybrid method leads to significant improvements over NMF-only or PLSI-only methods. We also show that at first-order approximation, NMF is identical to the {$\chi$}2-statistic.},
  file = {/Users/ivang/Zotero/storage/8LQRAP78/Ding et al. - 2008 - On the equivalence between Non-negative Matrix Fac.pdf}
}

@inproceedings{doogan_topic_2021,
  title = {Topic {{Model}} or {{Topic Twaddle}}? {{Re-evaluating Semantic Interpretability Measures}}},
  shorttitle = {Topic {{Model}} or {{Topic Twaddle}}?},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Doogan, Caitlin and Buntine, Wray},
  editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and {Hakkani-Tur}, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  year = {2021},
  month = jun,
  pages = {3824--3848},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.300},
  urldate = {2024-06-09},
  abstract = {When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models' generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data.},
  file = {/Users/ivang/Zotero/storage/FT36D4JM/Doogan and Buntine - 2021 - Topic Model or Topic Twaddle Re-evaluating Semant.pdf}
}

@inproceedings{el_mokhtari_using_2020,
  title = {Using {{Topic Modelling}} to {{Improve Prediction}} of {{Financial Report Commentary Classes}}},
  booktitle = {Advances in {{Artificial Intelligence}}},
  author = {El Mokhtari, Karim and Cevik, Mucahit and Ba{\c s}ar, Ay{\c s}e},
  editor = {Goutte, Cyril and Zhu, Xiaodan},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {201--207},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-47358-7_19},
  abstract = {We consider the task of predicting the class of commentaries associated with financial discrepancies between actual and estimated sales data. Such analysis of the financial data is helpful in meeting targets and assessing the overall performance of the company. While generating a commentary and its associated class is the task of an analyst, these manual operations might be erroneous and as a result, might lead to a diminished performance for the employed prediction model due to wrong class labels. Accordingly, we propose using topic modelling, namely Latent Dirichlet Allocation (LDA), for automated extraction of the classes of the commentaries. In addition, we use feature selection strategies to improve the accuracy of the prediction models. Our analysis with various time series classification methods points to improved performance due to LDA and feature selection.},
  isbn = {978-3-030-47358-7},
  langid = {english},
  keywords = {LDA,NLP,Time series classification}
}

@misc{entropygoaway_why_2022,
  type = {Reddit {{Post}}},
  title = {Why Does {{Zero-Shot-Classification}} Not Work in This Simple Use-Case?},
  author = {EntropyGoAway},
  year = {2022},
  month = feb,
  journal = {r/LanguageTechnology},
  urldate = {2024-06-06},
  file = {/Users/ivang/Zotero/storage/CHW474IP/why_does_zeroshotclassification_not_work_in_this.html}
}

@article{ester_density-based_nodate,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/8JWF5R6R/Ester et al. - A Density-Based Algorithm for Discovering Clusters.pdf}
}

@misc{feurer_openml-python_2021,
  title = {{{OpenML-Python}}: An Extensible {{Python API}} for {{OpenML}}},
  shorttitle = {{{OpenML-Python}}},
  author = {Feurer, Matthias and {van Rijn}, Jan N. and Kadra, Arlind and Gijsbers, Pieter and Mallik, Neeratyoy and Ravi, Sahithya and M{\"u}ller, Andreas and Vanschoren, Joaquin and Hutter, Frank},
  year = {2021},
  month = jun,
  number = {arXiv:1911.02490},
  eprint = {1911.02490},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-03},
  abstract = {OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper we introduce OpenML-Python, a client API for Python, opening up the OpenML platform for a wide range of Python-based tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation is available at https://github.com/openml/openml-python/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ivang/Zotero/storage/TF27Z5H9/Feurer et al. - 2021 - OpenML-Python an extensible Python API for OpenML.pdf;/Users/ivang/Zotero/storage/VW9T57LF/1911.html}
}

@article{galuzzi_hyperparameter_2020,
  title = {Hyperparameter Optimization for Recommender Systems through {{Bayesian}} Optimization},
  author = {Galuzzi, B. G. and Giordani, I. and Candelieri, A. and Perego, R. and Archetti, F.},
  year = {2020},
  month = dec,
  journal = {Computational Management Science},
  volume = {17},
  number = {4},
  pages = {495--515},
  issn = {1619-6988},
  doi = {10.1007/s10287-020-00376-3},
  urldate = {2024-02-29},
  abstract = {Recommender systems represent one of the most successful applications of machine learning in B2C online services, to help the users in their choices in many web services. Recommender system aims to predict the user preferences from a huge amount of data, basically the past behaviour of the user, using an efficient prediction algorithm. One of the most used is the matrix-factorization algorithm. Like many machine learning algorithms, its effectiveness goes through the tuning of its hyper-parameters, and the associated optimization problem also called hyper-parameter optimization. This represents a noisy time-consuming black-box optimization problem. The related objective function maps any possible hyper-parameter configuration to a numeric score quantifying the algorithm performance. In this work, we show how Bayesian optimization can help the tuning of three hyper-parameters: the number of latent factors, the regularization parameter, and the learning rate. Numerical results are obtained on a benchmark problem and show that Bayesian optimization obtains a better result than the default setting of the hyper-parameters and the random search.},
  langid = {english},
  keywords = {Bayesian optimization,Collaborative filtering,Hyperparameters optimization,Matrix factorization,Recommender system},
  file = {/Users/ivang/Zotero/storage/V8MA5MTB/Galuzzi et al. - 2020 - Hyperparameter optimization for recommender system.pdf}
}

@article{garcia-mendez_automatic_2023,
  title = {Automatic Detection of Relevant Information, Predictions and Forecasts in Financial News through Topic Modelling with {{Latent Dirichlet Allocation}}},
  author = {{Garc{\'i}a-M{\'e}ndez}, Silvia and {de Arriba-P{\'e}rez}, Francisco and {Barros-Vila}, Ana and {Gonz{\'a}lez-Casta{\~n}o}, Francisco J. and {Costa-Montenegro}, Enrique},
  year = {2023},
  month = aug,
  journal = {Applied Intelligence},
  volume = {53},
  number = {16},
  pages = {19610--19628},
  issn = {1573-7497},
  doi = {10.1007/s10489-023-04452-4},
  urldate = {2024-01-03},
  abstract = {Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. They are typically written by market experts who describe stock market events within the context of social, economic and political change. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (nlp) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (lda) to separate relevant from less relevant text and then analyse the relevant text using a Machine Learning-oriented temporal approach to identify predictions and speculative statements. Our solution outperformed a rule-based baseline system. We created an experimental data set composed of 2,158 financial news items that were manually labelled by nlp researchers to evaluate our solution. Inter-agreement Alpha-reliability and accuracy values, and rouge-l results endorse its potential as a valuable tool for busy investors. The rouge-l values for the identification of relevant text and predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph topic segmentation and co-reference resolution to separate author expression patterns, topic modelling with lda to detect relevant text, and discursive temporality analysis to identify forecasts and predictions within this text. Our solution may have compelling applications in the financial field, including the possibility of extracting relevant statements on investment strategies to analyse authors' reputations.},
  langid = {english},
  keywords = {Financial news analysis,Knowledge extraction,Latent Dirichlet Allocation,Natural language processing,Personal finance management,Temporality analysis},
  file = {/Users/ivang/Zotero/storage/QSRC3YRC/García-Méndez et al. - 2023 - Automatic detection of relevant information, predi.pdf}
}

@misc{grootendorst_1_nodate,
  title = {1. {{Embeddings}} - {{BERTopic}}},
  author = {Grootendorst, Maarten P.},
  urldate = {2024-02-23},
  abstract = {Leveraging BERT and a class-based TF-IDF to create easily interpretable topics.},
  howpublished = {https://maartengr.github.io/BERTopic/getting\_started/embeddings/embeddings.html},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/3WJLIC74/embeddings.html}
}

@misc{grootendorst_algorithm_nodate,
  title = {The {{Algorithm}} - {{BERTopic}}},
  author = {Grootendorst, Maarten P.},
  urldate = {2024-02-28},
  abstract = {Leveraging BERT and a class-based TF-IDF to create easily interpretable topics.},
  howpublished = {https://maartengr.github.io/BERTopic/algorithm/algorithm.html},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/3ALN5GV5/algorithm.html}
}

@misc{grootendorst_bertopic_2022,
  title = {{{BERTopic}}: {{Neural}} Topic Modeling with a Class-Based {{TF-IDF}} Procedure},
  shorttitle = {{{BERTopic}}},
  author = {Grootendorst, Maarten},
  year = {2022},
  month = mar,
  number = {arXiv:2203.05794},
  eprint = {2203.05794},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-06},
  abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/WCZUW778/Grootendorst - 2022 - BERTopic Neural topic modeling with a class-based.pdf;/Users/ivang/Zotero/storage/VZVUAU4U/2203.html}
}

@misc{grootendorst_maartengrkeybert_2024,
  title = {{{MaartenGr}}/{{KeyBERT}}},
  author = {Grootendorst, Maarten},
  year = {2024},
  month = feb,
  urldate = {2024-02-28},
  abstract = {Minimal keyword extraction with BERT},
  copyright = {MIT},
  keywords = {bert,keyphrase-extraction,keyword-extraction,mmr}
}

@inproceedings{hoffman_online_2010,
  title = {Online {{Learning}} for {{Latent Dirichlet Allocation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoffman, Matthew and Bach, Francis and Blei, David},
  year = {2010},
  volume = {23},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-08},
  abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.},
  file = {/Users/ivang/Zotero/storage/S5W2FCS9/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf}
}

@inproceedings{hofmann_probabilistic_1999,
  title = {Probabilistic Latent Semantic Indexing},
  booktitle = {Proceedings of the 22nd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Hofmann, Thomas},
  year = {1999},
  month = aug,
  pages = {50--57},
  publisher = {ACM},
  address = {Berkeley California USA},
  doi = {10.1145/312624.312649},
  urldate = {2023-12-19},
  isbn = {978-1-58113-096-6},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/977SA8V9/Hofmann - 1999 - Probabilistic latent semantic indexing.pdf}
}

@inproceedings{hofmann_probabilistic_1999-1,
  title = {Probabilistic Latent Semantic Indexing},
  booktitle = {Proceedings of the 22nd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Hofmann, Thomas},
  year = {1999},
  month = aug,
  pages = {50--57},
  publisher = {ACM},
  address = {Berkeley California USA},
  doi = {10.1145/312624.312649},
  urldate = {2024-01-08},
  isbn = {978-1-58113-096-6},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/7GRRKUU5/Hofmann - 1999 - Probabilistic latent semantic indexing.pdf}
}

@inproceedings{hoyle_is_2021,
  title = {Is {{Automated Topic Model Evaluation Broken}}? {{The Incoherence}} of {{Coherence}}},
  shorttitle = {Is {{Automated Topic Model Evaluation Broken}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoyle, Alexander and Goel, Pranav and {Hian-Cheong}, Andrew and Peskov, Denis and {Boyd-Graber}, Jordan and Resnik, Philip},
  year = {2021},
  volume = {34},
  pages = {2018--2033},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-24},
  abstract = {Topic model evaluation, like evaluation of other unsupervised methods, can be contentious. However, the field has coalesced around automated estimates of topic coherence, which rely on the frequency of word co-occurrences in a reference corpus. Contemporary neural topic models surpass classical ones according to these metrics. At the same time, topic model evaluation suffers from a validation gap: automated coherence, developed for classical models, has not been validated using human experimentation for neural models. In addition, a meta-analysis of topic modeling literature reveals a substantial standardization gap in automated topic modeling benchmarks. To address the validation gap, we compare automated coherence with the two most widely accepted human judgment tasks: topic rating and word intrusion. To address the standardization gap, we systematically evaluate a dominant classical model and two state-of-the-art neural models on two commonly used datasets. Automated evaluations declare a winning model when corresponding human evaluations do not, calling into question the validity of fully automatic evaluations independent of human judgments.},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/EIVCRXKS/Hoyle et al. - 2021 - Is Automated Topic Model Evaluation Broken The In.pdf}
}

@book{iwata_topic_2009,
  title = {Topic {{Tracking Model}} for {{Analyzing Consumer Purchase Behavior}}.},
  author = {Iwata, Tomoharu and Watanabe, Shinji and Yamada, Takeshi and Ueda, Naonori},
  year = {2009},
  month = jan,
  pages = {1432},
  abstract = {We propose a new topic model for tracking time- varying consumer purchase behavior, in which con- sumer interests and item trends change over time. The proposed model can adaptively track changes in interests and trends based on current purchase logs and previously estimated interests and trends. The online nature of the proposed method means we do not need to store past data for current infer- ences and so we can considerably reduce the com- putational cost and the memory requirement. We use real purchase logs to demonstrate the effective- ness of the proposed method in terms of the predic- tion accuracy of purchase behavior and the compu- tational cost of the inference.},
  file = {/Users/ivang/Zotero/storage/AC7PK6E2/Iwata et al. - 2009 - Topic Tracking Model for Analyzing Consumer Purcha.pdf}
}

@incollection{jacobi_quantitative_2018,
  title = {Quantitative Analysis of Large Amounts of Journalistic Texts Using Topic Modelling},
  booktitle = {Rethinking {{Research Methods}} in an {{Age}} of {{Digital Journalism}}},
  author = {Jacobi, Carina and {van Atteveldt}, Wouter and Welbers, Kasper},
  year = {2018},
  publisher = {Routledge},
  abstract = {The huge collections of news content which have become available through digital technologies both enable and warrant scientific inquiry, challenging journalism scholars to analyse unprecedented amounts of texts. We propose Latent Dirichlet Allocation (LDA) topic modelling as a tool to face this challenge. LDA is a cutting edge technique for content analysis, designed to automatically organize large archives of documents based on latent topics, measured as patterns of word (co-)occurrence. We explain how this technique works, how different choices by the researcher affect the results and how the results can be meaningfully interpreted. To demonstrate its usefulness for journalism research, we conducted a case study of the New York Times coverage of nuclear technology from 1945 to the present, partially replicating a study by Gamson and Modigliani. This shows that LDA is a useful tool for analysing trends and patterns in news content in large digital news archives relatively quickly.},
  isbn = {978-1-315-11504-7}
}

@article{jelodar_latent_2019,
  title = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling: Models, Applications, a Survey},
  shorttitle = {Latent {{Dirichlet}} Allocation ({{LDA}}) and Topic Modeling},
  author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  year = {2019},
  month = jun,
  journal = {Multimedia Tools and Applications},
  volume = {78},
  number = {11},
  pages = {15169--15211},
  issn = {1573-7721},
  doi = {10.1007/s11042-018-6894-4},
  urldate = {2024-01-08},
  abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
  langid = {english},
  keywords = {Gibbs sampling,Latent Dirichlet allocation,Semantic web,Tag recommendation,Topic modeling},
  file = {/Users/ivang/Zotero/storage/8KMFTDLW/Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf}
}

@inproceedings{joachims_probabilistic_1997,
  title = {A {{Probabilistic Analysis}} of the {{Rocchio Algorithm}} with {{TFIDF}} for {{Text Categorization}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Machine Learning}}},
  author = {Joachims, Thorsten},
  year = {1997},
  month = jul,
  series = {{{ICML}} '97},
  pages = {143--151},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2024-02-28},
  isbn = {978-1-55860-486-5},
  file = {/Users/ivang/Zotero/storage/JDHQGUQG/Joachims - A Probabilistic Analysis of the Rocchio Algorithm .pdf}
}

@inproceedings{kasiviswanathan_emerging_2011,
  title = {Emerging Topic Detection Using Dictionary Learning},
  booktitle = {Proceedings of the 20th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Kasiviswanathan, Shiva Prasad and Melville, Prem and Banerjee, Arindam and Sindhwani, Vikas},
  year = {2011},
  month = oct,
  series = {{{CIKM}} '11},
  pages = {745--754},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2063576.2063686},
  urldate = {2024-01-08},
  abstract = {Streaming user-generated content in the form of blogs, microblogs, forums, and multimedia sharing sites, provides a rich source of data from which invaluable information and insights maybe gleaned. Given the vast volume of such social media data being continually generated, one of the challenges is to automatically tease apart the emerging topics of discussion from the constant background chatter. Such emerging topics can be identified by the appearance of multiple posts on a unique subject matter, which is distinct from previous online discourse. We address the problem of identifying emerging topics through the use of dictionary learning. We propose a two stage approach respectively based on detection and clustering of novel user-generated content. We derive a scalable approach by using the alternating directions method to solve the resulting optimization problems. Empirical results show that our proposed approach is more effective than several baselines in detecting emerging topics in traditional news story and newsgroup data. We also demonstrate the practical application to social media analysis, based on a study on streaming data from Twitter.},
  isbn = {978-1-4503-0717-8},
  keywords = {clustering,dictionary learning,l1 reconstruction}
}

@inproceedings{lafferty_correlated_2005,
  title = {Correlated {{Topic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lafferty, John and Blei, David},
  year = {2005},
  volume = {18},
  publisher = {MIT Press},
  urldate = {2024-01-08},
  abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
  file = {/Users/ivang/Zotero/storage/LU8FVKY5/Lafferty and Blei - 2005 - Correlated Topic Models.pdf}
}

@inproceedings{lau_best_2010,
  title = {Best {{Topic Word Selection}} for {{Topic Labelling}}},
  booktitle = {Coling 2010: {{Posters}}},
  author = {Lau, Jey Han and Newman, David and Karimi, Sarvnaz and Baldwin, Timothy},
  editor = {Huang, Chu-Ren and Jurafsky, Dan},
  year = {2010},
  month = aug,
  pages = {605--613},
  publisher = {Coling 2010 Organizing Committee},
  address = {Beijing, China},
  urldate = {2024-06-13},
  file = {/Users/ivang/Zotero/storage/7PXV6Y7N/Lau et al. - 2010 - Best Topic Word Selection for Topic Labelling.pdf}
}

@misc{lau_empirical_2016,
  title = {An {{Empirical Evaluation}} of Doc2vec with {{Practical Insights}} into {{Document Embedding Generation}}},
  author = {Lau, Jey Han and Baldwin, Timothy},
  year = {2016},
  month = jul,
  number = {arXiv:1607.05368},
  eprint = {1607.05368},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.05368},
  urldate = {2024-03-02},
  abstract = {Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/SR6Q7I2U/Lau and Baldwin - 2016 - An Empirical Evaluation of doc2vec with Practical .pdf;/Users/ivang/Zotero/storage/QT4VFD89/1607.html}
}

@inproceedings{lau_machine_2014,
  title = {Machine {{Reading Tea Leaves}}: {{Automatically Evaluating Topic Coherence}} and {{Topic Model Quality}}},
  shorttitle = {Machine {{Reading Tea Leaves}}},
  booktitle = {Proceedings of the 14th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lau, Jey Han and Newman, David and Baldwin, Timothy},
  editor = {Wintner, Shuly and Goldwater, Sharon and Riezler, Stefan},
  year = {2014},
  month = apr,
  pages = {530--539},
  publisher = {Association for Computational Linguistics},
  address = {Gothenburg, Sweden},
  doi = {10.3115/v1/E14-1056},
  urldate = {2024-06-07},
  file = {/Users/ivang/Zotero/storage/KPQ3AZ8M/Lau et al. - 2014 - Machine Reading Tea Leaves Automatically Evaluati.pdf}
}

@misc{laurer_building_2024,
  title = {Building {{Efficient Universal Classifiers}} with {{Natural Language Inference}}},
  author = {Laurer, Moritz and {van Atteveldt}, Wouter and Casas, Andreu and Welbers, Kasper},
  year = {2024},
  month = mar,
  number = {arXiv:2312.17543},
  eprint = {2312.17543},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-28},
  abstract = {Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier,1 and (3) shares the resulting universal classifier that is trained on 33 datasets with 389 diverse classes. Parts of the code we share has been used to train our older zeroshot classifiers that have been downloaded more than 55 million times via the Hugging Face Hub as of December 2023. Our new classifier improves zeroshot performance by 9.4\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/HZ3M5R74/Laurer et al. - 2024 - Building Efficient Universal Classifiers with Natu.pdf}
}

@misc{le_distributed_2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  year = {2014},
  month = may,
  number = {arXiv:1405.4053},
  eprint = {1405.4053},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ivang/Zotero/storage/V6QJVAXV/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/Users/ivang/Zotero/storage/X4A454S2/1405.html}
}

@article{lee_human_2017,
  title = {The Human Touch: {{How}} Non-Expert Users Perceive, Interpret, and Fix Topic Models},
  shorttitle = {The Human Touch},
  author = {Lee, Tak Yeon and Smith, Alison and Seppi, Kevin and Elmqvist, Niklas and {Boyd-Graber}, Jordan and Findlater, Leah},
  year = {2017},
  month = sep,
  journal = {International Journal of Human-Computer Studies},
  volume = {105},
  pages = {28--42},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2017.03.007},
  urldate = {2024-05-24},
  abstract = {Topic modeling is a common tool for understanding large bodies of text, but is typically provided as a ``take it or leave it'' proposition. Incorporating human knowledge in unsupervised learning is a promising approach to create high-quality topic models. Existing interactive systems and modeling algorithms support a wide range of refinement operations to express feedback. However, these systems' interactions are primarily driven by algorithmic convenience, ignoring users who may lack expertise in topic modeling. To better understand how non-expert users understand, assess, and refine topics, we conducted two user studies---an in-person interview study and an online crowdsourced study. These studies demonstrate a disconnect between what non-expert users want and the complex, low-level operations that current interactive systems support. In particular, our findings include: (1) analysis of how non-expert users perceive topic models; (2) characterization of primary refinement operations expected by non-expert users and ordered by relative preference; (3) further evidence of the benefits of supporting users in directly refining a topic model; (4) design implications for future human-in-the-loop topic modeling interfaces.},
  keywords = {Mixed-initiative interaction,Read,Topic modeling,User study},
  file = {/Users/ivang/Zotero/storage/VPDU5UKU/Lee et al. - 2017 - The human touch How non-expert users perceive, in.pdf;/Users/ivang/Zotero/storage/YJ5QTT5G/S1071581917300472.html}
}

@article{lee_learning_1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  journal = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44565},
  urldate = {2023-12-19},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/EXUSC24S/Lee and Seung - 1999 - Learning the parts of objects by non-negative matr.pdf}
}

@article{lee_learning_1999-1,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  year = {1999},
  month = oct,
  journal = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/44565},
  urldate = {2024-03-01},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  copyright = {1999 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{li_dirichlet_2019,
  title = {Dirichlet {{Multinomial Mixture}} with {{Variational Manifold Regularization}}: {{Topic Modeling}} over {{Short Texts}}},
  shorttitle = {Dirichlet {{Multinomial Mixture}} with {{Variational Manifold Regularization}}},
  author = {Li, Ximing and Zhang, Jiaojiao and Ouyang, Jihong},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {7884--7891},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33017884},
  urldate = {2024-01-11},
  abstract = {Conventional topic models suffer from a severe sparsity problem when facing extremely short texts such as social media posts. The family of Dirichlet multinomial mixture (DMM) can handle the sparsity problem, however, they are still very sensitive to ordinary and noisy words, resulting in inaccurate topic representations at the document level. In this paper, we alleviate this problem by preserving local neighborhood structure of short texts, enabling to spread topical signals among neighboring documents, so as to correct the inaccurate topic representations. This is achieved by using variational manifold regularization, constraining the close short texts should have similar variational topic representations. Upon this idea, we propose a novel Laplacian DMM (LapDMM) topic model. During the document graph construction, we further use the word mover's distance with word embeddings to measure document similarities at the semantic level. To evaluate LapDMM, we compare it against the state-of-theart short text topic models on several traditional tasks. Experimental results demonstrate that our LapDMM achieves very significant performance gains over baseline models, e.g., achieving even about 0.2 higher scores on clustering and classification tasks in many cases.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/G8A8QV3N/Li et al. - 2019 - Dirichlet Multinomial Mixture with Variational Man.pdf}
}

@article{li_filtering_2018,
  title = {Filtering out the Noise in Short Text Topic Modeling},
  author = {Li, Ximing and Wang, Yue and Zhang, Ang and Li, Changchun and Chi, Jinjin and Ouyang, Jihong},
  year = {2018},
  month = aug,
  journal = {Information Sciences},
  volume = {456},
  pages = {83--96},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2018.04.071},
  urldate = {2024-01-11},
  abstract = {Nowadays, massive short texts, such as social media posts and newspaper titles, are available on the Internet. Analyzing these short texts is very significant for many content analysis tasks. However, the commonly used text analysis tools, i.e., topic models, lose effectiveness on short texts because of the sparsity and noise problems. Recent topic models mainly attempt to solve the sparsity problem, but neglect the noise issue. To address this, we propose a common semantics topic model (CSTM) in this paper. The key idea is to introduce a new type of topic, namely common topic, to gather the noise words. The experimental results on real-world datasets indicate that our CSTM outperforms the existing short text topic models on the traditional tasks.},
  keywords = {Noise words,Short text,Topic inference,Topic modeling},
  file = {/Users/ivang/Desktop/li2018.pdf}
}

@inproceedings{li_topic_2016,
  title = {Topic {{Modeling}} for {{Short Texts}} with {{Auxiliary Word Embeddings}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR}} Conference on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Li, Chenliang and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
  year = {2016},
  month = jul,
  series = {{{SIGIR}} '16},
  pages = {165--174},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2911451.2911499},
  urldate = {2024-01-10},
  abstract = {For many applications that require semantic understanding of short texts, inferring discriminative and coherent latent topics from short texts is a critical and fundamental task. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Data sparsity therefore becomes a bottleneck for conventional topic models to achieve good results on short texts. On the other hand, when a human being interprets a piece of short text, the understanding is not solely based on its content words, but also her background knowledge (e.g., semantically related words). The recent advances in word embedding offer effective learning of word semantic relations from a large corpus. Exploiting such auxiliary word embeddings to enrich topic modeling for short texts is the main focus of this paper. To this end, we propose a simple, fast, and effective topic model for short texts, named GPU-DMM. Based on the Dirichlet Multinomial Mixture (DMM) model, GPU-DMM promotes the semantically related words under the same topic during the sampling process by using the generalized Polya urn (GPU) model. In this sense, the background knowledge about word semantic relatedness learned from millions of external documents can be easily exploited to improve topic modeling for short texts. Through extensive experiments on two real-world short text collections in two languages, we show that GPU-DMM achieves comparable or better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to the best accuracy in text classification task, which is used as an indirect evaluation.},
  isbn = {978-1-4503-4069-4},
  keywords = {short texts,topic model,word embeddings}
}

@inproceedings{lim_large-scale_2023,
  title = {Large-{{Scale Correlation Analysis}} of {{Automated Metrics}} for {{Topic Models}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lim, Jia Peng and Lauw, Hady},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {13874--13898},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.776},
  urldate = {2024-06-13},
  abstract = {Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling approach to mine topics for the purpose of metric evaluation, and conduct the analysis via three large corpora showing that certain automated coherence metrics are correlated. Moreover, we extend the analysis to measure topical differences between corpora. Lastly, we examine the reliability of human judgement by conducting an extensive user study, which is designed as an amalgamation of different proxy tasks to derive a finer insight into the human decision-making processes. Our findings reveal some correlation between automated coherence metrics and human judgement, especially for generic corpora.},
  file = {/Users/ivang/Zotero/storage/VTWFHIIX/Lim and Lauw - 2023 - Large-Scale Correlation Analysis of Automated Metr.pdf}
}

@misc{liu_roberta_2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2024-02-12},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/CYW6889A/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/ivang/Zotero/storage/TUCM4RP9/1907.html}
}

@misc{marjanen_topic_2020,
  title = {Topic Modelling Discourse Dynamics in Historical Newspapers},
  author = {Marjanen, Jani and Zosa, Elaine and Hengchen, Simon and Pivovarova, Lidia and Tolonen, Mikko},
  year = {2020},
  month = nov,
  number = {arXiv:2011.10428},
  eprint = {2011.10428},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.10428},
  urldate = {2024-01-03},
  abstract = {This paper addresses methodological issues in diachronic data analysis for historical research. We apply two families of topic models (LDA and DTM) on a relatively large set of historical newspapers, with the aim of capturing and understanding discourse dynamics. Our case study focuses on newspapers and periodicals published in Finland between 1854 and 1917, but our method can easily be transposed to any diachronic data. Our main contributions are a) a combined sampling, training and inference procedure for applying topic models to huge and imbalanced diachronic text collections; b) a discussion on the differences between two topic models for this type of data; c) quantifying topic prominence for a period and thus a generalization of document-wise topic assignment to a discourse level; and d) a discussion of the role of humanistic interpretation with regard to analysing discourse dynamics through topic models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/GP8XBYWS/Marjanen et al. - 2020 - Topic modelling discourse dynamics in historical n.pdf;/Users/ivang/Zotero/storage/SBH7MDR6/2011.html}
}

@inproceedings{mcauliffe_supervised_2007,
  title = {Supervised {{Topic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mcauliffe, Jon and Blei, David},
  year = {2007},
  volume = {20},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-01-10},
  abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
  file = {/Users/ivang/Zotero/storage/PQT3JGCB/Mcauliffe and Blei - 2007 - Supervised Topic Models.pdf}
}

@inproceedings{mcinnes_accelerated_2017,
  title = {Accelerated {{Hierarchical Density Based Clustering}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {McInnes, Leland and Healy, John},
  year = {2017},
  month = nov,
  pages = {33--42},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2017.12},
  urldate = {2024-03-02},
  abstract = {We present an accelerated algorithm for hierarchical density based clustering. Our new algorithm improves upon HDBSCAN*, which itself provided a significant qualitative improvement over the popular DBSCAN algorithm. The accelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while supporting variable density clusters, and eliminating the need for the difficult to tune distance scale parameter epsilon. This makes accelerated HDBSCAN* the default choice for density based clustering.},
  keywords = {Acceleration,Algorithm design and analysis,clustering,Clustering algorithms,Couplings,Data analysis,density based clustering,Density functional theory,hierarchical clustering,Robustness},
  file = {/Users/ivang/Zotero/storage/27SH6MG6/McInnes and Healy - 2017 - Accelerated Hierarchical Density Based Clustering.pdf}
}

@article{mcinnes_hdbscan_2017,
  title = {Hdbscan: {{Hierarchical}} Density Based Clustering},
  shorttitle = {Hdbscan},
  author = {McInnes, Leland and Healy, John and Astels, Steve},
  year = {2017},
  month = mar,
  journal = {The Journal of Open Source Software},
  volume = {2},
  number = {11},
  pages = {205},
  issn = {2475-9066},
  doi = {10.21105/joss.00205},
  urldate = {2023-12-19},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/T78F3FCR/McInnes et al. - 2017 - hdbscan Hierarchical density based clustering.pdf}
}

@misc{mcinnes_umap_2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  number = {arXiv:1802.03426},
  eprint = {1802.03426},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ivang/Zotero/storage/DZT2VGYZ/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/Users/ivang/Zotero/storage/D7PZ7JKE/1802.html}
}

@inproceedings{miao_neural_2016,
  title = {Neural {{Variational Inference}} for {{Text Processing}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Miao, Yishu and Yu, Lei and Blunsom, Phil},
  year = {2016},
  month = jun,
  pages = {1727--1736},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-01-10},
  abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/DU6L8N49/Miao et al. - 2016 - Neural Variational Inference for Text Processing.pdf}
}

@misc{mikolov_efficient_2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/FW24HNAW/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/ivang/Zotero/storage/GZXLMXDW/1301.html}
}

@article{mimno_optimizing_nodate,
  title = {Optimizing {{Semantic Coherence}} in {{Topic Models}}},
  author = {Mimno, David and Wallach, Hanna and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
  langid = {english},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/Z84HS8AL/Mimno et al. - Optimizing Semantic Coherence in Topic Models.pdf}
}

@misc{moody_mixing_2016,
  title = {Mixing {{Dirichlet Topic Models}} and {{Word Embeddings}} to {{Make}} Lda2vec},
  author = {Moody, Christopher E.},
  year = {2016},
  month = may,
  number = {arXiv:1605.02019},
  eprint = {1605.02019},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1605.02019},
  urldate = {2024-01-10},
  abstract = {Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/EWLCQACH/Moody - 2016 - Mixing Dirichlet Topic Models and Word Embeddings .pdf;/Users/ivang/Zotero/storage/BRLXUCQ8/1605.html}
}

@misc{muennighoff_mteb_2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2023},
  month = mar,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-26},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/ivang/Zotero/storage/L7RZLJ64/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf}
}

@inproceedings{musil_exploring_2024,
  title = {Exploring {{Interpretability}} of {{Independent Components}} of {{Word Embeddings}} with {{Automated Word Intruder Test}}},
  booktitle = {Proceedings of the 2024 {{Joint International Conference}} on {{Computational Linguistics}}, {{Language Resources}} and {{Evaluation}} ({{LREC-COLING}} 2024)},
  author = {Musil, Tom{\'a}{\v s} and Mare{\v c}ek, David},
  editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  year = {2024},
  month = may,
  pages = {6922--6928},
  publisher = {{ELRA and ICCL}},
  address = {Torino, Italia},
  urldate = {2024-05-29},
  abstract = {Independent Component Analysis (ICA) is an algorithm originally developed for finding separate sources in a mixed signal, such as a recording of multiple people in the same room speaking at the same time. Unlike Principal Component Analysis (PCA), ICA permits the representation of a word as an unstructured set of features, without any particular feature being deemed more significant than the others. In this paper, we used ICA to analyze word embeddings. We have found that ICA can be used to find semantic features of the words and these features can easily be combined to search for words that satisfy the combination. We show that most of the independent components represent such features. To quantify the interpretability of the components, we use the word intruder test, performed both by humans and by large language models. We propose to use the automated version of the word intruder test as a fast and inexpensive way of quantifying vector interpretability without the need for human effort.},
  file = {/Users/ivang/Zotero/storage/3EWK5BBU/Musil and Mareček - 2024 - Exploring Interpretability of Independent Componen.pdf}
}

@inproceedings{nallapati_multiscale_2007,
  title = {Multiscale Topic Tomography},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Nallapati, Ramesh M. and Ditmore, Susan and Lafferty, John D. and Ung, Kin},
  year = {2007},
  month = aug,
  series = {{{KDD}} '07},
  pages = {520--529},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1281192.1281249},
  urldate = {2024-01-08},
  abstract = {Modeling the evolution of topics with time is of great value in automatic summarization and analysis of large document collections. In this work, we propose a new probabilistic graphical model to address this issue. The new model, which we call the Multiscale Topic Tomography Model (MTTM), employs non-homogeneous Poisson processes to model generation of word-counts. The evolution of topics is modeled through a multi-scale analysis using Haar wavelets. One of the new features of the model is its modeling the evolution of topics at various time-scales of resolution, allowing the user to zoom in and out of the time-scales. Our experiments on Science data using the new model uncovers some interesting patterns in topics. The new model is also comparable to LDA in predicting unseen data as demonstrated by our perplexity experiments.},
  isbn = {978-1-59593-609-7},
  keywords = {poisson,temporal evolution,time-scale,topic modeling},
  file = {/Users/ivang/Zotero/storage/DSB7B22N/Nallapati et al. - 2007 - Multiscale topic tomography.pdf}
}

@inproceedings{newman_automatic_2010,
  title = {Automatic {{Evaluation}} of {{Topic Coherence}}},
  booktitle = {Human {{Language Technologies}}: {{The}} 2010 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
  editor = {Kaplan, Ron and Burstein, Jill and Harper, Mary and Penn, Gerald},
  year = {2010},
  month = jun,
  pages = {100--108},
  publisher = {Association for Computational Linguistics},
  address = {Los Angeles, California},
  urldate = {2024-02-29},
  file = {/Users/ivang/Zotero/storage/ZJ9SS94L/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf}
}

@inproceedings{newman_automatic_2010-1,
  title = {Automatic {{Evaluation}} of {{Topic Coherence}}},
  booktitle = {Human {{Language Technologies}}: {{The}} 2010 {{Annual Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
  editor = {Kaplan, Ron and Burstein, Jill and Harper, Mary and Penn, Gerald},
  year = {2010},
  month = jun,
  pages = {100--108},
  publisher = {Association for Computational Linguistics},
  address = {Los Angeles, California},
  urldate = {2024-06-04},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/ZDTQY5N6/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf}
}

@inproceedings{newman_evaluating_2010,
  title = {Evaluating Topic Models for Digital Libraries},
  booktitle = {Proceedings of the 10th Annual Joint Conference on {{Digital}} Libraries},
  author = {Newman, David and Noh, Youn and Talley, Edmund and Karimi, Sarvnaz and Baldwin, Timothy},
  year = {2010},
  month = jun,
  pages = {215--224},
  publisher = {ACM},
  address = {Gold Coast Queensland Australia},
  doi = {10.1145/1816123.1816156},
  urldate = {2024-05-24},
  isbn = {978-1-4503-0085-8},
  langid = {english},
  keywords = {Skimmed},
  file = {/Users/ivang/Zotero/storage/67XBV9CS/Newman et al. - 2010 - Evaluating topic models for digital libraries.pdf}
}

@article{newman_external_2011,
  title = {External {{Evaluation}} of {{Topic Models}}},
  author = {Newman, David and Karimi, Sarvnaz and Cavedon, Lawrence},
  year = {2011},
  month = jan,
  journal = {ADCS 2009 - Proceedings of the Fourteenth Australasian Document Computing Symposium},
  abstract = {Topic models can learn topics that are highly interpretable, semantically-coherent and can be used similarly to subject headings. But sometimes learned topics are lists of words that do not convey much useful information. We propose models that score the usefulness of topics, including a model that computes a score based on pointwise mutual information (PMI) of pairs of words in a topic. Our PMI score, computed using word-pair co-occurrence statistics from external data sources, has relatively good agreement with human scoring. We also show that the ability to identify less useful topics can improve the results of a topic-based document similarity metric.},
  file = {/Users/ivang/Zotero/storage/CMUGKJVN/Newman et al. - 2011 - External Evaluation of Topic Models.pdf}
}

@article{nguyen_improving_2015,
  title = {Improving {{Topic Models}} with {{Latent Feature Word Representations}}},
  author = {Nguyen, Dat Quoc and Billingsley, Richard and Du, Lan and Johnson, Mark},
  year = {2015},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {299--313},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00140},
  urldate = {2024-01-10},
  abstract = {Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/9P8WKBJ5/Nguyen et al. - 2015 - Improving Topic Models with Latent Feature Word Re.pdf}
}

@incollection{nicholson_search_2020,
  title = {In {{Search}} of {{America}}: {{Topic}} Modelling Nineteenth-Century Newspaper Archives},
  shorttitle = {In {{Search}} of {{America}}},
  booktitle = {Journalism {{History}} and {{Digital Archives}}},
  author = {Nicholson, Bob, Quintus Van Galen},
  year = {2020},
  publisher = {Routledge},
  abstract = {This article considers how, and why, "Topic Modelling" tools can be used to analyse historical newspaper archives. While a growing number of media and communication studies projects have applied these techniques to corpuses of born-digital journalism, using the same tools to analyse large-scale collections of historical newspapers requires us to overcome additional technological and methodological challenges. Our discussion is framed around a historical case study examining references to the United States in the 19th Century British Library Newspaper Archive. The article begins by highlighting the problems that researchers of both digital and historical journalism face when attempting to deal with an enormous body of evidence. Next, it argues that Topic Modelling offers one potential solution to these problems by providing a way to ``distant read" the archive. The remainder of the article is divided into five experiments that demonstrate how Topic Modelling can be applied to a series of research questions, each of which is applicable to other projects that might make use of newspaper archives. As well as demonstrating the investigative potential of topic modelling, the article also highlights the practical and technological barriers that currently undermine its effectiveness, particularly when it is applied to archives of historical material.},
  isbn = {978-1-00-309884-3}
}

@article{nigam_text_2000,
  title = {Text {{Classification}} from {{Labeled}} and {{Unlabeled Documents}} Using {{EM}}},
  author = {Nigam, Kamal and Mccallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
  year = {2000},
  month = may,
  journal = {Machine Learning},
  volume = {39},
  number = {2},
  pages = {103--134},
  issn = {1573-0565},
  doi = {10.1023/A:1007692713085},
  urldate = {2024-01-10},
  abstract = {This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.},
  langid = {english},
  keywords = {Bayesian learning,combining labeled and unlabeled data,Expectation-Maximization,integrating supervised and unsupervised learning,text classification},
  file = {/Users/ivang/Zotero/storage/8VX4LP4H/Nigam et al. - 2000 - Text Classification from Labeled and Unlabeled Doc.pdf}
}

@misc{noauthor_explosionspacy_nodate,
  title = {Explosion/{{spaCy}}: {{Industrial-strength Natural Language Processing}} ({{NLP}}) in {{Python}}},
  urldate = {2024-02-28},
  howpublished = {https://github.com/explosion/spaCy/tree/master},
  file = {/Users/ivang/Zotero/storage/AMK2V8NF/master.html}
}

@misc{noauthor_github_nodate,
  title = {{{GitHub}}: {{Let}}'s Build from Here},
  urldate = {2024-01-06},
  howpublished = {https://github.com/},
  file = {/Users/ivang/Zotero/storage/GRGCH7NI/github.com.html}
}

@misc{noauthor_hugging_2024,
  title = {Hugging {{Face}} -- {{The AI}} Community Building the Future.},
  year = {2024},
  month = jan,
  urldate = {2024-01-06},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/},
  file = {/Users/ivang/Zotero/storage/GNXGQWGY/huggingface.co.html}
}

@misc{noauthor_microsoftmpnet-base_nodate,
  title = {Microsoft/Mpnet-Base {$\cdot$} {{Hugging Face}}},
  urldate = {2024-02-23},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/microsoft/mpnet-base},
  file = {/Users/ivang/Zotero/storage/MQBFLCHC/mpnet-base.html}
}

@misc{noauthor_sentence-transformersall-mpnet-base-v2_2024,
  title = {Sentence-Transformers/All-Mpnet-Base-v2 {$\cdot$} {{Hugging Face}}},
  year = {2024},
  month = jan,
  urldate = {2024-02-23},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/sentence-transformers/all-mpnet-base-v2},
  file = {/Users/ivang/Zotero/storage/9AZVJB29/all-mpnet-base-v2.html}
}

@misc{noauthor_wolfram_nodate,
  title = {Wolfram {{Data Repository}}: {{Computable Access}} to {{Curated Data}}},
  urldate = {2024-01-06},
  howpublished = {https://datarepository.wolframcloud.com/},
  file = {/Users/ivang/Zotero/storage/G7Y4HVGS/datarepository.wolframcloud.com.html}
}

@misc{noauthor_zero-shot_2020,
  title = {Zero-{{Shot Learning}} in {{Modern NLP}}},
  year = {2020},
  month = may,
  journal = {Joe Davison Blog},
  urldate = {2024-06-06},
  abstract = {State-of-the-art NLP models for text classification without annotated data},
  howpublished = {https://joeddav.github.io/blog/2020/05/29/ZSL.html},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/VVP834B5/ZSL.html}
}

@article{oneill_analysis_2016,
  title = {An Analysis of Topic Modelling for Legislative Texts},
  author = {O'Neill, James and Robin, Cecile and O'Brien, Leona and Buitelaar, Paul},
  year = {2016},
  publisher = {CEUR Workshop Proceedings},
  issn = {1613-0073},
  urldate = {2024-01-03},
  abstract = {The uprise of legislative documents within the past decade has risen dramatically, making it difficult for law practitioners to attend to legislation such as Statutory Instrument orders and Acts. This work focuses on the use of topic models for summarizing and visualizing British legislation, with a view toward easier browsing and identification of salient legal topics and their respective set of topic specific terms. We provide an initial qualitative evaluation from a legal expert on how the models have performed by ranking them for each jurisdiction according to topic coherency and relevance.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/PDJ57TUH/O'Neill et al. - 2016 - An analysis of topic modelling for legislative tex.pdf}
}

@article{pandove_systematic_2018,
  title = {Systematic {{Review}} of {{Clustering High-Dimensional}} and {{Large Datasets}}},
  author = {Pandove, Divya and Goel, Shivan and Rani, Rinkl},
  year = {2018},
  month = jan,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {12},
  number = {2},
  pages = {16:1--16:68},
  issn = {1556-4681},
  doi = {10.1145/3132088},
  urldate = {2024-02-28},
  abstract = {Technological advancement has enabled us to store and process huge amount of data in relatively short spans of time. The nature of data is rapidly changing, particularly its dimensionality is more commonly multi- and high-dimensional. There is an immediate need to expand our focus to include analysis of high-dimensional and large datasets. Data analysis is becoming a mammoth task, due to incremental increase in data volume and complexity in terms of heterogony of data. It is due to this dynamic computing environment that the existing techniques either need to be modified or discarded to handle new data in multiple high-dimensions. Data clustering is a tool that is used in many disciplines, including data mining, so that meaningful knowledge can be extracted from seemingly unstructured data. The aim of this article is to understand the problem of clustering and various approaches addressing this problem. This article discusses the process of clustering from both microviews (data treating) and macroviews (overall clustering process). Different distance and similarity measures, which form the cornerstone of effective data clustering, are also identified. Further, an in-depth analysis of different clustering approaches focused on data mining, dealing with large-scale datasets is given. These approaches are comprehensively compared to bring out a clear differentiation among them. This article also surveys the problem of high-dimensional data and the existing approaches, that makes it more relevant. It also explores the latest trends in cluster analysis, and the real-life applications of this concept. This survey is exhaustive as it tries to cover all the aspects of clustering in the field of data mining.},
  keywords = {Cluster analysis,clustering tendency,data clustering applications,data clustering process,dimensionality reduction,large scale data mining},
  file = {/Users/ivang/Zotero/storage/V59Q85QB/Pandove et al. - 2018 - Systematic Review of Clustering High-Dimensional a.pdf}
}

@article{paul_discovering_2014,
  title = {Discovering {{Health Topics}} in {{Social Media Using Topic Models}}},
  author = {Paul, Michael J. and Dredze, Mark},
  year = {2014},
  month = aug,
  journal = {PLOS ONE},
  volume = {9},
  number = {8},
  pages = {e103408},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0103408},
  urldate = {2024-01-03},
  abstract = {By aggregating self-reported health statuses across millions of users, we seek to characterize the variety of health information discussed in Twitter. We describe a topic modeling framework for discovering health topics in Twitter, a social media website. This is an exploratory approach with the goal of understanding what health topics are commonly discussed in social media. This paper describes in detail a statistical topic model created for this purpose, the Ailment Topic Aspect Model (ATAM), as well as our system for filtering general Twitter data based on health keywords and supervised classification. We show how ATAM and other topic models can automatically infer health topics in 144 million Twitter messages from 2011 to 2013. ATAM discovered 13 coherent clusters of Twitter messages, some of which correlate with seasonal influenza (r = 0.689) and allergies (r = 0.810) temporal surveillance data, as well as exercise (r = .534) and obesity (r = -.631) related geographic survey data in the United States. These results demonstrate that it is possible to automatically discover topics that attain statistically significant correlations with ground truth data, despite using minimal human supervision and no historical data to train the model, in contrast to prior work. Additionally, these results demonstrate that a single general-purpose model can identify many different health topics in social media.},
  langid = {english},
  keywords = {Allergies,Behavioral and social aspects of health,Cancers and neoplasms,Diet,Exercise,Influenza,Social media,Twitter},
  file = {/Users/ivang/Zotero/storage/EFPF2VY6/Paul and Dredze - 2014 - Discovering Health Topics in Social Media Using To.pdf}
}

@inproceedings{pennacchiotti_investigating_2011,
  title = {Investigating Topic Models for Social Media User Recommendation},
  booktitle = {Proceedings of the 20th International Conference Companion on {{World}} Wide Web},
  author = {Pennacchiotti, Marco and Gurumurthy, Siva},
  year = {2011},
  month = mar,
  series = {{{WWW}} '11},
  pages = {101--102},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1963192.1963244},
  urldate = {2024-01-03},
  abstract = {This paper presents a user recommendation system that recommends to a user new friends having similar interests. We automatically discover users' interests using Latent Dirichlet Allocation (LDA), a linguistic topic model that represents users as mixtures of topics. Our system is able to recommend friends for 4 million users with high recall, outperforming existing strategies based on graph analysis.},
  isbn = {978-1-4503-0637-9},
  keywords = {LDA,social media,topic models,user recommendation}
}

@inproceedings{pennington_glove_2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2024-01-12},
  file = {/Users/ivang/Zotero/storage/GPAT6LEM/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{porteous_fast_2008,
  title = {Fast Collapsed Gibbs Sampling for Latent Dirichlet Allocation},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Porteous, Ian and Newman, David and Ihler, Alexander and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
  year = {2008},
  month = aug,
  series = {{{KDD}} '08},
  pages = {569--577},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1401890.1401960},
  urldate = {2024-01-08},
  abstract = {In this paper we introduce a novel collapsed Gibbs sampling method for the widely used latent Dirichlet allocation (LDA) model. Our new method results in significant speedups on real world text corpora. Conventional Gibbs sampling schemes for LDA require O(K) operations per sample where K is the number of topics in the model. Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample. On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA. No approximations are necessary, and we show that our fast sampling scheme produces exactly the same results as the standard (but slower) sampling scheme. Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes. For the PubMed collection of over 8 million documents with a required computation time of 6 CPU months for LDA, our speedup of 5.7 can save 5 CPU months of computation.},
  isbn = {978-1-60558-193-4},
  keywords = {latent dirichlet allocation,sampling},
  file = {/Users/ivang/Zotero/storage/3FLJJVDK/Porteous et al. - 2008 - Fast collapsed gibbs sampling for latent dirichlet.pdf}
}

@inproceedings{qiang_topic_2017,
  title = {Topic {{Modeling}} over {{Short Texts}} by {{Incorporating Word Embeddings}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Qiang, Jipeng and Chen, Ping and Wang, Tong and Wu, Xindong},
  editor = {Kim, Jinho and Shim, Kyuseok and Cao, Longbing and Lee, Jae-Gil and Lin, Xuemin and Moon, Yang-Sae},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {363--374},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-57529-2_29},
  abstract = {Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn semantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo-texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.},
  isbn = {978-3-319-57529-2},
  langid = {english},
  keywords = {Short text,Topic modeling,Word embeddings},
  file = {/Users/ivang/Zotero/storage/AUYAPDCJ/Qiang et al. - 2017 - Topic Modeling over Short Texts by Incorporating W.pdf}
}

@inproceedings{quan_short_2015,
  title = {Short and {{Sparse Text Topic Modeling}} via {{Self-Aggregation}}},
  booktitle = {Proceedings of the {{Twenty-Fourth International Joint Conference}} on {{Artificial Intelligence}} ({{IJCAI}} 2015)},
  author = {Quan, Xiaojun and Kit, Chunyu and Ge, Yong and Pan, Sinno Jialin},
  year = {2015},
  month = jul,
  pages = {2270--2276},
  publisher = {AAAI Press/International Joint Conferences on Artificial Intelligence},
  urldate = {2024-01-09},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/UL5SGJ36/Quan et al. - 2015 - Short and Sparse Text Topic Modeling via Self-Aggr.pdf}
}

@article{radford_improving_nodate,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/FMFF5CBH/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radford_language_nodate,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/JFRQKAKF/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{raffel_exploring_2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
  issn = {1533-7928},
  urldate = {2024-10-02},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  file = {/Users/ivang/Zotero/storage/DCBVWVR6/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;/Users/ivang/Zotero/storage/XMYXWMNI/text-to-text-transfer-transformer.html}
}

@article{raj_p_m_sentiment_2022,
  title = {Sentiment Analysis, Opinion Mining and Topic Modelling of Epics and Novels Using Machine Learning Techniques},
  author = {Raj P M, Krishna and Sai D, Jagadeesh},
  year = {2022},
  month = jan,
  journal = {Materials Today: Proceedings},
  series = {{{CMAE}}'21},
  volume = {51},
  pages = {576--584},
  issn = {2214-7853},
  doi = {10.1016/j.matpr.2021.06.001},
  urldate = {2024-01-03},
  abstract = {Sentiment analysis systems can collect and automatically structure unstructured information by collecting public views of services, products, policy, brands, etc. This information is of major value in the fields of marketing analysis, public relations, product reviews, net promoter evaluations, customer feedback and client reviews. Literary works, on the other hand, are less susceptible to computational analysis because there are no immediate commercial incentives. However, similar techniques can be used to evaluate literary work, comprehend the underlying social network, and obtain or validate literary work. This project is about analyzing the book's characters and predicting their characteristics and relationships with one another. A lot of human effort is expended during the adaptation of a novel/book in any form, which is inconvenient and undesirable. Furthermore, the human brain has a tendency to overlook a number of minor details about the events/characters in the book. The scenario described above can frequently result in inaccuracies in the adaptation's plot. As a result, the project is an innovation that aims to aid in the easy and accurate adaptation of a book, making the process much simpler and precise. Machine learning (supervised and unattended) and lexical approaches include the current sentiment analysis techniques. The model's goal is to scan the massive amounts of text in the book. Following digitization, the model will display interesting ideas derived from the given book using a combination of natural language processing, feelings and emotions analysis, and social network analysis methodology.},
  keywords = {Latent Dirichlet Allocation Social Network Analysis,Natural language processing,Sentiment analysis,Vader Sentiment Analysis}
}

@book{rehurek_software_2010,
  title = {Software {{Framework}} for {{Topic Modelling}} with {{Large Corpora}}},
  author = {{\v R}eh{\r u}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  month = may,
  publisher = {University of Malta},
  urldate = {2024-03-02},
  abstract = {Large corpora are ubiquitous in today's world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model (VSM). We identify gap in existing VSM implementations, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. In this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library DML-CZ.},
  isbn = {978-2-9517408-6-0},
  langid = {english}
}

@misc{reimers_making_2020,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2020},
  month = oct,
  number = {arXiv:2004.09813},
  eprint = {2004.09813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09813},
  urldate = {2024-02-28},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/8QI6A52F/Reimers and Gurevych - 2020 - Making Monolingual Sentence Embeddings Multilingua.pdf;/Users/ivang/Zotero/storage/PWI3SHD7/2004.html}
}

@misc{reimers_sentence-bert_2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/FG3TQT3W/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf;/Users/ivang/Zotero/storage/BI5MRI2U/1908.html}
}

@article{rijcken_towards_nodate,
  title = {Towards {{Interpreting Topic Models}} with {{ChatGPT}}},
  author = {Rijcken, Emil and Scheepers, Floortje and Zervanou, Kalliopi and Spruit, Marco and Mosteiro, Pablo and Kaymak, Uzay},
  abstract = {Topic modeling has become a popular approach to identify semantic structures in text corpora. Despite its wide applications, interpreting the outputs of topic models remains challenging. This paper presents an initial study regarding a new approach to better understand this output, leveraging the large language model ChatGPT. Our approach is built on a threestage process where we first use topic modeling to identify the main topics in the corpus. Then, we ask a domain expert to assign themes to these topics and prompt ChatGPT to generate humanreadable summaries of the topics. Lastly, we compare the humanand machine-produced interpretations. The domain expert found half of ChatGPT's descriptions useful. This explorative work demonstrates ChatGPT's capability to describe topics accurately and provide useful insights if prompted accurately.},
  langid = {english},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/RNLJGFBQ/Rijcken et al. - Towards Interpreting Topic Models with ChatGPT.pdf}
}

@inproceedings{shahapure_cluster_2020,
  title = {Cluster {{Quality Analysis Using Silhouette Score}}},
  booktitle = {2020 {{IEEE}} 7th {{International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Shahapure, Ketan Rajshekhar and Nicholas, Charles},
  year = {2020},
  month = oct,
  pages = {747--748},
  doi = {10.1109/DSAA49011.2020.00096},
  urldate = {2024-10-08},
  abstract = {Clustering is an important phase in data mining. Selecting the number of clusters in a clustering algorithm, e.g. choosing the best value of k in the various k-means algorithms [1], can be difficult. We studied the use of silhouette scores and scatter plots to suggest, and then validate, the number of clusters we specified in running the k-means clustering algorithm on two publicly available data sets. Scikit-learn's [4] silhouette score method, which is a measure of the quality of a cluster, was used to find the mean silhouette co-efficient of all the samples for different number of clusters. The highest silhouette score indicates the optimal number of clusters. We present several instances of utilizing the silhouette score to determine the best value of k for those data sets.},
  keywords = {Benchmark testing,Clustering algorithms,Computer science,Electrical engineering,Inspection,Iris,Writing},
  file = {/Users/ivang/Zotero/storage/LMACNHQV/Shahapure and Nicholas - 2020 - Cluster Quality Analysis Using Silhouette Score.pdf}
}

@article{shahnaz_document_2006,
  title = {Document Clustering Using Nonnegative Matrix Factorization},
  author = {Shahnaz, Farial and Berry, Michael W. and Pauca, V. Paul and Plemmons, Robert J.},
  year = {2006},
  month = mar,
  journal = {Information Processing \& Management},
  volume = {42},
  number = {2},
  pages = {373--386},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2004.11.005},
  urldate = {2024-03-01},
  abstract = {A methodology for automatically identifying and clustering semantic features or topics in a heterogeneous text collection is presented. Textual data is encoded using a low rank nonnegative matrix factorization algorithm to retain natural data nonnegativity, thereby eliminating the need to use subtractive basis vector and encoding calculations present in other techniques such as principal component analysis for semantic feature abstraction. Existing techniques for nonnegative matrix factorization are reviewed and a new hybrid technique for nonnegative matrix factorization is proposed. Performance evaluations of the proposed method are conducted on a few benchmark text collections used in standard topic detection studies.},
  keywords = {Conjugate gradient,Constrained least squares,Nonnegative matrix factorization,Text mining},
  file = {/Users/ivang/Zotero/storage/FAP6ZKHV/Shahnaz et al. - 2006 - Document clustering using nonnegative matrix facto.pdf;/Users/ivang/Zotero/storage/CV5SQK6Q/S0306457304001542.html}
}

@article{silveira_topic_nodate,
  title = {Topic {{Modelling}} of {{Legal Documents}} via {{LEGAL-BERT}}},
  author = {Silveira, Raquel and Fernandes, Carlos G O and Neto, Jo{\~a}o A Monteiro and Furtado, Vasco and Filho, Ernesto Pimentel},
  abstract = {Legal text processing is a challenging task for modeling approaches due to the peculiarities inherent to its features, such as long texts and their technical vocabulary. Topic modeling consists of discovering a semantic structure in the text. This way, it requires specific approaches. The relevant topics strongly depend on the context in which the legal documents will be presented. This work aims to describe and evaluate the use of BERTopic for topic modeling in legal documents. The authors have focused on a subset of landmark cases from the US Caselaw dataset to evaluate the impact of topic modeling, via domain-specific embeddings pre-trained from LEGAL-BERT. The research investigated different variations of generating sentence embeddings from the cases. Results here presented demonstrate that considering the references to statutory law (e.g. US Code) during the process of text embeddings improves the quality of topic modeling.},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/2MAWZZS5/Silveira et al. - Topic Modelling of Legal Documents via LEGAL-BERT.pdf}
}

@inproceedings{smith_closing_2018,
  title = {Closing the {{Loop}}: {{User-Centered Design}} and {{Evaluation}} of a {{Human-in-the-Loop Topic Modeling System}}},
  shorttitle = {Closing the {{Loop}}},
  booktitle = {23rd {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Smith, Alison and Kumar, Varun and {Boyd-Graber}, Jordan and Seppi, Kevin and Findlater, Leah},
  year = {2018},
  month = mar,
  pages = {293--304},
  publisher = {ACM},
  address = {Tokyo Japan},
  doi = {10.1145/3172944.3172965},
  urldate = {2024-05-24},
  isbn = {978-1-4503-4945-1},
  langid = {english},
  keywords = {Read},
  file = {/Users/ivang/Zotero/storage/VEHG58K5/Smith et al. - 2018 - Closing the Loop User-Centered Design and Evaluat.pdf}
}

@inproceedings{snoek_practical_2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-29},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ``black art'' requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.},
  file = {/Users/ivang/Zotero/storage/68ZW6EZC/Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf}
}

@incollection{steinbach_challenges_2004,
  title = {The {{Challenges}} of {{Clustering High Dimensional Data}}},
  booktitle = {New {{Directions}} in {{Statistical Physics}}: {{Econophysics}}, {{Bioinformatics}}, and {{Pattern Recognition}}},
  author = {Steinbach, Michael and Ert{\"o}z, Levent and Kumar, Vipin},
  editor = {Wille, Luc T.},
  year = {2004},
  pages = {273--309},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-08968-2_16},
  urldate = {2024-02-28},
  abstract = {Cluster analysis divides data into groups (clusters) for the purposes of summarization or improved understanding. For example, cluster analysis has been used to group related documents for browsing, to find genes and proteins that have similar functionality, or as a means of data compression. While clustering has a long history and a large number of clustering techniques have been developed in statistics, pattern recognition, data mining, and other fields, significant challenges still remain. In this chapter we provide a short introduction to cluster analysis, and then focus on the challenge of clustering high dimensional data. We present a brief overview of several recent techniques, including a more detailed description of recent work of our own which uses a concept-based clustering approach.},
  isbn = {978-3-662-08968-2},
  langid = {english},
  keywords = {Concept Space,Document Cluster,Frequent Itemset,Grid Cell,High Dimensional Data},
  file = {/Users/ivang/Zotero/storage/Q5C3JWZW/Steinbach et al. - 2004 - The Challenges of Clustering High Dimensional Data.pdf}
}

@inproceedings{teh_sharing_2004,
  title = {Sharing {{Clusters}} among {{Related Groups}}: {{Hierarchical Dirichlet Processes}}},
  shorttitle = {Sharing {{Clusters}} among {{Related Groups}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Teh, Yee and Jordan, Michael and Beal, Matthew and Blei, David},
  year = {2004},
  volume = {17},
  publisher = {MIT Press},
  urldate = {2024-01-08},
  abstract = {We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generaliza- tion to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.},
  file = {/Users/ivang/Zotero/storage/H9CFMUWV/Teh et al. - 2004 - Sharing Clusters among Related Groups Hierarchica.pdf}
}

@inproceedings{terragni_octis_2021,
  title = {{{OCTIS}}: {{Comparing}} and {{Optimizing Topic}} Models Is {{Simple}}!},
  shorttitle = {{{OCTIS}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Terragni, Silvia and Fersini, Elisabetta and Galuzzi, Bruno Giovanni and Tropeano, Pietro and Candelieri, Antonio},
  editor = {Gkatzia, Dimitra and Seddah, Djam{\'e}},
  year = {2021},
  month = apr,
  pages = {263--270},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.eacl-demos.31},
  urldate = {2023-12-19},
  abstract = {In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: https://github.com/MIND-Lab/OCTIS.},
  file = {/Users/ivang/Zotero/storage/696LQBPZ/Terragni et al. - 2021 - OCTIS Comparing and Optimizing Topic models is Si.pdf}
}

@misc{thompson_topic_2020,
  title = {Topic {{Modeling}} with {{Contextualized Word Representation Clusters}}},
  author = {Thompson, Laure and Mimno, David},
  year = {2020},
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-01-12},
  abstract = {Clustering token-level contextualized word representations produces output that shares many similarities with topic models for English text collections. Unlike clusterings of vocabulary-level word embeddings, the resulting models more naturally capture polysemy and can be used as a way of organizing documents. We evaluate token clusterings trained from several different output layers of popular contextualized language models. We find that BERT and GPT-2 produce high quality clusterings, but RoBERTa does not. These cluster models are simple, reliable, and can perform as well as, if not better than, LDA topic models, maintaining high topic quality even when the number of topics is large relative to the size of the local collection.},
  howpublished = {https://arxiv.org/abs/2010.12626v1},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/JUTMDYGQ/Thompson and Mimno - 2020 - Topic Modeling with Contextualized Word Representa.pdf}
}

@misc{van_der_maaten_visualizing_2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}. {\textbar} {{Journal}} of {{Machine Learning Research}} {\textbar} {{EBSCOhost}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  month = nov,
  volume = {9},
  number = {11},
  pages = {2579},
  issn = {1532-4435},
  urldate = {2024-09-17},
  howpublished = {https://openurl.ebsco.com/contentitem/gcd:36099312?sid=ebsco:plink:crawler\&id=ebsco:gcd:36099312},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/ZZQNMAHE/detailv2.html}
}

@article{vanschoren_openml_2014,
  title = {{{OpenML}}: Networked Science in Machine Learning},
  shorttitle = {{{OpenML}}},
  author = {Vanschoren, Joaquin and {van Rijn}, Jan N. and Bischl, Bernd and Torgo, Luis},
  year = {2014},
  month = jun,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {15},
  number = {2},
  eprint = {1407.7722},
  primaryclass = {cs},
  pages = {49--60},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/2641190.2641198},
  urldate = {2024-01-03},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/ivang/Zotero/storage/QYSEJC27/Vanschoren et al. - 2014 - OpenML networked science in machine learning.pdf;/Users/ivang/Zotero/storage/AFQX7M9U/1407.html}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-02-12},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/ivang/Zotero/storage/IMJRN72N/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vavasis_complexity_2010,
  title = {On the {{Complexity}} of {{Nonnegative Matrix Factorization}}},
  author = {Vavasis, Stephen A.},
  year = {2010},
  month = jan,
  journal = {SIAM Journal on Optimization},
  volume = {20},
  number = {3},
  pages = {1364--1377},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/070709967},
  urldate = {2024-01-08},
  abstract = {Nonnegative matrix factorization (NMF) is a dimension reduction method that has been widely used for numerous applications, including text mining, computer vision, pattern discovery, and bioinformatics. A mathematical formulation for NMF appears as a nonconvex optimization problem, and various types of algorithms have been devised to solve the problem. The alternating nonnegative least squares (ANLS) framework is a block coordinate descent approach for solving NMF, which was recently shown to be theoretically sound and empirically efficient. In this paper, we present a novel algorithm for NMF based on the ANLS framework. Our new algorithm builds upon the block principal pivoting method for the nonnegativity-constrained least squares problem that overcomes a limitation of the active set method. We introduce ideas that efficiently extend the block principal pivoting method within the context of NMF computation. Our algorithm inherits the convergence property of the ANLS framework and can easily be extended to other constrained NMF formulations. Extensive computational comparisons using data sets that are from real life applications as well as those artificially generated show that the proposed algorithm provides state-of-the-art performance in terms of computational speed.},
  file = {/Users/ivang/Zotero/storage/D3HF9TSV/Vavasis - 2010 - On the Complexity of Nonnegative Matrix Factorizat.pdf}
}

@inproceedings{viegas_cluhtm_2020,
  title = {{{CluHTM}} - {{Semantic Hierarchical Topic Modeling}} Based on {{CluWords}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Viegas, Felipe and Cunha, Washington and Gomes, Christian and Pereira, Ant{\^o}nio and Rocha, Leonardo and Goncalves, Marcos},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  pages = {8138--8150},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.724},
  urldate = {2024-01-11},
  abstract = {Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the ``ideal'' number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM's novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information -- when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the ``shape'' the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500\% over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.},
  file = {/Users/ivang/Zotero/storage/6GQAZLHL/Viegas et al. - 2020 - CluHTM - Semantic Hierarchical Topic Modeling base.pdf}
}

@inproceedings{viegas_cluwords_2019,
  title = {{{CluWords}}: {{Exploiting Semantic Word Clustering Representation}} for {{Enhanced Topic Modeling}}},
  shorttitle = {{{CluWords}}},
  booktitle = {Proceedings of the {{Twelfth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Viegas, Felipe and Canuto, S{\'e}rgio and Gomes, Christian and Luiz, Washington and Rosa, Thierson and Ribas, Sabir and Rocha, Leonardo and Gon{\c c}alves, Marcos Andr{\'e}},
  year = {2019},
  month = jan,
  series = {{{WSDM}} '19},
  pages = {753--761},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3289600.3291032},
  urldate = {2024-01-11},
  abstract = {In this paper, we advance the state-of-the-art in topic modeling by means of a new document representation based on pre-trained word embeddings for non-probabilistic matrix factorization. Specifically, our strategy, called CluWords, exploits the nearest words of a given pre-trained word embedding to generate meta-words capable of enhancing the document representation, in terms of both, syntactic and semantic information. The novel contributions of our solution include: (i)the introduction of a novel data representation for topic modeling based on syntactic and semantic relationships derived from distances calculated within a pre-trained word embedding space and (ii)the proposal of a new TF-IDF-based strategy, particularly developed to weight the CluWords. In our extensive experimentation evaluation, covering 12 datasets and 8 state-of-the-art baselines, we exceed (with a few ties) in almost cases, with gains of more than 50\% against the best baselines (achieving up to 80\% against some runner-ups). Finally, we show that our method is able to improve document representation for the task of automatic text classification.},
  isbn = {978-1-4503-5940-5},
  keywords = {data representation,topic modeling,word embedding}
}

@inproceedings{wallach_evaluation_2009,
  title = {Evaluation Methods for Topic Models},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
  year = {2009},
  month = jun,
  series = {{{ICML}} '09},
  pages = {1105--1112},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1553374.1553515},
  urldate = {2024-06-07},
  abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
  isbn = {978-1-60558-516-1},
  file = {/Users/ivang/Zotero/storage/6L7BFAV7/Wallach et al. - 2009 - Evaluation methods for topic models.pdf}
}

@misc{wang_continuous_2012,
  title = {Continuous {{Time Dynamic Topic Models}}},
  author = {Wang, Chong and Blei, David and Heckerman, David},
  year = {2012},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-01-08},
  abstract = {In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.},
  howpublished = {https://arxiv.org/abs/1206.3298v2},
  langid = {english},
  file = {/Users/ivang/Zotero/storage/I6KH4LST/Wang et al. - 2012 - Continuous Time Dynamic Topic Models.pdf}
}

@misc{wang_large_2023,
  title = {Large {{Language Models Are Zero-Shot Text Classifiers}}},
  author = {Wang, Zhiqiang and Pang, Yiran and Lin, Yanbin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01044},
  eprint = {2312.01044},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01044},
  urldate = {2024-06-06},
  abstract = {Retrained large language models (LLMs) have become extensively used across various sub-disciplines of natural language processing (NLP). In NLP, text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes. With the proposal of chain of thought prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with the step by step reasoning prompts, instead of conventional question and answer formats. The zero-shot LLMs in the text classification problems can alleviate these limitations by directly utilizing pretrained models to predict both seen and unseen classes. Our research primarily validates the capability of GPT models in text classification. We focus on effectively utilizing prompt strategies to various text classification scenarios. Besides, we compare the performance of zero shot LLMs with other state of the art text classification methods, including traditional machine learning methods, deep learning methods, and ZSL methods. Experimental results demonstrate that the performance of LLMs underscores their effectiveness as zero-shot text classifiers in three of the four datasets analyzed. The proficiency is especially advantageous for small businesses or teams that may not have extensive knowledge in text classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/7WVG7QHL/Wang et al. - 2023 - Large Language Models Are Zero-Shot Text Classifie.pdf;/Users/ivang/Zotero/storage/BIP3BZBK/2312.html}
}

@inproceedings{yan_biterm_2013,
  title = {A Biterm Topic Model for Short Texts},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}}},
  author = {Yan, Xiaohui and Guo, Jiafeng and Lan, Yanyan and Cheng, Xueqi},
  year = {2013},
  month = may,
  series = {{{WWW}} '13},
  pages = {1445--1456},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2488388.2488514},
  urldate = {2024-01-09},
  abstract = {Uncovering the topics within short texts, such as tweets and instant messages, has become an important task for many content analysis applications. However, directly applying conventional topic models (e.g. LDA and PLSA) on such short texts may not work well. The fundamental reason lies in that conventional topic models implicitly capture the document-level word co-occurrence patterns to reveal topics, and thus suffer from the severe data sparsity in short documents. In this paper, we propose a novel way for modeling topics in short texts, referred as biterm topic model (BTM). Specifically, in BTM we learn the topics by directly modeling the generation of word co-occurrence patterns (i.e. biterms) in the whole corpus. The major advantages of BTM are that 1) BTM explicitly models the word co-occurrence patterns to enhance the topic learning; and 2) BTM uses the aggregated patterns in the whole corpus for learning topics to solve the problem of sparse word co-occurrence patterns at document-level. We carry out extensive experiments on real-world short text collections. The results demonstrate that our approach can discover more prominent and coherent topics, and significantly outperform baseline methods on several evaluation metrics. Furthermore, we find that BTM can outperform LDA even on normal texts, showing the potential generality and wider usage of the new topic model.},
  isbn = {978-1-4503-2035-1},
  keywords = {biterm,content analysis,short text,topic model}
}

@incollection{yan_learning_2013,
  title = {Learning {{Topics}} in {{Short Texts}} by {{Non-negative Matrix Factorization}} on {{Term Correlation Matrix}}},
  booktitle = {Proceedings of the 2013 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Yan, Xiaohui and Guo, Jiafeng and Liu, Shenghua and Cheng, Xueqi and Wang, Yanfeng},
  year = {2013},
  month = may,
  series = {Proceedings},
  pages = {749--757},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972832.83},
  urldate = {2024-01-08},
  abstract = {Nowadays, short texts are very prevalent in various web applications, such as microblogs, instant messages. The severe sparsity of short texts hinders existing topic models to learn reliable topics. In this paper, we propose a novel way to tackle this problem. The key idea is to learn topics by exploring term correlation data, rather than the high-dimensional and sparse term occurrence information in documents. Such term correlation data is less sparse and more stable with the increase of the collection size, and can well capture the necessary information for topic learning. To obtain reliable topics from term correlation data, we first introduce a novel way to compute term correlation in short texts by representing each term with its co-occurred terms. Then we formulated the topic learning problem as symmetric non-negative matrix factorization on the term correlation matrix. After learning the topics, we can easily infer the topics of documents. Experimental results on three data sets show that our method provides substantially better performance than the baseline methods.},
  isbn = {978-1-61197-262-7},
  file = {/Users/ivang/Zotero/storage/U96SE3XQ/Yan et al. - 2013 - Learning Topics in Short Texts by Non-negative Mat.pdf}
}

@inproceedings{yao_efficient_2009,
  title = {Efficient Methods for Topic Model Inference on Streaming Document Collections},
  booktitle = {Proceedings of the 15th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Yao, Limin and Mimno, David and McCallum, Andrew},
  year = {2009},
  month = jun,
  series = {{{KDD}} '09},
  pages = {937--946},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1557019.1557121},
  urldate = {2024-01-08},
  abstract = {Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace. Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive. With today's large-scale, constantly expanding document collections, it is useful to be able to infer topic distributions for new documents without retraining the model. In this paper, we empirically evaluate the performance of several methods for topic inference in previously unseen documents, including methods based on Gibbs sampling, variational inference, and a new method inspired by text classification. The classification-based inference method produces results similar to iterative inference methods, but requires only a single matrix multiplication. In addition to these inference methods, we present SparseLDA, an algorithm and data structure for evaluating Gibbs sampling distributions. Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods, while also using substantially less memory.},
  isbn = {978-1-60558-495-9},
  keywords = {inference,topic modeling},
  file = {/Users/ivang/Zotero/storage/KZJA3GB6/Yao et al. - 2009 - Efficient methods for topic model inference on str.pdf}
}

@inproceedings{yin_dirichlet_2014,
  title = {A Dirichlet Multinomial Mixture Model-Based Approach for Short Text Clustering},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Yin, Jianhua and Wang, Jianyong},
  year = {2014},
  month = aug,
  series = {{{KDD}} '14},
  pages = {233--242},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2623330.2623715},
  urldate = {2024-01-10},
  abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.},
  isbn = {978-1-4503-2956-9},
  keywords = {dirichlet multinomial mixture,gibbs sampling,short text clustering}
}

@misc{zhu_survey_2023,
  title = {A {{Survey}} on {{Model Compression}} for {{Large Language Models}}},
  author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  year = {2023},
  month = sep,
  number = {arXiv:2308.07633},
  eprint = {2308.07633},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07633},
  urldate = {2024-01-06},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ivang/Zotero/storage/2LRXV6VL/Zhu et al. - 2023 - A Survey on Model Compression for Large Language M.pdf;/Users/ivang/Zotero/storage/K4TIKTZJ/2308.html}
}

@inproceedings{zuo_topic_2016,
  title = {Topic {{Modeling}} of {{Short Texts}}: {{A Pseudo-Document View}}},
  shorttitle = {Topic {{Modeling}} of {{Short Texts}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Zuo, Yuan and Wu, Junjie and Zhang, Hui and Lin, Hao and Wang, Fei and Xu, Ke and Xiong, Hui},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {2105--2114},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2939672.2939880},
  urldate = {2024-01-10},
  abstract = {Recent years have witnessed the unprecedented growth of online social media, which empower short texts as the prevalent format for information of Internet. Given the nature of sparsity, however, short text topic modeling remains a critical yet much-watched challenge in both academy and industry. Rich research efforts have been put on building different types of probabilistic topic models for short texts, among which the self aggregation methods without using auxiliary information become an emerging solution for providing informative cross-text word co-occurrences. However, models along this line are still rarely seen, and the representative one Self-Aggregation Topic Model (SATM) is prone to overfitting and computationally expensive. In light of this, in this paper, we propose a novel probabilistic model called Pseudo-document-based Topic Model (PTM) for short text topic modeling. PTM introduces the concept of pseudo document to implicitly aggregate short texts against data sparsity. By modeling the topic distributions of latent pseudo documents rather than short texts, PTM is expected to gain excellent performance in both accuracy and efficiency. A Sparsity-enhanced PTM (SPTM for short) is also proposed by applying Spike and Slab prior, with the purpose of eliminating undesired correlations between pseudo documents and latent topics. Extensive experiments on various real-world data sets with state-of-the-art baselines demonstrate the high quality of topics learned by PTM and its robustness with reduced training samples. It is also interesting to show that i) SPTM gains a clear edge over PTM when the number of pseudo documents is relatively small, and ii) the constraint that a short text belongs to only one pseudo document is critically important for the success of PTM. We finally take an in-depth semantic analysis to unveil directly the fabulous function of pseudo documents in finding cross-text word co-occurrences for topic modeling.},
  isbn = {978-1-4503-4232-2},
  keywords = {latent dirichlet allocation,pseudo document,short texts,topic modeling}
}
