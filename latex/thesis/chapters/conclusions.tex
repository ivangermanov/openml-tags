\chapter{Conclusions}\label{chapter:conclusions}

In this master thesis, we developed a novel approach to automatically generate tags for OpenML dataset descriptions using topic modeling techniques. Our research addressed four main research questions, making several key contributions to both the OpenML platform and the field of topic modeling.

Addressing \hyperref[rq1]{\textbf{RQ1}} (What are the specifications of the OpenML dataset descriptions, and how do they impact model performance?), our exploratory data analysis of 5200 OpenML dataset descriptions revealed that they are generally short, with a median length comparable to two tweets, and exhibit a wide range of readability scores, ranging from 0 to 100 on the Flesch Reading Ease scale, with many falling in the 0-60 range, indicating relatively high and varying complexity. We found that many descriptions contain domain-specific technical terms, and that a significant portion of datasets already have associated tags, with most having between 0 and 5 tags. We augmented the descriptions with metadata (dataset name, existing tags, column names up to 50), and text scraped from external URLs. We also removed approximately 300 duplicate datasets identified through high cosine similarity. Named Entity Recognition (NER) and Part-of-Speech (POS) tagging revealed challenges in tag generation due to domain-specific terms and the prevalence of author names.

For \hyperref[rq2]{\textbf{RQ2}} (What are the different approaches to topic modeling that can be applied to the OpenML dataset descriptions, and what are the advantages and disadvantages involved in their use?), we developed a modular, ten-step pipeline (\cref{sec:tag_generation_pipeline}) that extends the Base BERTopic model. We combined it with an LLM (Llama-3-70b) for prompt-based tag generation and a zeroshot classifier (MoritzLaurer/deberta-v3-large-zeroshot-v2.0) for tag filtering. This approach was chosen after evaluating various topic modeling techniques, including LDA, NMF, Top2Vec, and CTM. The rationale for selecting each submodel in the pipeline was provided in \cref{sec:rationale,sec:tag_generation_results}, highlighting the advantages over naive LLM-based methods in terms of contextual consistency, multi-level tag generation, and computational efficiency. We also explored a cost-effective configuration, omitting description rewriting, using a smaller LLM (GPT-4o-mini), and combining tag generation and classification into a single step. This alternative configuration reduced computational costs by decreasing total time from approximately 25 hours to approximately 6 hours and total cost from \$12.80 to \$0.80, but resulted in a decrease in tag quality, as reflected in the degraded c-TF-IDF representations and described clustering issues.

In response to \hyperref[rq3]{\textbf{RQ3}} (How to objectively optimize the quality of the topics and terms?), we implemented and analyzed multiple evaluation metrics, including NPMI, diversity, and silhouette score. We compared the hyperparameter-optimized Base BERTopic model against several baselines: LDA, NMF, CTM, Top2Vec, and different BERTopic configurations. Statistical analysis using Welch’s ANOVA, Kruskal-Wallis, and post-hoc tests confirmed significant differences between models for NPMI and diversity, with large effect sizes. Our optimized BERTopic model achieved the highest combined NPMI and diversity score of 0.779, outperforming all baseline models on this combined metric. Notably, NMF achieved the highest NPMI score but had very low diversity (0.399), while our optimized model had the highest diversity score (0.808). We performed hyperparameter tuning using Bayesian optimization with a custom weighted metric combining NPMI and diversity, exploring a wide range of hyperparameter values as detailed in \cref{sec:hyperparameter_tuning_results}. The optimized hyperparameters were all recorded in \cref{tab:bertopic_parameters}.

Finally, addressing \hyperref[rq4]{\textbf{RQ4}} (How to evaluate the tags from a human perspective?), we conducted a comprehensive human evaluation study with 21 participants, and a large-scale automated evaluation using GPT-4-mini. For the human evaluation, we used the Base BERTopic model as a baseline and created a human-generated set of tags for comparison. The evaluation involved two stages: individual document evaluation and document pair evaluation. In the individual document evaluation stage, participants performed intruder detection tasks and evaluated tag relevance, generality, and coverage. Our model achieved a mean relevance score of 3.63, outperforming the baseline (2.39) and approaching human-generated tags (3.95). For generality, our model showed a better balance between specific and general tags (SD = 1.40) compared to the baseline (SD = 1.33) and was close to human-generated tags (SD = 1.41). In terms of coverage, our model (mean = 4.46) outperformed the baseline (mean = 2.65) and was close to human-generated tags (mean = 4.54). The intruder detection task demonstrated our model's ability to generate cohesive tag sets, with a 95.2\% detection rate, compared to 42.1\% for the baseline and 100\% for human-generated tags. In the document pair evaluation stage, participants identified common tags between document pairs and assessed their shared coverage. Our model achieved balanced precision and recall, with accuracies of 0.87 and 0.76 for the two text pairs, which was higher than the baseline (0.69 and 0.60) and comparable to human-generated tags (0.83 and 0.86). While our model’s tags were rated (statistically) significantly lower in relevance compared to human-generated tags, there were no significant differences in generality, coverage, and shared coverage. Inter-rater agreement analysis using Fleiss’ Kappa, ICC, and Krippendorff’s Alpha showed varying levels of agreement, with some metrics indicating moderate to strong agreement, highlighting the complexity of assessing inter-rater reliability for this task.

The large-scale automated evaluation using GPT-4-mini across the entire OpenML dataset showed that the LLM detected 88.3\% of intruders, indicating the cohesiveness of the generated tags. Tag quality assessment by the LLM yielded a mean relevance score of 4.11, a mean generality score of 3.29 (SD = 0.87), and a mean coverage score of 3.72. The LLM found that keyword tags were the most relevant, and slightly more general than regular tags. The evaluation also found a weakly positive (near-neutral) correlation between relevance and generality, differing from the negative correlation observed in the human evaluation.

Our results demonstrate that the proposed model generates high-quality, relevant, and diverse tags for OpenML dataset descriptions, outperforming the baseline model and approaching human-level performance in several aspects. The modular design of our pipeline, along with the exploration of a cost-effective configuration, showcases the flexibility and adaptability of our approach to different resource constraints.

From a practical perspective, our approach provides OpenML with an automated, scalable solution for dataset tagging that produces highly relevant, diverse tags with good coverage. The modular nature of our pipeline ensures that it can evolve alongside advances in language models and embedding techniques, making it a sustainable long-term solution for the platform. The techniques we developed could be applied to similar problems in other domains where automated categorization of content is needed. Our method of combining traditional topic modeling with modern language models and zeroshot text classification represents a novel contribution to the field of topic modeling itself.