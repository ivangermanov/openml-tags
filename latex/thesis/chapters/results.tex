\chapter{Results}\label{chapter:results}

In this chapter, we present the results following the methodology described in Chapter~\ref{chapter:methodology}. We first present the results of the data exploration, preprocessing and augmentation. We then present the results of the automated evaluation, comparison with baseline models, and hyperparameter tuning. Then, we show the tag generation pipeline, including the base BERTopic model, its subcomponents and hyperparameters, the additional fine-tuning model, and zeroshot text classifier. Finally, we present the results of the human evaluation, followed by the results of the large-scale automated evaluation.

\section{Data exploration, preprocessing and augmentation}
\label{sec:data_exploration}
Following an initial exploratory data analysis, we identified several methods for augmenting the dataset descriptions. The dataset descriptions are augmented with the following additional information:

\begin{itemize}
    \item \textbf{Name} — The name of the dataset.
    \item \textbf{Tags} — The generated tags that have already been created for the dataset.
    \item \textbf{Features} — The names of the dataset's features (columns).
    \item \textbf{Scraped text} — For some datasets, we scrape text from the original sources and append it to the dataset description.
\end{itemize}

Additionally, we remove all datasets that have a cosine similarity of 0.99 or higher with another dataset, as these datasets are likely to be duplicates or different versions of the same dataset. This results in the removal of $\sim$300 datasets from the original $\sim$5500 datasets.

After augmenting the dataset descriptions, we observe that the augmented descriptions are longer, and potentially more informative, as illustrated in \cref{fig:description_vs_augmented_description}. All subsequent analyses are performed on the augmented dataset descriptions.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.65\textwidth]{figures/description_vs_augmented_description.pdf}
%     \caption{Histogram of the length of dataset descriptions vs augmented dataset descriptions}
%     \label{fig:description_vs_augmented_description}
% \end{figure}

\cref{fig:words_of_descriptions} presents a histogram depicting the number of words in dataset descriptions. The lengths range from 0 to 1500 words, with the majority of descriptions being under 500 words. From the histogram, we can infer that the dataset descriptions are relatively short, with the majority being comparable in length to two Twitter tweets. This is important to note, as descriptions that are too short may lack enough information to generate meaningful tags. However, this may not necessarily pose an issue, as prior studies have successfully applied topic modeling to tweets and other short texts \cite{cataldi_emerging_2010, churchill_percolation-based_2020, curiskis_evaluation_2020, kasiviswanathan_emerging_2011, paul_discovering_2014, yin_dirichlet_2014}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/words_of_descriptions.pdf}
%     \caption{Histogram of the number of words of dataset descriptions}
%     \label{fig:words_of_descriptions}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Histogram of the length of dataset descriptions vs augmented dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/description_vs_augmented_description.pdf}\label{fig:description_vs_augmented_description}}
    \hfill
    \subfloat[Histogram of the number of words of dataset descriptions]{\includegraphics[width=0.49\textwidth]{figures/words_of_descriptions.pdf}\label{fig:words_of_descriptions}}
    \caption{Analysis of dataset descriptions}
    \label{fig:dataset_description_histograms}
\end{figure}

When we examine the availability of tags, we find that the majority of datasets already have tags associated with them (\cref{fig:tag_availability}). This suggests that the tag generation model may be able to leverage the existing tags to improve its performance.

When we analyze the number of tags associated with each dataset, we find that most datasets have between 0 and 5 tags, with a few datasets having more than 5 tags (\cref{fig:number_of_tags}).

\begin{figure}[h]
    \centering
    \subfloat[Histogram of the number of tags associated with datasets]{\includegraphics[width=0.5\textwidth]{figures/number_of_tags.pdf}\label{fig:number_of_tags}}
    \hfill
    \subfloat[Histogram of the number of datasets with and without tags]{\includegraphics[width=0.5\textwidth]{figures/tag_availability.pdf}\label{fig:tag_availability}}
    \caption{Analysis of tag distribution and availability in datasets}
    \label{fig:tag_analysis}
\end{figure}

We then look at the number of features (columns) in the datasets. The distribution of the number of features is shown in \cref{fig:number_of_features}. Most datasets have fewer than 100 features, with a large number of datasets having more than 100 features. This is because those datasets are likely to be high-dimensional datasets from domains such as genomics, text processing, or image analysis, where numerous variables or measurements are collected for each sample. Four tag generation pipeline, we use up to 50 features per dataset, as including all features may lead to a high-dimensional space that is difficult to model.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/number_of_features.pdf}
%     \caption{Histogram of the number of features in datasets}
%     \label{fig:number_of_features}
% \end{figure}

Additionally, we examine the cosine similarity between the augmented dataset descriptions to see whether there are datasets with similar descriptions and duplicates. The heatmap of the cosine similarity is shown in \cref{fig:cosine_similarity}. The majority of dataset descriptions have a low cosine similarity, indicating that they are distinct from one another. However, there are some datasets with high cosine similarity, suggesting that they may be duplicates or different versions of the same dataset.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/cosine_similarity.png}
%     \caption{Heatmap of the cosine similarity between dataset descriptions}
%     \label{fig:cosine_similarity}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Histogram of the number of features in datasets]{\includegraphics[width=0.5\textwidth]{figures/number_of_features.pdf}\label{fig:number_of_features}}
    \hfill
    \subfloat[Heatmap of the cosine similarity between dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/cosine_similarity.png}\label{fig:cosine_similarity}}
    \caption{}
    \label{fig:dataset_features_and_similarity}
\end{figure}


We find that OpenML datasets can have multiple versions. Our analysis of dataset versions reveals that most datasets have only 2 versions, though several datasets have more than 2 (\cref{fig:number_of_versions}).

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/number_of_versions.pdf}
%     \caption{Histogram of the number of versions of dataset descriptions}
%     \label{fig:number_of_versions}
% \end{figure}

We also examine the similarity between different versions of the datasets. Our analysis shows that most datasets have a cosine similarity of 0.9 or higher within versions, suggesting that the versions are highly similar to one another (\cref{fig:cosine_similarity_dataset_versions}).

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/cosine_similarity_dataset_versions.pdf}
%     \caption{Histogram of the cosine similarity of different versions of dataset descriptions}
%     \label{fig:cosine_similarity_dataset_versions}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Histogram of the cosine similarity of different versions of dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/cosine_similarity_dataset_versions.pdf}\label{fig:cosine_similarity_dataset_versions}}
    \hfill
    \subfloat[Histogram of the number of versions of dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/number_of_versions.pdf}\label{fig:number_of_versions}}
    \caption{Analysis of dataset versions: similarity and distribution}
    \label{fig:dataset_version_analysis}
\end{figure}


We apply Named Entity Recognition (NER) and Part-of-Speech (POS) tagging to the dataset descriptions to identify the most common entities and parts of speech (\cref{fig:pos,fig:ner}). A significant number of words are tagged as \textit{X}, indicating that the POS tagger is unable to determine the part of speech for these words. This is likely due to the presence of domain-specific terms not included in the POS tagger's vocabulary, as well as unrecognized or anomalous tokens and symbols. Additionally, we find that one of the most frequent named entities are \textit{PERSON} and \textit{ORG}, which can be attributed to the fact that many dataset descriptions reference the names of dataset authors or associated papers and their organizations. Both of these findings pose challenges for the performance of the tag generation model, as unrecognized tokens and author names do not contribute meaningful information for generating relevant tags.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/pos.pdf}
%     \caption{Bar chart of the most common parts of speech in dataset descriptions}
%     \label{fig:pos}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/ner.pdf}
%     \caption{Bar chart of the most common named entities in dataset descriptions}
%     \label{fig:ner}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Bar chart of the most common parts of speech in dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/pos_descriptions.pdf}\label{fig:pos}}
    \hfill
    \subfloat[Bar chart of the most common named entities in dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/ner_descriptions.pdf}\label{fig:ner}}
    \caption{Analysis of linguistic features in dataset descriptions}
    \label{fig:linguistic_features}
\end{figure}

We also examine how many original datasets include URLs to original sources in their descriptions (\cref{fig:url_availability}). A significant number of datasets contain URLs, which could be used to scrape additional text for augmenting the descriptions. However, upon closer inspection, we find that the information in these URLs is often redundant with what is already provided in the dataset descriptions. Nonetheless, for some datasets, the URLs contain additional text, which is not in the original description, that could add additional information useful for generating tags. We scrape these texts and we augment the dataset descriptions with them.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/url_availability.pdf}
%     \caption{Bar chart of the number of datasets with and without URLs to original sources}
%     \label{fig:url_availability}
% \end{figure}

We also explore the readability and complexity of the dataset descriptions by calculating the Flesch Reading Ease \cite{flesch_new_1948} (\cref{fig:flesch_reading_ease}). The Flesch Reading Ease score ranges from 0 to 100, with higher scores indicating easier readability. We find that the dataset descriptions are somewhat difficult to read, with many falling in the 0-60 range. This suggests that the descriptions may contain complex language and jargon that could require specialized knowledge to understand. This may also affect the embedding model performance, as it may struggle to capture the semantic meaning of complex or domain-specific terms which the embedding model has not been trained on.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/flesch_reading_ease.pdf}
%     \caption{Histogram of the Flesch Reading Ease scores for dataset descriptions}
%     \label{fig:flesch_reading_ease}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Bar chart of the number of datasets with and without URLs to original sources]{\includegraphics[width=0.5\textwidth]{figures/url_availability.pdf}\label{fig:url_availability}}
    \hfill
    \subfloat[Histogram of the Flesch Reading Ease scores for dataset descriptions]{\includegraphics[width=0.5\textwidth]{figures/flesch_reading_ease.pdf}\label{fig:flesch_reading_ease}}
    \caption{}
    \label{fig:url_and_readability_analysis}
\end{figure}

In this section, we described the most pertinent findings from our data exploration. For readers interested in a more detailed analysis, additional information can be found in the \textit{eda.ipynb} notebook in the \href{https://github.com/ivangermanov/openml-tags}{GitHub repository} \cite{germanov_topic_modeling_of_2024}.

\section{Automated evaluation metrics and baselines}
\label{sec:automated_evaluation_results}
Following \cref{sec:automated_evaluation_methodology}, we first explain the hyperparameter tuning (Bayesian optimization) process. Then, we explain the baseline models we use and what parameters we choose for them. We then showcase the results of the comparison between the baseline models and the hyperparameter-optimized BERTopic model.

\subsection{Hyperparameter tuning}
\label{sec:hyperparameter_tuning_results}
Using OCTIS, we perform hyperparameter tuning for the \textit{Base BERTopic model}. As an objective metric, we use the weighted metric we defined in \cref{sec:hyperparameter_tuning}. We set ranges for the hyperparameters, which are based on prior knowledge of good default values and the OpenML dataset characteristics. The hyperparameters and their respective search spaces are as follows:

\begin{itemize}
    \item \textbf{min\_topic\_size}: This determines the minimum number of documents a topic should have. We set the range to [2, 3] to explore small topic sizes.
    \item \textbf{ctfidf\_reduce\_frequent\_words}: This boolean hyperparameter determines whether frequent words should be reduced when constructing the c-TF-IDF matrix. The values considered are \texttt{True} or \texttt{False}.
    \item \textbf{umap\_n\_neighbors}: This hyperparameter controls the number of neighbors considered in the UMAP algorithm. We set the range to [2, 3]. We choose smaller values, since larger values result in more global views of the manifold, while smaller values result in more local data being preserved.
    \item \textbf{umap\_n\_components}: This controls the number of dimensions UMAP reduces the embeddings to. We have set the range to [2, 10].
    \item \textbf{umap\_min\_dist}: This defines the minimum distance between points in the UMAP embedding space. We are exploring a small range of [0.0, 0.01].
    \item \textbf{umap\_metric}: The metric used to calculate distances between points in the UMAP algorithm. The two metrics we consider are 'cosine' and 'euclidean'.
    \item \textbf{hdbscan\_min\_cluster\_size}: This defines the minimum cluster size for HDBSCAN. We set the range to [2, 3]. Similarly to \texttt{min\_topic\_size} and \texttt{umap\_n\_neighbors}, we explore small values to capture smaller clusters.
    \item \textbf{hdbscan\_metric}: This defines the distance metric used by HDBSCAN. We restrict this to 'euclidean'.
    \item \textbf{hdbscan\_cluster\_selection\_method}: This determines how clusters are selected in HDBSCAN. We use \textit{eom} (excess of mass).
    \item \textbf{vectorizer\_ngram\_range}: This defines the n-gram range for the vectorizer. We consider both unigram (1,1) and bigram (1,2) settings.
    \item \textbf{vectorizer\_stop\_words}: We set the stop words for vectorization to be 'english'.
    \item \textbf{vectorizer\_tokenizer}: This boolean hyperparameter controls whether a custom tokenizer is used. We set this to \texttt{False}.
    \item \textbf{outliers\_strategy}: This defines how outliers should be handled. For this study, we set it to \textit{none}, meaning that no specific outlier handling strategy is applied.
    \item \textbf{embedding\_model}: This hyperparameter defines the embedding model used for the \textit{Base BERTopic model}. Since they are slower to evaluate, we only consider the \texttt{Salesforce/\allowbreak SFR-Embedding-2\allowbreak\_R} \cite{noauthor_salesforcesfr-embedding-2_r_2024} model, as it is one of the best performing models on the MTEB leaderboard \cite{muennighoff_mteb_2023} at the time of writing.
    \item \textbf{representation\_model}: This hyperparameter defines the representation model used for the BERTopic model. Since they are slower to evaluate, we only consider spaCy's Part-of-Speech model called \texttt{en\_core\_web\_lg}.
\end{itemize}

This automated hyperparameter optimization process allows us to systematically explore the space of possible configurations and identify the best-performing model for our dataset.

As for the Bayesian optimization \cite{archetti_bayesian_2019, galuzzi_hyperparameter_2020, snoek_practical_2012}, we employ the \texttt{Optimizer} class from OCTIS. This method efficiently explores the hyperparameter space by constructing a probabilistic (surrogate) model to approximate the objective function. In our case, the surrogate model is a Random Forest (RF), which is updated iteratively to predict the performance of unobserved hyperparameter configurations based on previous evaluations.

The surrogate model relies on the Matern kernel with a smoothness parameter \( \nu = 1.5 \). This kernel helps balance the trade-off between exploration and exploitation by controlling the smoothness of the Gaussian process used in the optimization.

To guide the search for optimal hyperparameters, we use the Lower Confidence Bound (LCB) acquisition function. The LCB acquisition function is particularly effective in encouraging exploration of uncertain regions while still exploiting areas that show high potential for improvement.

Before the optimization begins, we initialize the surrogate model with a diverse set of hyperparameter configurations. These initial points are generated using Latin Hypercube Sampling (LHS), which ensures a broad coverage of the search space from the outset.

Additionally, we set the number of iterations for the optimization process to 125, and the number of model runs per iteration to 10. This configuration allows us to explore the hyperparameter space thoroughly while maintaining a reasonable computational cost. We set the model runs to 10 to account for the stochastic nature of some of the models, e.g. BERTopic, which may yield different results each time it is run primarily due to UMAP. Furthermore, having 10 runs per iteration allows us to identify statistically significant differences between the models.

\cref{fig:bayesian_optimization} shows the results of the Bayesian optimization process. The x-axis represents the number of iterations, while the y-axis represents the weighted metric score, \textit{Median(model\_runs)} and \textit{Mean(model\_runs)}, and other metrics. 

We can see that the optimization process converges to a stable solution relatively quickly (looking at \textit{Mean(model\_runs)}). This indicates that the hyperparameter search space has been thoroughly explored, and a good configuration has been found. It is also worth noting that the starting state of the optimization process is relatively close to the final state, suggesting that the initial default hyperparameter configurations are already performing well, meaning that the optimization process does not need to make large adjustments to find a good solution. This indicates that the BERTopic model is robust to changes in hyperparameters and can achieve good performance across a wide range of settings on the OpenML dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/bayesian_optimization.pdf}
    \caption{Results of the Bayesian optimization process}
    \label{fig:bayesian_optimization}
\end{figure}

\cref{tab:bertopic_parameters} shows the hyperparameters we explored and the optimal values found by the Bayesian optimization process.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
    \toprule
    Parameter & Search Space & Optimal Value \\
    \midrule
    min\_topic\_size & [2, 3] & 2 \\
    ctfidf\_reduce\_frequent\_words & \{True, False\} & True \\
    umap\_n\_neighbors & [2, 3] & 3 \\
    umap\_n\_components & [2, 10] & 5 \\
    umap\_min\_dist & [0.0, 0.01] & 0.008 \\
    umap\_metric & \{cosine, euclidean\} & euclidean \\
    hdbscan\_min\_cluster\_size & [2, 3] & 3 \\
    hdbscan\_metric & euclidean & euclidean \\
    hdbscan\_cluster\_selection\_method & eom & eom \\
    vectorizer\_ngram\_range & \{(1,1), (1,2)\} & (1, 1) \\
    vectorizer\_stop\_words & english & english \\
    vectorizer\_tokenizer & False & False \\
    outliers\_strategy & none & none \\
    \bottomrule
    \end{tabular}
    \caption{BERTopic Hyperparameter Settings and Optimal Values}
    \label{tab:bertopic_parameters}
\end{table}

\subsection{Baselines}
\subsubsection{Definition of Baselines}
We select the hyperparameter-optimized BERTopic model as our primary model and compare its performance against several baseline models. The baseline models are as follows:
\begin{enumerate}
    \item \textbf{BERTopic model with default parameters}: This model uses the default hyperparameters provided by the BERTopic library. We use this model to compare the performance of the hyperparameter-optimized model with the default settings.
    \item \textbf{LDA}: We fit an LDA model to the dataset descriptions using the \texttt{LDA} class from OCTIS. LDA is a model which requires cleaning and preprocessing of the text data, such as removing stop words, stemming, and lemmatization. We use the default settings for the LDA model.
    \item \textbf{NMF}: We fit an NMF model to the dataset descriptions using the \texttt{NMF} class from OCTIS. We use the default settings for the NMF model.
    \item \textbf{CTM}: We fit a CTM model to the dataset descriptions using the \texttt{CTM} class from the Contextualized Topic Models library \cite{noauthor_milanlproccontextualized-topic-models_2024}. We preprocess the data and create the required embeddings with the \texttt{all-mpnet-base-v2} embedding model and use the default settings for the CTM model with the \texttt{contextual\_size} parameter set to 768.
    \item \textbf{Top2Vec}: We fit a custom implementation of Top2Vec based on the original model. This implementation uses the \texttt{all-mpnet-base-v2} embedding model from the Sentence Transformers library for document and word embeddings. The HDBSCAN clustering algorithm is used with custom arguments set in the \texttt{hdbscan\_args} parameter. We use the \texttt{Top2VecNew} class from our custom implementation, which allows for more flexibility in topic number specification.
\end{enumerate}

For each model, we train using a predefined set of topic numbers. This approach is necessary because some models require a fixed number of topics, while others, such as BERTopic, can either operate with a fixed number or determine the number of topics automatically. Specifically, we set the number of topics to 10, 20, 30, 40, 50, 100, and 200.

Additionally, for each specified number of topics, we run each model 10 times to account for their stochastic nature. This approach allows us to later assess whether there are statistically significant differences in the models' performance.

\subsubsection{Results}
In \cref{fig:openml_npmi} we present the NPMI scores for the hyperparameter-optimized BERTopic model and the baseline models. We remind the reader that we use the Base BERTopic model, not the proposed model, for this comparison. There are several BERTopic models with different hyperparameters:
\begin{itemize}
    \item \textbf{BERTopic\_optimized\_POS\_reduced\_range}: The BERTopic model we optimized with Bayesian optimization above.
    \item \textbf{BERTopic\_optimized\_POS\_full\_range}: Similar to the previous model, also optimized with Bayesian optimization, but with a wider range of hyperparameters. We choose not to use this model as we are interested in exploring smaller topic sizes, but we provide it for comparison in the case where we do not restrict the hyperparameter search space.
    \item \textbf{BERTopic\_POS}: The BERTopic model with default hyperparameters (also with the spaCy Part-of-Speech model as the representation model). This is the default model provided by the BERTopic library, without any hyperparameter optimization. We include this model to compare the performance of the hyperparameter-optimized model with the default settings.
    \item \textbf{BERTopic\_POS\_mpnet}: Similar to the model above, but with the \texttt{all-mpnet-base-v2} embedding model instead of \texttt{Salesforce/\allowbreak SFR-Embedding-2\allowbreak\_R} \cite{noauthor_salesforcesfr-embedding-2_r_2024}. We choose this model to explore the impact of different embedding models on the BERTopic model, since \texttt{all\allowbreak-mpnet\allowbreak-base\allowbreak-v2} is a relatively small model compared to \texttt{Salesforce/\allowbreak SFR-Embedding-2\allowbreak\_R} \cite{noauthor_salesforcesfr-embedding-2_r_2024}, which is one of the best performing models on the MTEB leaderboard \cite{muennighoff_mteb_2023}.
\end{itemize}

We choose these four variations of the BERTopic model to explore the impact of different hyperparameters on the model's performance. 

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{figures/openml_npmi.pdf}
%     \caption{Line chart of the NPMI scores for the hyperparameter-optimized BERTopic model and the baseline models}
%     \label{fig:openml_npmi}
% \end{figure}

We observe that the NMF model outperforms all models across all topic numbers. However, when we look at \cref{fig:openml_diversity}, we see that the NMF model has very low diversity scores. This suggests that the NMF model may be picking the same terms in each topic, leading to high coherence but low diversity, as explained in the coherence-diversity trade-off in \cref{sec:chapter_conclusion_preliminaries}. For NPMI, we then see that the BERTopic models perform the second best, with small differences between them. We see that \textbf{BERTopic\_optimized\_POS\_reduced\_range} performs slightly worse than the other BERTopic models, but this is likely due to the smaller topic sizes we explored, since this leads to a higher number of smaller clusters, leading to slightly lower coherence, but higher diversity. We see that LDA, CTM, and Top2Vec perform worse than the BERTopic models.

As for diversity, we see that the \textbf{BERTopic\_optimized\_POS\_reduced\_range} has the highest average diversity scores, followed by the other BERTopic models. This suggests that the BERTopic models are able to capture a wider range of terms in their topics compared to the other models. Surprisingly, the Top2Vec model has the lowest NPMI and diversity scores.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figures/openml_diversity.pdf}
%     \caption{Line chart of the diversity scores for the hyperparameter-optimized BERTopic model and the baseline models}
%     \label{fig:openml_diversity}
% \end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Line chart of the diversity scores for the hyperparameter-optimized BERTopic model and the baseline models]{\includegraphics[width=0.49\textwidth]{figures/openml_diversity.pdf}\label{fig:openml_diversity}}
    \hfill
    \subfloat[Line chart of the NPMI scores for the hyperparameter-optimized BERTopic model and the baseline models]{\includegraphics[width=0.5\textwidth]{figures/openml_npmi.pdf}\label{fig:openml_npmi}}
    \caption{Comparison of diversity and NPMI Scores for different topic models}
    \label{fig:diversity_and_npmi_comparison}
\end{figure}

In \cref{tab:openml_results}, we present the averaged NPMI and diversity scores for the models. We highlight the best scores in dark green and the subsequent best scores in lighter green. We see that the BERTopic models perform well in terms of diversity, with the \textbf{\allowbreak BERTopic\_\allowbreak optimized\_\allowbreak POS\_\allowbreak reduced\_range} model having the highest diversity score. If we only look at the BERTopic models, we can note a strong negative Pearson correlation between NPMI and diversity scores of -0.67. Surpisingly, \textbf{BERTopic\_\allowbreak POS\_\allowbreak mpnet} has the highest NPMI score (after NMF), even though it uses a smaller embedding model. However, we can see it has a lower diversity score compared to the other BERTopic models. This suggests that the higher coherence may be driven by the inclusion of more common terms in the topics, while the diversity score is driven by the inclusion of more unique terms, as explained in \cref{sec:chapter_conclusion_preliminaries}. In any case, the BERTopic models outperform the baseline models in terms of combined NPMI and diversity scores. 

Looking at the combined scores (NPMI + diversity), the top three performing models are \textbf{BERTopic\_\allowbreak optimized\_\allowbreak POS\_\allowbreak full\_\allowbreak range} with a score of 0.812, followed by \textbf{BERTopic\_POS} with 0.783, and then \textbf{BERTopic\_\allowbreak optimized\_\allowbreak POS\_\allowbreak reduced\_\allowbreak range} with 0.779. This is expected, as for the first model, we explore a wider range of hyperparameters, which allows for more flexibility in the model's performance. For the second model, we use the default hyperparameters, which are already performing well. For the third model, we explore smaller topic sizes, which leads to a higher number of smaller clusters, leading to slightly lower coherence, but higher diversity.

\begin{table}[h]
    \centering
    \definecolor{color64db00}{HTML}{64db00}
    \definecolor{color76FF03}{HTML}{76FF03}
    \definecolor{colore1ffc7}{HTML}{e1ffc7}
    \begin{tabular}{>{\centering\arraybackslash}m{25em}>{\centering\arraybackslash}m{7em}>{\centering\arraybackslash}m{7em}}
        \toprule
        \textbf{Model}                           & \textbf{npmi}                 & \textbf{diversity}            \\
        \midrule
        Top2Vec                                  & -0.202                        & 0.364                         \\
        BERTopic\_optimized\_POS\_reduced\_range & -0.029                        & \cellcolor{color64db00} 0.808 \\
        LDA                                      & -0.017                        & 0.411                         \\
        CTM\_CUSTOM                              & 0.011                         & 0.678                         \\
        BERTopic\_POS                            & 0.013                         & \cellcolor{colore1ffc7} 0.770 \\
        BERTopic\_optimized\_POS\_full\_range    & \cellcolor{colore1ffc7} 0.014 & \cellcolor{color76FF03} 0.798 \\
        BERTopic\_POS\_mpnet                     & \cellcolor{color76FF03} 0.019 & 0.727                         \\
        NMF                                      & \cellcolor{color64db00} 0.082 & 0.399                         \\
        \bottomrule
    \end{tabular}
    \caption{Average NPMI and diversity scores for the hyperparameter-optimized BERTopic model and the baseline models}
    \label{tab:openml_results}
\end{table}

\subsubsection{Statistical significance}
As mentioned earlier, we had 10 runs for each model and topic number combination. We will now assess whether there are statistically significant differences between the performance of the models. We check whether the assumptions for ANOVA are met, and if not, we use Welch's ANOVA or Kruskal-Wallis (non-parametric) tests.

The first assumption for ANOVA is that the residuals are normally distributed. We apply the Shapiro-Wilk test to check whether the residuals for the NPMI values (the differences between the observed and predicted NPMI values) are normally distributed. The Shapiro-Wilk test returns a statistic of 0.905 and a p-value of 3.30e-18, indicating a significant deviation from normality. Although the statistic is relatively close to 1, suggesting that the residuals are not drastically non-normal, the extremely small p-value suggests that the deviation is statistically significant. As for the diversity values, the Shapiro-Wilk test returns a statistic of 0.968 and a p-value of 8.70e-10, also indicating a significant deviation from normality, but with a statistic even closer to 1.

Given this result, we further inspect the residuals using visual diagnostics with Q-Q plots and histograms to assess the nature and extent of the deviation from normality. If the deviation is minor, ANOVA may still be appropriate. \cref{fig:qqplot_npmi} and \cref{fig:histogram_npmi} show the Q-Q plot and histogram of the residuals for the NPMI values, respectively. We observe that the residuals are approximately normally distributed, with some deviations at the tails. \cref{fig:qqplot_diversity} and \cref{fig:histogram_diversity} show the Q-Q plot and histogram of the residuals for the diversity values, respectively. We observe a similar pattern for the diversity residuals, with some deviations at the tails. Given that ANOVA is relatively robust to deviations at the tails, since extreme values do not have a large impact on the F-statistic, we consider this assumption to be met.

\begin{figure}[h]
    \centering
    \subfloat[Q-Q plot of the residuals for the NPMI values]{\includegraphics[width=0.5\textwidth]{figures/qqplot_npmi.pdf}\label{fig:qqplot_npmi}}
    \hfill
    \subfloat[Histogram of the residuals for the NPMI values]{\includegraphics[width=0.5\textwidth]{figures/histogram_npmi.pdf}\label{fig:histogram_npmi}}
    \caption{Analysis of residuals for NPMI values}
    \label{fig:npmi_residuals_analysis}
\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Q-Q plot of the residuals for the diversity values]{\includegraphics[width=0.5\textwidth]{figures/qqplot_diversity.pdf}\label{fig:qqplot_diversity}}
    \hfill
    \subfloat[Histogram of the residuals for the diversity values]{\includegraphics[width=0.5\textwidth]{figures/histogram_diversity.pdf}\label{fig:histogram_diversity}}
    \caption{Analysis of Residuals for Diversity Values}
    \label{fig:diversity_residuals_analysis}
\end{figure}

The second assumption for ANOVA is that the residuals have equal variance. We apply the Levene test and the Bartlett test to check whether the residuals have equal variance, for each model and topic number combination. \cref{tab:levene_bartlett} shows the results of the Levene and Bartlett tests for the NPMI and diversity values. We observe that the p-values are mostly below 0.05, indicating that the residuals do not have equal variance. This violates the assumption of equal variance for ANOVA. Therefore, we will use Welch's ANOVA, since the first assumption is met, and the residuals are approximately normally distributed, but the assumption of equal variance is violated.

\begin{table}[htbp]
    \centering
    \caption{Levene's and Bartlett's tests for NPMI and diversity}
    \begin{tabular}{@{}lcc|cc@{}}
        \toprule
                            & \multicolumn{2}{c}{\textbf{NPMI}} & \multicolumn{2}{c}{\textbf{Diversity}}                                         \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        \textbf{nr\_topics} & \textbf{Statistic}                & \textbf{p-value}                       & \textbf{Statistic} & \textbf{p-value} \\ \midrule
        \multicolumn{5}{c}{\textbf{Levene's Test}}                                                                                               \\ \midrule
        10                  & 1.4665                            & 0.1930                                 & 1.6530             & 0.1346           \\
        20                  & 4.0018                            & 0.0009                                 & 1.2666             & 0.2791           \\
        30                  & 2.9904                            & 0.0082                                 & 2.0219             & 0.0638           \\
        40                  & 2.6662                            & 0.0164                                 & 1.1821             & 0.3239           \\
        50                  & 2.9896                            & 0.0082                                 & 2.5184             & 0.0225           \\
        100                 & 1.0988                            & 0.3733                                 & 1.4504             & 0.1990           \\
        200                 & 2.4367                            & 0.0268                                 & 2.6070             & 0.0186           \\ \midrule
        \multicolumn{5}{c}{\textbf{Bartlett's Test}}                                                                                             \\ \midrule
        10                  & 19.3408                           & 0.0072                                 & 17.4000            & 0.0150           \\
        20                  & 39.6442                           & 1.4724e-06                             & 7.5180             & 0.3770           \\
        30                  & 24.0258                           & 0.0011                                 & 14.8213            & 0.0384           \\
        40                  & 27.6841                           & 0.0003                                 & 11.7576            & 0.1088           \\
        50                  & 34.6455                           & 1.3038e-05                             & 19.0298            & 0.0081           \\
        100                 & 9.8099                            & 0.1996                                 & 15.5252            & 0.0298           \\
        200                 & 20.3993                           & 0.0048                                 & 31.0744            & 6.0240e-05       \\ \bottomrule
    \end{tabular}
    \label{tab:levene_bartlett}
\end{table}

Applying Welch's ANOVA to the NPMI and diversity values, we find that there are statistically significant differences between the models for both metrics (\cref{tab:welch_anova}). For NPMI, 90.75\% of the variance is explained by the model, and for diversity, 81.50\% of the variance is explained by the model. These are both large effect sizes, suggesting that the models have a substantial impact on both metrics.

We then perform post-hoc tests using the Games-Howell method to determine which models are significantly different from one another, as this test is appropriate when the assumption of equal variance is violated. The results of the Games-Howell post-hoc test for the NPMI values are presented in \cref{tab:games_howell_npmi}. We observe a notable homogeneity among the BERTopic models, with generally high p-values indicating a lack of statistically significant difference in their coherence scores. The exception is \texttt{B\_OPT\_REDUCED}, which was configured to explore smaller topic sizes, showing statistically significant differences from other BERTopic models, suggesting that restricting the topic size in this way might negatively impact topic coherence. While NMF achieved the highest average NPMI score, the post-hoc test shows a statistically significant difference between NMF and all BERTopic models. The relatively high NPMI of \texttt{BERTopic\_POS\_mpnet} is worth noting, as this variant utilizes a smaller embedding model compared to others, suggesting that the choice of the embedding model might not be the primary determinant of topic coherence. The post-hoc analysis of diversity scores (\cref{tab:games_howell_diversity}) further underscores the strength of the BERTopic approach, with all BERTopic models demonstrating significantly higher diversity compared to the baseline models. The \texttt{B\_OPT\_FULL} and \texttt{B\_OPT\_REDUCED} models exhibit the highest diversity scores. These results highlight a trade-off between coherence and diversity, suggesting that models excelling in one metric may not perform as well in the other.

\begin{table}[ht]
    \centering
    \caption{Welch's ANOVA results for NPMI and diversity}
    \label{tab:welch_anova}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Metric}    & \textbf{Source} & \textbf{df1} & \textbf{df2} & \textbf{F-value} & \textbf{p-value}            & \textbf{np2} \\
        \midrule
        \textbf{NPMI}      & Model           & 7            & 231.601      & 2549.36          & $3.165650 \times 10^{-215}$ & 0.90749      \\
        \textbf{Diversity} & Model           & 7            & 232.705      & 1091.24          & $4.954462 \times 10^{-174}$ & 0.815016     \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Tag generation}
\label{sec:tag_generation_results}
In \cref{sec:tag_generation_pipeline} and \cref{fig:tag_generation_pipeline}, we presented the tag generation pipeline on a high level. \cref{fig:tag_generation_pipeline_specifics}, which is similar to \cref{fig:tag_generation_pipeline} shows the specifics of the tag generation pipeline. In particular, we show which specific submodels we used for the different steps in the pipeline:
\begin{enumerate}
    \item \textbf{Original Descriptions}: Same as in the high-level pipeline, we start with the original OpenML dataset descriptions.
    \item \textbf{Augmented Descriptions}: In \cref{sec:data_exploration}, we discussed how we augment the descriptions with additional information.
    \item \textbf{Prompt Descriptions Human-Readable}: For this step, we used the \texttt{Llama-3-70b}, model, as it was a model offering a good balance between performance and computational resources at the time of the experiment. We engineered a prompt to extract keyword tags from each individual description. We do not include the prompt here for brevity, but it is available in the \href{https://github.com/ivangermanov/openml-tags}{GitHub repository} \cite{germanov_topic_modeling_of_2024}.
    \item \textbf{Create Embeddings}: We use the \texttt{Salesforce/SFR-Embedding-2\_R} model \cite{noauthor_salesforcesfr-embedding-2_r_2024}, which is one of the best performing models on the MTEB benchmark \cite{muennighoff_mteb_2023}.
    \item \textbf{Base BERTopic Model}: For dimensionality reduction, clustering, bag-of-words construction and c-TF-IDF calculation, we use the hyperparameter-optimized BERTopic model.
    \item \textbf{Fine-tune to Extract Tags}: For the fine-tuning step, we use the \texttt{Llama-3-70b} model. We prompt the model to generate tags for each cluster. The prompt can again be found in the repository.
    \item \textbf{Zeroshot Text Classifier}: We use the \texttt{MoritzLaurer/deberta-v3-large-zeroshot-v2.0} model \cite{noauthor_moritzlaurerdeberta-v3-large-zeroshot-v20_2024}, which at the time of the experiment was the best performing model for the zeroshot text classification task.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/tag_generation_pipeline_specifics.pdf}
    \caption{Tag generation pipeline specifics (\textit{proposed model})}
    \label{fig:tag_generation_pipeline_specifics}
\end{figure}

\subsection{Hardware}
We run the tag generation pipeline, with the exception of the \textit{Prompt Descriptions Human-Readable} step and the \textit{Fine-tune to Extract Tags} step, on an Apple M2 Max MacBook Pro with 64 GB of internal memory. We run the \textit{Prompt Descriptions Human-Readable} step and the \textit{Fine-tune to Extract Tags} step on \href{https://docs.together.ai/docs/introduction}{Together AI}, a platform that offers a cloud-based environment for running large language models.

The LLM-based rewriting of all dataset descriptions into a more human-readable format took approximately 3 hours and cost around \$8.00. Generating the embeddings using the \textit{Salesforce/SFR-Embedding-2\_R} \cite{noauthor_salesforcesfr-embedding-2_r_2024} model took around 4 hours on the Apple M2 Max MacBook Pro. Training the base BERTopic model required only about 5 minutes. The LLM fine-tuning step using the Llama-3-70b model took roughly 2 hours and cost approximately \$4.80. These reduced costs in the fine-tuning step compared to the rewriting step are likely due to the smaller size of the rewritten descriptions and the implementation of prompt truncation for excessively large inputs. The Llama 3 70B model provided by TogetherAI, \texttt{Meta Llama 3 70B Instruct}, was priced at \$0.88 per 1M tokens at the time of this writing.

By far the most time-consuming step in the pipeline was the zeroshot classification. Despite using a relatively small model (MoritzLaurer/deberta-v3-large-zeroshot-v2.0 with a maximum context length of 8192 tokens \cite{noauthor_moritzlaurerdeberta-v3-large-zeroshot-v20_2024}), this step took approximately 16 hours to complete. This is because each potential tag from the top \textit{n} topic clusters for each dataset description is individually fed to the zeroshot classifier, generating a confidence score for each tag-description pair. This process involves a large number of individual classifications, leading to a substantial time requirement.



\subsection{Results}
We now show the results of the output of the tag generation pipeline. Approximately 6300 regular tags and 300 overarching tags were generated. It is expected that the number of regular tags is higher than the number of overarching tags, as the regular tags are more specific, while the overarching tags are more general and cover a wider range of topics.

Investigating the generated tag frequency in detail, we see that the distribution of the tags is skewed. \cref{fig:histogram_tag_counts_log} shows the histogram of tag counts on a log scale. We see that the majority of tags have a low count, with a few tags having a very high count. This is expected, as natural language has been found to follow Zipf's law (Zipfian distribution), where a few terms are very common, while the majority of terms are rare \cite{zipf_psycho-biology_1935, piantadosi_zipfs_2014}.

We observe a similar pattern when looking at histograms of the overarching tag counts, as seen in \cref{fig:histogram_overarching_tag_counts_log}.

\begin{figure}[h]
    \centering
    \subfloat[Tag counts]{\includegraphics[width=0.50\textwidth]{figures/histogram_tag_counts_log.pdf}\label{fig:histogram_tag_counts_log}}
    \hfill
    \subfloat[Overarching tag counts]{\includegraphics[width=0.50\textwidth]{figures/histogram_overarching_tag_counts_log.pdf}\label{fig:histogram_overarching_tag_counts_log}}
    \caption{Histograms of tag counts (log scale)}
    \label{fig:histograms}
\end{figure}

We now turn to investigating the tag scores, which are a measure from 0 to 1 that the zeroshot text classifier assigns to each tag based on each individual description. \cref{fig:histogram_scores} shows the histogram of tag scores, while \cref{fig:histogram_overarching_scores} shows the histogram of overarching tag scores. We see that the distribution of scores is skewed, with a few tags having a very low score (close to 0) and a few tags having a very high score (close to 1), with the remaining tags having scores in between. This means that the zeroshot text classifier is filtering out tags that are not relevant to the descriptions, while assigning high scores to tags that are relevant.

\begin{figure}[h]
    \centering
    \subfloat[Tag scores]{\includegraphics[width=0.50\textwidth]{figures/histogram_scores.pdf}\label{fig:histogram_scores}}
    \hfill
    \subfloat[Overarching tag scores]{\includegraphics[width=0.50\textwidth]{figures/histogram_overarching_scores.pdf}\label{fig:histogram_overarching_scores}}
    \caption{Histograms of tag scores}
    \label{fig:histograms_scores}
\end{figure}

\section{Human evaluation}
In this section, we present the results of the human evaluation based on the definition of the study design in \cref{sec:human_evaluation}.

\subsection{Materials}
The same dataset descriptions were used across all three surveys to maintain consistency. Based on a pilot study, we selected three texts for the \textit{Individual Document Evaluation} stage and two for the \textit{Document Pair Evaluation} stage. The selection criteria focused on appropriate length and complexity to ensure participant comprehension. This number of texts allowed participants to complete the survey within approximately 45 minutes, balancing the need for sufficient data collection while preventing participant fatigue.

For brevity, we include only a few examples of the text-tag pairs used in the surveys. All three surveys can be found in the \href{https://github.com/ivangermanov/openml-tags}{GitHub repository} \cite{germanov_topic_modeling_of_2024} under the names of \texttt{proposed\_model\_survey.pdf}, \texttt{baseline\_survey.pdf}, and \texttt{human\_generated\_survey.pdf}.

An example text from the first stage, \textit{Individual Document Evaluation}:

\begin{quote}
    \textbf{FOREX USD/JPY Minute High}

    This dataset contains historical price data of the FOREX USD/JPY from Dukascopy. Each instance, or row, represents one candlestick of one minute. The dataset spans from January first to December thirteenth and does not include weekends, as the FOREX market is not traded on weekends. The timezone of the feature Timestamp is Europe/Amsterdam.
\end{quote}

This text was contained in all three surveys, and for each survey, a different set of tags was provided. \cref{tab:tag_comparison} shows a comparison of tags for different models.

\begin{table}[h]
    \centering
    \begin{tabular}{|>{\raggedright\arraybackslash}p{4cm}|>{\raggedright\arraybackslash}p{4cm}|>{\raggedright\arraybackslash}p{4cm}|}
        \hline
        \textbf{Proposed Model} & \textbf{Human-Generated} & \textbf{Baseline Model} \\ \hline
        Historical Price Data   & Historical Price Data    & Thyrotropin             \\ \hline
        Minute Interval         & Forex                    & Minute                  \\ \hline
        Historical Data         & USD/JPY                  & USD                     \\ \hline
        Forex                   & Currency Pairs           & Releasing               \\ \hline
        Candlestick             & Yearly Data              & High                    \\ \hline
        Minute                  & Finance                  & Bid                     \\ \hline
        High                    & Minute High              & Ask                     \\ \hline
    \end{tabular}
    \caption{Comparison of tags for different models}
    \label{tab:tag_comparison}
\end{table}

For the first task, \textit{Intruder Detection}, an intruder tag, which the participant had to identify, was added to the set of tags for each text. The intruder tag was selected at random from the generated tags of another OpenML dataset.

In the second task, \textit{Tag Quality Assessment}, for each tag, participants were asked to rate the relevance and generality. For each tag set, participants were asked to rate the coverage.

In the second stage, \textit{Document Pair Evaluation}, participants were provided pairs of texts. For example, one pair consisted of the following two texts:

\begin{quote}
    \textbf{Movies on Netflix, Prime Video, Hulu, and Disney+}

    This dataset is an amalgamation of data that was scraped, comprising a comprehensive list of movies available on various streaming platforms, and the IMDb dataset, which provides inspiration for analysis.

    Which streaming platform or platforms can I find this movie on? This dataset allows us to explore the availability of movies across different streaming services. Additionally, we can examine the average IMDb rating of movies produced in a specific country, providing insights into the quality of films from different regions.

    \textbf{Popular Movies of IMDb}

    TMDB.org is a crowd-sourced movie information database widely used by various film-related consoles, sites, and apps, such as XBMC, MythTV, and Plex. Dozens of media managers, mobile apps, and social sites utilize its API. At the time of writing, TMDB lists a substantial number of films, which is considerably fewer than IMDb. While not as comprehensive as IMDb, it holds extensive information for most popular and Hollywood films.
\end{quote}

In the first task, \textit{Common Tags Identification}, participants were asked to identify tags that were common to both texts, i.e., the intersection of the two tag sets. For instance, for the two tag sets in \cref{tab:tag_comparison_two_datasets}, the common tags were \textit{Film}, \textit{Entertainment}, \textit{Movies}, and \textit{Media} (as predicted by the model).

\begin{table}[h]
    \centering
    \begin{tabular}{|>{\raggedright\arraybackslash}p{6cm}|>{\raggedright\arraybackslash}p{6cm}|}
        \hline
        \textbf{Movies on Netflix, Prime Video, Hulu, and Disney+} & \textbf{Popular Movies of IMDb} \\ \hline
        Film                                                       & Entertainment                   \\ \hline
        Media                                                      & Technology                      \\ \hline
        Entertainment                                              & Film                            \\ \hline
        Movies                                                     & Media                           \\ \hline
        Film Industry                                              & Movies                          \\ \hline
        Streaming Platforms                                        & Popular Movies                  \\ \hline
                                                                   & Film Information                \\ \hline
    \end{tabular}
    \caption{Comparison of tags for two datasets}
    \label{tab:tag_comparison_two_datasets}
\end{table}


In the second task, \textit{Shared Coverage Assessment}, participants were asked to rate the shared coverage of the tags for both texts.

\subsection{Participants}
For the \textit{proposed model} survey, we recruited 21 participants, all of whom completed the survey. For the \textit{baseline model} survey, we recruited 19 participants, and for the \textit{human-generated} survey, we recruited 18 participants. There was a large overlap between the participants in the three surveys, with 93.3\% of participants completing all three surveys.

As shown in \cref{tab:demographics}, the majority of participants across all three surveys hold a Bachelor's degree (57.1-63.2\%), followed by those with a Master's degree (19.0-22.2\%). This educational background aligns with our aim to recruit participants with backgrounds similar to OpenML users. The age distribution shows that most participants (81.0-84.2\%) are between 25 and 34 years old. Regarding English proficiency, the majority of participants demonstrate advanced language skills, with 71.4-78.9\% at C1 (Advanced) level and an additional 10.5-19.0\% at C2 (Proficient) level.

\begin{table}[h]
    \centering
    \caption{Demographic characteristics of participants across surveys}
    \begin{tabular}{llccc}
        \hline
        Category & Characteristic & Human-generated & Baseline model & Proposed model \\
        \hline
        \multirow{5}{*}{Education} & Bachelor's degree & 63.2\% & 57.1\% & 61.1\% \\
        & Master's degree & 21.1\% & 19.0\% & 22.2\% \\
        & Doctoral degree & 5.3\% & 19.0\% & 5.6\% \\
        & Some college & 5.3\% & 4.8\% & 5.6\% \\
        & High school or equivalent & 5.3\% & 0.0\% & 5.6\% \\
        \hline
        \multirow{3}{*}{Age} & 25-34 & 84.2\% & 81.0\% & 83.3\% \\
        & 18-24 & 10.5\% & 9.5\% & 11.1\% \\
        & 35-44 & 5.3\% & 9.5\% & 5.6\% \\
        \hline
        \multirow{4}{*}{English} & C1 (Advanced) & 78.9\% & 71.4\% & 77.8\% \\
        & C2 (Proficient) & 10.5\% & 19.0\% & 11.1\% \\
        & B1 (Intermediate) & 5.3\% & 4.8\% & 5.6\% \\
        & Native speaker & 5.3\% & 4.8\% & 5.6\% \\
        \hline
    \end{tabular}
    \label{tab:demographics}
\end{table}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/education_pie.pdf}
%     \caption{Education level of participants}
%     \label{fig:education_pie}
% \end{figure}

% \cref{fig:age_range_pie} shows the age range of the participants. We see that the majority of participants are between 25 and 34 years old.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/age_range_pie.pdf}
%     \caption{Age range of participants}
%     \label{fig:age_range_pie}
% \end{figure}

% \cref{fig:english_prof_pie} shows the English proficiency of the participants. We see that the majority of participants possess a high level of English proficiency.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/english_prof_pie.pdf}
%     \caption{English proficiency of participants}
%     \label{fig:english_prof_pie}
% \end{figure}

\subsection{Results}
\subsubsection{Stage 1 — Task 1: Intruder Detection}
\Cref{tab:intruder_detection} presents the intruder detection results for all three texts (\textit{League of Legends}, \textit{Forex} and \textit{Lung Cancer}), across all three surveys. The proposed model demonstrated substantially better performance than the baseline model (95.2\% vs 42.1\% overall detection rate), though it did not quite match the perfect performance achieved by human-generated tags (100\% detection rate). The baseline model showed particularly poor performance on the Forex text, with only 5.3\% detection rate. These results suggest that human-generated tags exhibited the highest quality, as participants consistently identified the intruder tags, indicating stronger cohesion among the human-generated tag sets.

\begin{table}[h]
    \centering
    \caption{Intruder detection results across different text sources and survey types}
    \begin{tabular}{llcc}
        \hline
        Survey Type & Text Source & Detected & Not Detected \\
        \hline
        \multirow{4}{*}{Baseline Model} & League of Legends & 63.2\% (12) & 36.8\% (7) \\
        & Forex & 5.3\% (1) & 94.7\% (18) \\
        & Lung Cancer & 57.9\% (11) & 42.1\% (8) \\
        & Total & 42.1\% (24) & 57.9\% (33) \\
        \hline
        \multirow{4}{*}{Proposed Model} & League of Legends & 100.0\% (21) & 0.0\% (0) \\
        & Forex & 90.5\% (19) & 9.5\% (2) \\
        & Lung Cancer & 95.2\% (20) & 4.8\% (1) \\
        & Total & 95.2\% (60) & 4.8\% (3) \\
        \hline
        \multirow{4}{*}{Human-generated} & League of Legends & 100.0\% (18) & 0.0\% (0) \\
        & Forex & 100.0\% (18) & 0.0\% (0) \\
        & Lung Cancer & 100.0\% (18) & 0.0\% (0) \\
        & Total & 100.0\% (54) & 0.0\% (0) \\
        \hline
    \end{tabular}
    \label{tab:intruder_detection}
\end{table}

\subsubsection{Stage 1 — Task 2: Tag Quality Assessment}
\cref{fig:tags_analysis_comparison_relevance,fig:tags_analysis_comparison_generality,fig:tags_analysis_comparison_coverage} show the tag quality assessment results for the baseline model, the proposed model, and the human-generated tags, respectively. The histograms display the distribution of scores for relevance, generality, and coverage across all surveys (Human-generated — HG, Proposed Model — PM, Baseline Model — BM), combining data from all three texts. For each measure, we include the mean scores, standard deviations, and 95\% confidence intervals.

\begin{figure}[h]
    \centering
    \subfloat[Relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_relevance_comparison.pdf}\label{fig:tags_analysis_comparison_relevance}}
    \hfill
    \subfloat[Generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_generality_comparison.pdf}\label{fig:tags_analysis_comparison_generality}}
    \hfill
    \subfloat[Coverage scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_coverage_comparison.pdf}\label{fig:tags_analysis_comparison_coverage}}
    \caption{Aggregated scores comparison by model}
    \label{fig:tags_analysis_comparison}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \subfloat[Relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_baseline_relevance.pdf}\label{fig:tags_analysis_baseline_relevance}}
%     \hfill
%     \subfloat[Generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_baseline_generality.pdf}\label{fig:tags_analysis_baseline_generality}}

%     \subfloat[Coverage scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_baseline_coverage.pdf}\label{fig:tags_analysis_baseline_coverage}}
%     \caption{Tag quality assessment results for the baseline model}
%     \label{fig:tags_analysis_baseline}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \subfloat[Relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_model_relevance.pdf}\label{fig:tags_analysis_model_relevance}}
%     \hfill
%     \subfloat[Generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_model_generality.pdf}\label{fig:tags_analysis_model_generality}}

%     \subfloat[Coverage scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_model_coverage.pdf}\label{fig:tags_analysis_model_coverage}}
%     \caption{Tag quality assessment results for the proposed model}
%     \label{fig:tags_analysis_proposed}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \subfloat[Relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_human_relevance.pdf}\label{fig:tags_analysis_human_relevance}}
%     \hfill
%     \subfloat[Generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_human_generality.pdf}\label{fig:tags_analysis_human_generality}}

%     \subfloat[Coverage scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_human_coverage.pdf}\label{fig:tags_analysis_human_coverage}}
%     \caption{Tag quality assessment results for the human-generated tags}
%     \label{fig:tags_analysis_human}
% \end{figure}

Analysis of the scoring metrics revealed that the proposed model demonstrated superior performance compared to the baseline model. For relevance scores (higher is better), the baseline model achieved a mean of 2.39, while the proposed model achieved 3.63, which is closer to the human-generated tags' score of 3.95. Regarding generality, where larger standard deviations indicate a better balance between general and specific tags, the proposed model (mean = 2.93, SD = 1.40) performed better than the baseline (mean = 3.13, SD = 1.33) and nearly matched human-generated tags (mean = 3.00, SD = 1.41). In terms of coverage, the proposed model (mean = 4.46) outperformed the baseline model (mean = 2.65) and was close to the human-generated tags (mean = 4.54). However, it is notable that the baseline model had a significantly higher number of tags marked as "Not Applicable" (97 tags), compared to the proposed model (11) and human-generated tags (3). This suggests that while the baseline model's valid tags showed similar generality levels, it produced many irrelevant tags that evaluators couldn't meaningfully assess for generality.

The human-generated tags achieved the best scores in relevance, generality and coverage, indicating that the human-generated tags were of higher quality. The proposed model outperformed the baseline model by a wide margin, and only somewhat lagged behind the human-generated tags. This suggests that the proposed model was able to generate better tags than the baseline model, but still not quite as high-quality as human-generated tags.

Another notable observation is the relationship between relevance and generality scores across the three tag sources. The proposed model exhibited the strongest negative correlation (-0.59), indicating that as tags became more specific, they were generally rated as more relevant. The baseline model showed a similar but moderately weaker pattern (-0.42). In contrast, human-generated tags demonstrated a much weaker negative correlation (-0.19), suggesting that human-generated tags were more relevant across both specific and general levels. For all three, we observe p-values below 0.001, indicating that the correlations are statistically significant. This balance between specificity and generality in human-generated tags indicates a more nuanced approach to tag creation, where both specific details and broader contextual tags can maintain high relevance.

Additionally, we inspect the aggregated relevance and generality scores by tag type (regular vs. overarching tags, as explained in \cref{sec:tag_generation_pipeline}) in \cref{fig:tags_analysis_human_multiple_documents} for human-generated tags and in \cref{fig:tags_analysis_model_multiple_documents} for the proposed model. We see that the relevance scores mean for the proposed model is slightly higher for regular tags (3.77) than for overarching tags (3.48), while the generality scores are higher for overarching tags (3.52) than for regular tags (2.38). This second observation indicates that the pipeline we designed was successful in generating more specific tags for regular tags and more general tags for overarching tags. For human-generated tags, we see a similar pattern, with higher relevance scores for regular tags (4.03) than for overarching tags (3.81), and higher generality scores for overarching tags (3.74) than for regular tags (2.61).


\begin{figure}[h]
    \centering
    \subfloat[Aggregated relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_model_multiple_documents_relevance.pdf}\label{fig:tags_analysis_model_multiple_documents_relevance}}
    \hfill
    \subfloat[Aggregated generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_model_multiple_documents_generality.pdf}\label{fig:tags_analysis_model_multiple_documents_generality}}
    \caption{Aggregated relevance and generality scores by tag type (proposed model)}
    \label{fig:tags_analysis_model_multiple_documents}
\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Aggregated relevance scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_human_multiple_documents_relevance.pdf}\label{fig:tags_analysis_human_multiple_documents_relevance}}
    \hfill
    \subfloat[Aggregated generality scores]{\includegraphics[width=0.5\textwidth]{figures/tags_analysis_human_multiple_documents_generality.pdf}\label{fig:tags_analysis_human_multiple_documents_generality}}
    \caption{Aggregated relevance and generality scores by tag type (human-generated)}
    \label{fig:tags_analysis_human_multiple_documents}
\end{figure}


The histograms provided here only show the aggregated results across all three texts, across all tags. For readers interested in the details for each individual text or tag, additional information can be found in the \textit{human\_evaluation\_analysis.ipynb} notebook in the \href{https://github.com/ivangermanov/openml-tags}{GitHub repository} \cite{germanov_topic_modeling_of_2024}.

\paragraph{Inter-rater Reliability}
We calculated the inter-rater reliability for the relevance, generality, and coverage scores using multiple metrics, including Fleiss' Kappa, Interclass Correlation Coefficient (ICC) and Krippendorff's Alpha. We do not use Cohen's Kappa, and use Fleiss' Kappa instead, as it is more suitable for multiple raters and multiple categories.

\cref{tab:fleiss_kappa_comparison} shows the Fleiss' Kappa values for the different evaluation metrics for the baseline model, the proposed model, and the human-generated tags. We see that all values are small, indicating slight agreement at best. However, it is important to note that our data are Likert scale data, which are ordinal, and are not handled well by Fleiss' Kappa, as it treats categories as nominal.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Evaluation metric} & \textbf{Baseline model} & \textbf{Proposed model} & \textbf{Human-generated} \\
        \hline
        Relevance                  & 0.15              & 0.12                    & 0.04                     \\
        Generality                 & -0.02             & 0.18                    & 0.10                     \\
        Coverage                   & 0.03              & 0.01                    & -0.02                    \\
        Shared Coverage            & 0.03              & 0.06                    & -0.01                    \\
        \hline
    \end{tabular}
    \caption{Fleiss' Kappa values comparison for different evaluation metrics across models}
    \label{tab:fleiss_kappa_comparison}
\end{table}

We also used ICC to assess the consistency of ratings across our raters (\cref{fig:icc_baseline,fig:icc_model,fig:icc_human}). Among the main forms of ICC, ICC(1) assumes random raters and absolute agreement, ICC(2) assumes random raters with consistency agreement, while ICC(3) is designed for fixed raters with consistency agreement. Furthermore, ICC(3,k) specifically measures the reliability of averaged ratings rather than individual ratings. ICC(3,k) is most appropriate for our study design because we have a fixed set of participants rating the same texts across different conditions (baseline, proposed model, and human-generated tags), and we are interested in the reliability of averaged ratings rather than individual ratings. ICC(3,k) accounts for systematic differences in how individual participants use the rating scale while providing reliability estimates for the averaged scores, making it the most suitable choice for comparing the reliability of ratings across our three tag generation methods.



Looking at the ICC(3,k) values for relevance, where the F-statistic indicates the ratio of between-group to within-group variance (higher values showing stronger rater agreement), we see that the proposed model achieved an ICC of 0.95 ($F=19.27$, $p=7.72\times10^{-47}$, CI$_{95\%}$[0.91,0.98]), the baseline model an ICC of 0.96 ($F=25.92$, $p=1.18\times10^{-57}$, CI$_{95\%}$[0.93,0.98]), and the human-generated tags an ICC of 0.89 ($F=8.70$, $p=7.21\times10^{-20}$, CI$_{95\%}$[0.80,0.95]). For generality, the proposed model performed best with an ICC of 0.97 ($F=31.71$, $p=4.86\times10^{-53}$, CI$_{95\%}$[0.94,0.99]), followed by human-generated tags at 0.94 ($F=16.76$, $p=5.01\times10^{-32}$, CI$_{95\%}$[0.89,0.97]), while the baseline model showed poor reliability with -0.01 ($F=0.99$, $p=0.416$, CI$_{95\%}$[-1.98,0.88]) and a non-significant $p$-value indicating unreliable results. For coverage, the baseline model achieved the highest reliability at 0.95 ($F=21.51$, $p=7.14\times10^{-7}$, CI$_{95\%}$[0.81,1.00]), followed by the proposed model at 0.82 ($F=5.57$, $p=0.007$, CI$_{95\%}$[0.27,1.00]), while human-generated tags performed poorly at 0.08 ($F=1.09$, $p=0.348$, CI$_{95\%}$[-2.78,0.98]) with non-significant results. For shared coverage, the proposed model showed the strongest reliability at 0.90 ($F=10.25$, $p=0.004$, CI$_{95\%}$[0.43,1.00]), followed by human-generated tags at 0.76 ($F=4.25$, $p=0.055$, CI$_{95\%}$[-0.42,1.00]) and the baseline model at 0.73 ($F=3.65$, $p=0.072$, CI$_{95\%}$[-0.64,1.00]), though the latter two had borderline significant $p$-values suggesting less reliable results.

Similar to Fleiss' Kappa, ICC may not be ideal for ordinal data, as it assumes continuous data. We also provide results for Krippendorff's Alpha, which can handle both ordinal and nominal data. \cref{tab:kalpha} shows the Krippendorff's Alpha values for the different evaluation metrics for the baseline model, the proposed model, and the human-generated tags. We see that the baseline model performs best in shared coverage ($\alpha=0.4818$), while showing moderate reliability for generality ($\alpha=0.2863$) and coverage ($\alpha=0.2600$), but lower reliability for relevance ($\alpha=0.1687$). The proposed model shows strongest reliability in coverage ($\alpha=0.5647$), moderate reliability in shared coverage ($\alpha=0.2497$) and relevance ($\alpha=0.2263$), but lower reliability in generality ($\alpha=0.1671$). The human-generated tags show the highest reliability for relevance ($\alpha=0.4692$) and coverage ($\alpha=0.4503$), but lower reliability for generality ($\alpha=0.2044$) and shared coverage ($\alpha=0.1898$). 

This analysis reveals a complex and sometimes contradictory pattern of interrater reliability. The metrics often disagree substantially in their assessment of reliability. For instance, while the proposed model shows remarkably high ICC values for generality (0.97) and shared coverage (0.90), the corresponding Krippendorff's Alpha values are notably low (0.1671 and 0.2497 respectively). Similarly, the baseline model's high ICC scores for relevance (0.96) and coverage (0.95) contrast sharply with its low Krippendorff's Alpha (0.1687). Human-generated tags show their own contradictions - strong ICC for relevance (0.89) paired with moderate Krippendorff's Alpha (0.4692), and very low ICC for coverage (0.08) yet moderate Krippendorff's Alpha (0.4503). These discrepancies between metrics make it particularly challenging to draw definitive conclusions about relative reliability across the different approaches.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Metric} & \textbf{Baseline model} & \textbf{Proposed model} & \textbf{Human-generated} \\
        \hline
        Relevance       & 0.1687            & 0.2263                  & 0.4692                   \\
        Generality      & 0.2863            & 0.1671                  & 0.2044                   \\
        Coverage        & 0.2600            & 0.5647                  & 0.4503                   \\
        Shared Coverage & 0.4818            & 0.2497                  & 0.1898                   \\
        \hline
    \end{tabular}
    \caption{Krippendorff's Alpha values for different rating types across models}
    \label{tab:kalpha}
\end{table}

\subsubsection{Stage 2 — Task 1: Common Tags Identification}
The confusion matrices in \cref{fig:first_pair_common_tags_confusion_matrix} show the common tags confusion matrix comparison for the first pair of texts. The confusion matrices in \cref{fig:second_pair_common_tags_confusion_matrix} show the common tags confusion matrix comparison for the second pair of texts.

The top-left quadrant of the confusion matrix represents the true positives, i.e., the number of tags that the model predicted as common tags that human evaluators also identified as common tags. The bottom-right quadrant represents the true negatives, i.e., the number of tags that the model predicted as not common tags that human evaluators also identified as not common tags. The top-right quadrant represents the false negatives, i.e., the number of tags that the model predicted as common tags that human evaluators identified as not common tags. The bottom-left quadrant represents the false positives, i.e., the number of tags that the model predicted as not common tags that human evaluators identified as common tags.

\begin{figure}[h]
    \centering
    \subfloat[Baseline model]{\includegraphics[width=0.5\textwidth]{figures/first_pair_common_tags_confusion_matrix_baseline.pdf}\label{fig:first_pair_common_tags_confusion_matrix_baseline}}
    \hfill
    \subfloat[Proposed model]{\includegraphics[width=0.5\textwidth]{figures/first_pair_common_tags_confusion_matrix_model.pdf}\label{fig:first_pair_common_tags_confusion_matrix_model}}

    \subfloat[Human-generated]{\includegraphics[width=0.5\textwidth]{figures/first_pair_common_tags_confusion_matrix_human.pdf}\label{fig:first_pair_common_tags_confusion_matrix_human}}
    \caption{Common tags confusion matrix comparison for the first pair}
    \label{fig:first_pair_common_tags_confusion_matrix}
\end{figure}

\begin{figure}[h]
    \centering
    \subfloat[Baseline model]{\includegraphics[width=0.5\textwidth]{figures/second_pair_common_tags_confusion_matrix_baseline.pdf}\label{fig:second_pair_common_tags_confusion_matrix_baseline}}
    \hfill
    \subfloat[Proposed model]{\includegraphics[width=0.5\textwidth]{figures/second_pair_common_tags_confusion_matrix_model.pdf}\label{fig:second_pair_common_tags_confusion_matrix_model}}

    \subfloat[Human-generated]{\includegraphics[width=0.5\textwidth]{figures/second_pair_common_tags_confusion_matrix_human.pdf}\label{fig:second_pair_common_tags_confusion_matrix_human}}
    \caption{Common tags confusion matrix comparison}
    \label{fig:second_pair_common_tags_confusion_matrix}
\end{figure}

To interpret the confusion matrices, in \cref{tab:metrics_comparison_combined}, we compare accuracy, precision, recall, specificity, and F1-Score between the first and second pairs for the baseline model, the proposed model, and human-generated tags, respectively. The results show varying performance patterns across different metrics. For the first pair, the proposed model achieved the highest accuracy (0.87) and F1-Score (0.83), outperforming both the baseline model (0.69 accuracy, 0.43 F1-Score) and slightly exceeding human-generated tags (0.83 accuracy, 0.79 F1-Score). However, for the second pair, human-generated tags demonstrated superior performance with 0.86 accuracy compared to the proposed model's 0.76 and the baseline's 0.60.

The baseline model showed a peculiar pattern in the second pair, with perfect precision (1.00) due to having no false positives, but poor recall (0.29) due to a high number of false negatives. This is because the two surveys were in the same topic cluster, and since the zeroshot classifier was not applied, all the tags from the cluster were provided. These results showcase how not using the LLM fine-tuning step and the zeroshot classifier can lead to poor performance.

The proposed model maintained more balanced metrics across both pairs, though with some decline in performance for the second pair, particularly in precision (dropping from 0.81 to 0.58). Human-generated tags showed the most consistent performance across both pairs, with all metrics maintaining relatively stable values between 0.69 and 0.89.

These results suggest that while the proposed model shows promising performance, particularly for the first pair, its effectiveness varies more between different text pairs compared to human evaluators. The baseline model's extreme variations in metrics indicate less reliable performance overall.



\begin{table}[h]
    \centering
    \begin{tabular}{lcccccc}
        \hline
        \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{Baseline model}} & \multicolumn{2}{c}{\textbf{Proposed model}} & \multicolumn{2}{c}{\textbf{Human-generated}}                           \\
        \cline{2-7}
                                         & First                                 & Second                                      & First                                        & Second & First & Second \\
        \hline
        Accuracy                         & 0.69                                  & 0.60                                        & 0.87                                         & 0.76   & 0.83  & 0.86   \\
        Precision                        & 0.40                                  & 1.00                                        & 0.81                                         & 0.58   & 0.84  & 0.69   \\
        Recall                           & 0.47                                  & 0.29                                        & 0.87                                         & 0.79   & 0.74  & 0.85   \\
        Specificity                      & 0.76                                  & 1.00                                        & 0.87                                         & 0.75   & 0.89  & 0.86   \\
        F1-Score                         & 0.43                                  & 0.45                                        & 0.83                                         & 0.67   & 0.79  & 0.76   \\
        \hline
    \end{tabular}
    \caption{Comparison of evaluation metrics between first and second pairs across all models}
    \label{tab:metrics_comparison_combined}
\end{table}

\paragraph{Statistical significance}
We additionally perform Kruskal-Wallis tests to determine if the differences in the evaluation metrics between the baseline model, the proposed model, and human-generated tags are statistically significant. The Kruskal-Wallis test is a non-parametric test that compares the medians of two or more groups. We use this test because our data are ordinal and do not meet the assumptions of parametric tests, which require a normal distribution and homogeneity of variances.

\cref{tab:kruskal_wallis} shows the results of the Kruskal-Wallis H test for the different evaluation metrics. We see that the differences in relevance, coverage, and shared coverage are statistically significant, while the differences in generality are not statistically significant. The H-statistic indicates the magnitude of difference between the groups, with higher values suggesting greater differences. The highest H-statistic is observed for relevance (H = 239.29), indicating substantial differences in how participants rated tag relevance across the three methods. Coverage (H = 65.54) and shared coverage (H = 47.82) also show considerable differences, while generality shows minimal differences (H = 4.56).

The effect sizes, measured by $\eta^2$, provide additional insight into the practical significance of these differences. According to common interpretation guidelines, $\eta^2$ values of 0.01, 0.06, and 0.14 represent small, medium, and large effects, respectively. The results show large effect sizes for coverage ($\eta^2 = 0.562$), shared coverage ($\eta^2 = 0.405$), and relevance ($\eta^2 = 0.300$). In contrast, generality shows a medium effect size ($\eta^2 = 0.005$). The p-values (< 0.001) for relevance, coverage, and shared coverage indicate that these differences are highly unlikely to have occurred by chance, while the non-significant p-value for generality (p = 0.102) suggests that any observed differences in generality ratings could be due to random variation.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Metric} & \textbf{H-statistic} & \textbf{p-value} & \textbf{$\eta^2$} \\
        \hline
        Relevance       & 239.29               & < 0.001          & 0.300             \\
        Generality      & 4.56                 & 0.102            & 0.005             \\
        Coverage        & 65.54                & < 0.001          & 0.562             \\
        Shared Coverage & 47.82                & < 0.001          & 0.405             \\
        \hline
    \end{tabular}
    \caption{Kruskal-Wallis H test results for different evaluation metrics}
    \label{tab:kruskal_wallis}
\end{table}

After finding these statistically significant differences, we perform post-hoc pairwise comparisons using Dunn's test with Bonferroni correction to identify which groups differ from each other. \cref{fig:dunns_test} shows the p-values for the post-hoc tests for relevance, generality, coverage, and shared coverage. The results indicate that the differences between the baseline model and the proposed model, as well as between the baseline model and human-generated tags, are statistically significant for all metrics, besides generality. The differences between the proposed model and human-generated tags are only statistically significant for relevance (p = 0.011). For generality, coverage, and shared coverage, there were no statistically significant differences between our proposed model and human-generated tags (p = 0.158, p = 1.000, and p = 0.511 respectively). This suggests that while our model still lags behind human performance in terms of tag relevance, it achieves comparable performance to humans in generating tags with appropriate generality levels and comprehensive coverage of the text content. Given our sample size of 21 participants, the lack of statistically significant differences between the proposed model and human-generated tags for generality, coverage, and shared coverage could be due to limited statistical power. A larger number of participants would increase our ability to detect smaller differences between these approaches, if they exist. Future work with more raters could provide stronger statistical evidence to either confirm the apparent equivalence between our model and human performance or reveal differences that our current sample size was unable to detect.

\begin{figure}[h]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{lccc}
            \hline
                     & \textbf{Baseline} & \textbf{Human} & \textbf{Model} \\
            \hline
            Baseline & 1.000             & < 0.001        & < 0.001        \\
            Human    & < 0.001           & 1.000          & 0.011          \\
            Model    & < 0.001           & 0.011          & 1.000          \\
            \hline
        \end{tabular}
        \caption*{(a) Relevance}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{lccc}
            \hline
                     & \textbf{Baseline} & \textbf{Human} & \textbf{Model} \\
            \hline
            Baseline & 1.000             & 1.000          & 0.417          \\
            Human    & 1.000             & 1.000          & 0.158          \\
            Model    & 0.417             & 0.158          & 1.000          \\
            \hline
        \end{tabular}
        \caption*{(b) Generality}
    \end{minipage}

    \vspace{1em}

    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{lccc}
            \hline
                     & \textbf{Baseline} & \textbf{Human} & \textbf{Model} \\
            \hline
            Baseline & 1.000             & < 0.001        & < 0.001        \\
            Human    & < 0.001           & 1.000          & 1.000          \\
            Model    & < 0.001           & 1.000          & 1.000          \\
            \hline
        \end{tabular}
        \caption*{(c) Coverage}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \begin{tabular}{lccc}
            \hline
                     & \textbf{Baseline} & \textbf{Human} & \textbf{Model} \\
            \hline
            Baseline & 1.000             & < 0.001        & < 0.001        \\
            Human    & < 0.001           & 1.000          & 0.511          \\
            Model    & < 0.001           & 0.511          & 1.000          \\
            \hline
        \end{tabular}
        \caption*{(d) Shared Coverage}
    \end{minipage}
    \caption{Dunn's post-hoc test p-values for different metrics}
    \label{fig:dunns_test}
\end{figure}

We include Cliff's Delta effect size to provide additional context on the magnitude of differences between groups. Cliff's Delta is a non-parametric effect size measure that quantifies the difference between two groups by calculating the probability that a randomly selected observation from one group will be greater than a randomly selected observation from the other group. The effect sizes are interpreted as small (0.147), medium (0.33), and large (0.474) based on common guidelines. \cref{tab:cliffs_delta} shows the effect sizes for the different metrics and comparisons. We see that baseline scores differ substantially from both human and model scores for most metrics. For relevance ratings, there are large negative effects when comparing baseline to both human ($\delta = -0.703$) and model ($\delta = -0.599$), indicating that baseline tags were rated consistently lower in relevance. The small positive effect between human and model ratings ($\delta = 0.163$) suggests that model-generated tags achieve relevance levels comparable to human-generated ones.

Generality shows a different pattern, with negligible effects across all comparisons ($\delta$ ranging from 0.014 to 0.112), indicating that all three methods produce tags with similar levels of generality. However, these results should be interpreted with caution, as the Not Applicable (NA) tags were excluded from the generality analysis.

Coverage metrics reveal the most pronounced differences. Both overall coverage and shared coverage show large negative effects when comparing baseline to human ($\delta = -0.885$ and $-0.789$ respectively) and model ($\delta = -0.888$ and $-0.681$ respectively). This strongly suggests that both human and model-generated tag sets provide substantially better coverage than baseline tags. The negligible to small effects between human and model coverage scores ($\delta = -0.053$ for coverage, $\delta = 0.200$ for shared coverage) indicate that the model achieves coverage levels similar to human performance.

\begin{table}[h]
    \centering
    \begin{tabular}{llrl}
        \hline
        \textbf{Metric} & \textbf{Comparison} & \textbf{Delta} & \textbf{Effect} \\
        \hline
        \multirow{3}{*}{Relevance}
                        & Baseline model vs Human-generated   & -0.703         & Large           \\
                        & Baseline model vs Proposed model   & -0.599         & Large           \\
                        & Human-generated vs Proposed model      & 0.163          & Small           \\
        \hline
        \multirow{3}{*}{Generality}
                        & Baseline model vs Human-generated   & 0.014          & Negligible      \\
                        & Baseline model vs Proposed model   & 0.112          & Negligible      \\
                        & Human-generated vs Proposed model      & 0.101          & Negligible      \\
        \hline
        \multirow{3}{*}{Coverage}
                        & Baseline model vs Human-generated   & -0.885         & Large           \\
                        & Baseline model vs Proposed model   & -0.888         & Large           \\
                        & Human-generated vs Proposed model      & -0.053         & Negligible      \\
        \hline
        \multirow{3}{*}{Shared Coverage}
                        & Baseline model vs Human-generated   & -0.789         & Large           \\
                        & Baseline model vs Proposed model   & -0.681         & Large           \\
                        & Human-generated vs Proposed model      & 0.200          & Small           \\
        \hline
    \end{tabular}
    \caption{Cliff's Delta effect sizes for different metrics and comparisons}
    \label{tab:cliffs_delta}
\end{table}

To provide a comprehensive analysis of effect sizes, we calculated Cohen's d alongside Cliff's Delta, while acknowledging its limitations for our data characteristics. Given our ordinal Likert scale data, non-normal distribution, and small sample size (n=21), Cohen's d results should be interpreted with caution. Nevertheless, as shown in \cref{tab:cohens_d}, the calculations reveal strong effects that align with our Cliff's Delta findings. For relevance, large negative effects were found comparing baseline to both human ($d = -1.589$) and model ($d = -1.254$), with a small to medium effect between human and model ($d = 0.293$). Generality showed negligible to small effects across comparisons ($d$ ranging from 0.027 to 0.202). Coverage metrics demonstrated the largest effects, with very large negative effects when comparing baseline to both human ($d = -2.454$) and model ($d = -2.539$) for overall coverage, and similarly large effects for shared coverage (baseline vs human: $d = -1.882$; baseline vs model: $d = -1.485$). The human vs model comparisons showed negligible effects for coverage ($d = -0.078$) and small to medium effects for shared coverage ($d = 0.391$). While these results support our main findings, we primarily rely on Cliff's Delta as our effect size measure due to its better alignment with our data characteristics and non-parametric analysis approach.

\subsubsection{Stage 2 — Task 2: Common Tags Quality Assessment}
\cref{fig:first_pair_common_tags_coverage_comparison,fig:second_pair_common_tags_coverage_comparison} show the common tags coverage comparison for the first and second pairs, respectively.

Similar to the tag quality assessment, we see that the proposed model outperformed the baseline model, but still lagged behind human-generated tags, for both pairs of texts.

For the first pair, the proposed model achieved a mean coverage of 4.67, while the baseline model achieved a mean coverage of 3.32. The human-generated tags achieved a mean coverage of 4.83. For the second pair, the proposed model achieved a mean coverage of 4.14, while the baseline model achieved a mean coverage of 2.89. The human-generated tags achieved a mean coverage of 4.50.

This indicates that the proposed model was able to identify common tags more accurately than the baseline model, but still not quite as accurately as the human-generated tags.

\begin{figure}[h]
    \centering
    \subfloat[Common tags coverage by model for the first pair]{\includegraphics[width=0.5\textwidth]{figures/first_pair_common_tags_coverage_comparison.pdf}\label{fig:first_pair_common_tags_coverage_comparison}}
    \hfill
    \subfloat[Common tags coverage by model for the second pair]{\includegraphics[width=0.5\textwidth]{figures/second_pair_common_tags_coverage_comparison.pdf}\label{fig:second_pair_common_tags_coverage_comparison}}
    \caption{Common tags coverage comparison by model for both pairs}
    \label{fig:common_tags_coverage_comparison}
\end{figure}

\subsection{Large-scale automated evaluation}
We follow the methodology outlined in \cref{sec:large_scale_evaluation} to evaluate the proposed model, running the evaluation on all $\sim$5200 datasets. As an evaluation model, we pick \textit{GPT-4o-mini}, as it is a smaller and relatively less expensive LLM compared to larger models, yet offers good performance compared to models of similar size \cite{noauthor_gpt-4o_nodate}.

\subsubsection{Automated Intruder Detection}
The automated intruder detection experiment revealed that the LLM successfully identified 88.3\% of intruders, with only 11.7\% of cases resulting in incorrect detection. This high accuracy in identifying intruder tags among true tags suggests that the tags generated by the proposed model form cohesive, semantically related groups. The ability to distinguish contextually unrelated tags from related ones provides evidence for the semantic consistency of the model's tag generation process. In contrast, for the baseline model, only 47.5\% of intruders were correctly identified, indicating a much lower level of semantic coherence in the tag sets generated by the baseline model.

We observe that the intruder detection accuracy decreases as the number of tags increases, as shown in \cref{fig:gpt_accuracy_by_num_tags}. This is expected, as the evaluation model has to consider more tags, making it more challenging to detect intruders. Choosing a larger model may help improve intruder detection accuracy for a larger number of tags.

This finding is corroborated by the correlation analysis in \cref{fig:gpt_correlation_num_tags}, which shows a negative correlation between the number of tags and intruder detection accuracy.

\begin{figure}[h]
    \centering
    \subfloat[Intruder detection accuracy by number of tags]{\includegraphics[width=0.5\textwidth]{figures/gpt_accuracy_by_num_tags.pdf}\label{fig:gpt_accuracy_by_num_tags}}
    \hfill
    \subfloat[Correlation between number of tags and intruder detection accuracy]{\includegraphics[width=0.5\textwidth]{figures/gpt_correlation_num_tags.pdf}\label{fig:gpt_correlation_num_tags}}
    \caption{Analysis of Intruder Detection Accuracy and Number of Tags}
    \label{fig:gpt_num_tags_analysis}
\end{figure}

\subsubsection{Automated Tag Quality Assessment}
We investigate the correlation between relevance and generality and see that there is a positive correlation between them, with a Pearson correlation coefficient of 0.108 and a Spearman correlation coefficient of 0.012, both of which have a p-value of less than 0.01. Given the Likert scale nature of the data, we consider the Spearman correlation coefficient to be more reliable, indicating that there is practically no correlation between relevance and generality according to the LLM.

This is a surprising finding, as the human evaluation showed a negative correlation between relevance and generality, namely, -0.59. This discrepancy may be attributed to the differences in how humans and LLMs perceive relevance and generality, or to the limitations of the LLM in capturing the nuances of human judgment. However, upon closer inspection of the data, we find that for the same texts used in the human study, the LLM also shows a negative correlation between relevance and generality, with a Spearman correlation coefficient of -0.407 and a p-value of 0.075. This suggests that the discrepancy in the correlation between relevance and generality is likely due to the differences in the three datasets used for the human evaluation rather than a fundamental difference in how humans and LLMs perceive relevance and generality. For the tags generated by our proposed model, this finding may indicate that there is a lower or nonexistent correlation between relevance and generality, meaning that both regular and overarching tags are found as relevant. This means that we should not lean on choosing one over the other, but rather consider including both types of tags in the final tag set.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/gpt_relevance_vs_generality.pdf}
%     \caption{Correlation between relevance and generality}
%     \label{fig:gpt_relevance_vs_generality}
% \end{figure}

% \cref{fig:gpt_average_relevance_score_per_tag} and \cref{fig:gpt_average_generality_score_per_tag} shows a histogram of the average relevance and generality scores per tag, respectively. We see how many tags are rated as extremely relevant (5), very relevant (4), etc., and how many tags are rated as very general (5), somewhat general (4), etc.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/gpt_average_relevance_score_per_tag.pdf}
%     \caption{Average relevance score per tag}
%     \label{fig:gpt_average_relevance_score_per_tag}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/gpt_average_generality_score_per_tag.pdf}
%     \caption{Average generality score per tag}
%     \label{fig:gpt_average_generality_score_per_tag}
% \end{figure}

When we investigate the scores for the generated tags by the proposed model, we find a mean relevance score of 4.11 and a mean generality score of 3.29 with a standard deviation of 0.87. The mean coverage score is 3.72 (\cref{fig:gpt_tag_type_score_distributions}).

\begin{figure}[h]
    \centering
    \subfloat[Distribution of relevance scores by tag type]{\includegraphics[width=0.5\textwidth]{figures/relevance_distribution.pdf}\label{fig:gpt_tag_type_relevance_distribution}}
    \hfill
    \subfloat[Distribution of generality scores by tag type]{\includegraphics[width=0.5\textwidth]{figures/generality_distribution.pdf}\label{fig:gpt_tag_type_generality_distribution}}
    
    \subfloat[Distribution of coverage scores]{\includegraphics[width=0.5\textwidth]{figures/coverage_distribution.pdf}\label{fig:gpt_coverage_distribution}}
    \caption{Distribution of relevance, generality, and coverage scores}
    \label{fig:gpt_tag_type_score_distributions}
\end{figure}

When comparing with the tags generated by the baseline model (\cref{fig:gpt_baseline_score_distributions}), for which we have a mean of 3.26 for relevance, 1.13 SD for generality and 3.11 mean for coverage, we see that the proposed model outperforms the baseline model in relevance and coverage, according to the LLM. Surprisingly, the generality score is lower for the proposed model compared to the baseline model, which seems contrary to the human evaluation results. However, in the evaluation of the human study results, we performed a statistical analysis to determine the significance of the differences in generality between the baseline model and the proposed model, and found no significant difference. Therefore, this findidng is not necessarily contradictory to the human evaluation results.

\begin{figure}[h]
    \centering
    \subfloat[Distribution of relevance scores (baseline)]{\includegraphics[width=0.5\textwidth]{figures/gpt_relevance_score_distribution_baseline.pdf}\label{fig:gpt_relevance_distribution_baseline}}
    \hfill
    \subfloat[Distribution of generality scores (baseline)]{\includegraphics[width=0.5\textwidth]{figures/gpt_generality_score_distribution_baseline.pdf}\label{fig:gpt_generality_distribution_baseline}}
    
    \subfloat[Distribution of coverage scores (baseline)]{\includegraphics[width=0.5\textwidth]{figures/gpt_coverage_score_distribution_baseline.pdf}\label{fig:gpt_coverage_distribution_baseline}}
    \caption{Distribution of relevance, generality, and coverage scores (baseline)}
    \label{fig:gpt_baseline_score_distributions}
\end{figure}

A comparison between the LLM's evaluation of the proposed model and the human evaluation of the proposed model reveals some interesting discrepancies. While the LLM rated the model's relevance at 4.11, humans rated it slightly lower at 3.63. For generality, the difference is more pronounced: the LLM assessed a mean score of 3.29 with a standard deviation of 0.87, whereas human ratings reflected in a mean of 2.93 and a standard deviation of 1.40. In terms of coverage the LLM and humans rated the model with scores of 3.72 and 4.46, respectively. These findings suggest that the LLM may be more lenient in its evaluation of generality compared to humans, while showing similar performance in relevance and coverage assessment. It could also be the case that the datasets chosen for the human evaluation were different from the datasets used for the LLM evaluation, leading to discrepancies in the results.




% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/gpt_relevance_score_distribution.pdf}
%     \caption{Distribution of relevance scores}
%     \label{fig:gpt_relevance_score_distribution}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/gpt_generality_score_distribution.pdf}
%     \caption{Distribution of generality scores}
%     \label{fig:gpt_generality_score_distribution}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/gpt_coverage_score_distribution.pdf}
%     \caption{Distribution of coverage scores}
%     \label{fig:gpt_coverage_score_distribution}
% \end{figure}




% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/relevance_distribution.pdf}
%     \caption{Relevance score distribution}
%     \label{fig:relevance_distribution}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\textwidth]{figures/generality_distribution.pdf}
%     \caption{Generality score distribution}
%     \label{fig:generality_distribution}
% \end{figure}


\paragraph{Normality testing}
The Q-Q plots in \cref{fig:all_qq_plots} show the distribution of relevance, generality, and coverage scores, respectively. We see that the scores are not normally distributed, which is expected given the ordinal nature of the Likert scale data, and is similar to the human evaluation results.

\cref{tab:normality_comparison} shows the results of the normality tests for the relevance, generality, and coverage scores. We see that the Shapiro-Wilk test and Anderson-Darling test both indicate that the scores are not normally distributed.

\begin{table}[htbp]
    \centering
    \caption{Normality tests across metrics}
    \begin{tabular}{llll}
        \hline
        \textbf{Test} & \textbf{Relevance} & \textbf{Generality} & \textbf{Coverage} \\
        \hline
        Shapiro-Wilk Statistic & 0.8799 & 0.8799 & 0.5734 \\
        Shapiro-Wilk p-value & <0.0001 & <0.0001 & <0.0001 \\
        Anderson-Darling Statistic & 1905.4133 & 2641.2348 & 1154.0435 \\
        \hline
    \end{tabular}
    \label{tab:normality_comparison}
\end{table}

\cref{tab:distribution_comparison} shows the distribution metrics for the relevance, generality, and coverage scores. Looking at the mean, median, mode, standard deviation, skewness, skewness z-score, and kurtosis, we see that the scores are generally well-distributed, with a slight to moderate negative skewness and kurtosis. The negative skewness shows that the distribution is left-skewed, and the kurtosis values indicate that the distribution is platykurtic (less peaked than a normal distribution). These findings corroborate the Q-Q plots and normality test results.

\begin{table}[htbp]
    \centering
    \caption{Distribution metrics comparison}
    \begin{tabular}{llll}
        \hline
        \textbf{Metric} & \textbf{Relevance} & \textbf{Generality} & \textbf{Coverage} \\
        \hline
        Mean & 3.1651 & 3.2869 & 3.7210 \\
        Median & 3.0 & 3.0 & 4.0 \\
        Mode & 3 & 3 & 4 \\
        Standard Deviation & 0.8667 & 0.8705 & 0.4536 \\
        Skewness & -0.3014 & -0.3544 & -0.9979 \\
        Skewness z-score & -0.33 & -10.41 & -1.08 \\
        Kurtosis & -0.0148 & -0.0169 & -0.7863 \\
        \hline
    \end{tabular}
    \label{tab:distribution_comparison}
\end{table}



\section{Evaluation of alternative configurations}
In addition to the primary pipeline evaluation, we explored a more cost-effective configuration designed to reduce computational expenses, particularly regarding LLM usage, following the alternative configuration outlined in \cref{sec:alternative_configurations}. This alternative pipeline deviates from the original approach in three aspects:

\begin{enumerate}
    \item \textbf{Omission of Description Rewriting}: We eliminated the step of rewriting augmented descriptions into a more human-readable format (Step 3 in the original pipeline).
    \item \textbf{Simplified Tag Generation and Classification}: We combined the tag generation and classification steps into a single step (step 9), utilizing GPT-4o-mini with a maximum output of 256 tokens per description, and removing the zeroshot classifier entirely (step 10).
    \item \textbf{Choice of LLM}: We opted for GPT-4o-mini for the combined tag generation and classification step, as it offers a good balance between performance and cost compared to larger models \cite{noauthor_gpt-4o_nodate}. GPT-4o-mini was priced at \$0.15 per 1M tokens of input and \$0.60 per 1M tokens of output.
\end{enumerate}

This configuration significantly reduced the overall processing time. Embedding generation (step 4) took approximately 4 hours using the \textit{Salesforce/SFR-Embedding-2\_R} model \cite{noauthor_salesforcesfr-embedding-2_r_2024}. Omission of description rewriting reduced processing time by 3 hours and expenses by approximately \$8.00 compared to the proposed pipeline. Training the base BERTopic model (steps 5-8) required about 5 minutes. Substantial resource saving was also achieved in the fine-tuning step (step 9), which took approximately 2 hours and cost only \$0.80 using GPT-4o-mini, compared to using the Llama-3-70b model in the original pipeline, which took similar time but cost approximately \$4.80.

While this configuration offers significant cost and time advantages, our initial observations suggest a potential trade-off in tag quality compared to the original pipeline. The absence of description rewriting led to a degradation in the quality of the c-TF-IDF representations. Specifically, the c-TF-IDF output for this configuration contained a higher proportion of nonsensical or irrelevant terms (e.g., \textit{q59fi3}, \textit{timmenziesus}, \textit{145160}) compared to the original pipeline, which benefited from cleaner, human-readable descriptions. This difference in c-TF-IDF quality likely impacted the subsequent tag generation, resulting in some less relevant or nonsensical tags (e.g., \textit{Rows}, \textit{Columns}, \textit{Seed}, \textit{Intra-Class}).

The impact on clustering is more difficult to quantify objectively. However, subjective inspection revealed instances where unrelated descriptions were grouped into the same cluster. Additionally, we observed a tendency towards fragmentation, with similar descriptions being split into multiple smaller clusters instead of being grouped into a single, larger cluster. For example, instead of one large cluster for all QSAR-related datasets, there were many smaller clusters containing different subsets of QSAR datasets.

Using a cheaper LLM (GPT-4o-mini) for the combined tag generation and classification step introduced further challenges. The model tended to extract more specific tags (e.g., \textit{Cropped Images} instead of \textit{Images}) at the expense of more general, common tags. While the overarching tags remained largely unaffected, the model occasionally produced incorrectly formatted JSON outputs, requiring manual correction in roughly 10 out of 900 cases. Furthermore, the model sometimes failed to assign generated tags during the classification phase or introduced new tags not present in the initial generation step. The confidence scores produced by the LLM were also less reliable than those from the dedicated zeroshot classifier in the original pipeline, as the LLM is not specifically trained for this task. Finally, this configuration does not provide tags from the top \textit{n} clusters for each description, potentially missing out on relevant tags.

Despite these limitations, the tags produced by this cost-effective configuration are still generally of good quality. This configuration offers a viable alternative when computational resources or budget constraints are a primary concern. Further evaluation using a large-scale automated approach, similar to the one performed for the original pipeline, would provide a more comprehensive assessment of the trade-offs between cost and quality.

% \section{Chapter conclusion}
% In this chapter, we explained how we followed the methodology outlined in \cref{chapter:methodology} to evaluate the proposed model. We addressed each research question from \hyperref[rq1]{\textbf{RQ1}} through \hyperref[rq4]{\textbf{RQ4}}.

% We answered \hyperref[rq1]{\textbf{RQ1}} by performing an exploratory data analysis and data augmentation. For the exploratory data analysis, we analyzed the dataset descriptions, investigating traits such as length, readability, duplicate content, their tags, features, and versions. We then proceeded to augment the descriptions, giving them more context which would help the proposed model generate better tags.

% \hyperref[rq2]{\textbf{RQ2}} was answered by following the tag generation methodology in \cref{sec:tag_generation_pipeline}. We chose and gave rationale for the chosen submodels, and discovered a good set of hyperparameters for them.

% For \hyperref[rq3]{\textbf{RQ3}}, we evaluated the proposed model using the automated evaluation metrics we discussed in \cref{sec:automated_evaluation_methodology}. We compared the proposed model to a set of baseline models on several metrics, namely coherence (NPMI), diversity, and silhouette score. The baseline models were LDA, NMF, CTM, Top2Vec, and several BERTopic model configurations. We found that our BERTopic configurations outperformed the baseline models, with the exception of NMF for NPMI. However, NMF had very low diversity score due to picking the same terms, driving NPMI higher, and its diversity lower. Additionally, we performed Bayesian Optimization to find the best hyperparameters for our base BERTopic model.

% Finally, we answered \hyperref[rq4]{\textbf{RQ4}} by evaluating the proposed model using human evaluation and large-scale automated evaluation. For the human evaluation,  we picked a suitable baseline model, and created a human-generated set of tags. We conducted a two-stage evaluation, where we first asked participants to perform intruder detection, and evaluated the relevance, generality, coverage, and shared coverage of the tags generated by the proposed model, the baseline model and the human-generated tags. Then, participants evaluated the common tags identified by the proposed model, the baseline model and the human-generated tags. We found that the proposed model outperformed the baseline model in most metrics, while slightly lagging behind the human-generated tags. For the large-scale automated evaluation, we evaluated the proposed model using an LLM, \textit{GPT-4-mini}. We found that the LLM managed to detect the majority of intruders in the dataset, indicating that the tags generated by the proposed model are cohesive. We also found that the tags generated by the proposed model are generally relevant, somewhat general, with a decent standard deviation, and provide good coverage.



\section{Chapter conclusion}

In this chapter, we explained how we followed the methodology outlined in \cref{chapter:methodology} to evaluate the proposed model. We addressed each research question from \hyperref[rq1]{\textbf{RQ1}} through \hyperref[rq4]{\textbf{RQ4}}.

We answered \hyperref[rq1]{\textbf{RQ1}} by performing an exploratory data analysis and data augmentation. For the exploratory data analysis, we analyzed the 5200 dataset descriptions, investigating traits such as length, readability, duplicate content, their tags, features, and versions. Our findings revealed that descriptions are generally short, with a median length comparable to two tweets, and exhibit a wide range of readability scores, ranging from 0 to 100, with many falling in the 0-60 range on the Flesch Reading Ease scale, indicating varying complexity. We observed that a significant portion of datasets already contain associated tags, with most datasets having between 0 and 5 tags, and many descriptions include URLs to external resources. We augmented the descriptions with metadata (dataset name, existing tags, column names) and text scraped from external URLs. We also removed \~300 duplicate datasets with high cosine similarity. Additionally, we applied Named Entity Recognition (NER) and Part-of-Speech (POS) tagging, which revealed challenges posed by domain-specific terms and author names for generating relevant tags.

\hyperref[rq2]{\textbf{RQ2}} was answered by following the tag generation methodology in \cref{sec:tag_generation_pipeline}. We detailed the implementation of our tag generation pipeline, extending the Base BERTopic model with an LLM (Llama-3-70b) for prompt-based tag generation and a zeroshot classifier (MoritzLaurer/deberta-v3-large-zeroshot-v2.0) for tag filtering. We provided a step-by-step description of the pipeline, including the rationale for selecting specific submodels. We highlighted the advantages of this approach over naive LLM-based methods, particularly in terms of contextual consistency, multi-level tag generation, and computational efficiency. The main limitation of the proposed tag generation pipeline is the computational cost of the Llama-3-70b model. We also explored a more cost-effective pipeline configuration, omitting the description rewriting step, using a cheaper LLM (GPT-4o-mini), and combining tag generation and classification into a single step. While this configuration reduced computational costs and time, it led to a decrease in tag quality, as observed through the degradation of c-TF-IDF representations and some clustering issues.

For \hyperref[rq3]{\textbf{RQ3}}, we evaluated the Base BERTopic model using the automated evaluation metrics we discussed in \cref{sec:automated_evaluation_methodology}. We compared the hyperparameter-optimized BERTopic model against several baselines, including LDA, NMF, CTM, Top2Vec, and different BERTopic configurations. Our results showed that the optimized BERTopic model achieved the highest combined NPMI and diversity score of 0.779, outperforming all baseline models on this combined metric. NMF achieved the highest NPMI, but had very low diversity. The optimized BERTopic model had the highest diversity (0.808). We performed hyperparameter tuning using Bayesian optimization with a custom weighted metric combining NPMI and diversity. The optimized hyperparameters included a \textit{min\_topic\_size} of 2, \textit{umap\_n\_neighbors} of 3, \textit{umap\_n\_components} of 5, \textit{hdbscan\_min\_cluster\_size} of 3, along with specific settings for other parameters detailed in \cref{tab:bertopic_parameters}. Statistical analysis using ANOVA, Kruskal-Wallis, and post-hoc tests confirmed significant differences between models for NPMI, diversity, with large effect sizes, highlighting the strong performance of our optimized BERTopic model and the BERTopic family as a whole.

Finally, we answered \hyperref[rq4]{\textbf{RQ4}} by evaluating the proposed model using human evaluation and large-scale automated evaluation. For the human evaluation, we picked a suitable baseline model (Base BERTopic), and created a human-generated set of tags. We conducted a two-stage evaluation, where we first asked participants to perform intruder detection, and evaluated the relevance, generality, coverage, and shared coverage of the tags generated by the proposed model, the baseline model and the human-generated tags. Our model achieved a mean relevance score of 3.63, outperforming the baseline (2.39) and approaching the performance of human-generated tags (3.95). For generality, our model showed a better balance between specific and general tags (SD = 1.40) compared to the baseline (SD = 1.33) and was close to human-generated tags (SD = 1.41). In terms of coverage, our model (4.46) outperformed the baseline (2.65) and was close to human-generated tags (4.54). The intruder detection task demonstrated our model's ability to generate cohesive tag sets, with a 95.2\% detection rate, compared to 42.1\% for the baseline and 100\% for human-generated tags. For inter-rater agreement, we calculated Fleiss' Kappa, Interclass Correlation Coefficient (ICC), and Krippendorff's Alpha. While Fleiss' Kappa indicated only slight agreement at best (all values below 0.18), ICC and Krippendorff's Alpha showed more nuanced results, with some metrics indicating moderate to strong agreement (e.g., ICC(3,k) values of 0.95 for relevance for the proposed model, and 0.97 for generality, and Krippendorff's Alpha of 0.5647 for coverage). However, there was also disagreement between the metrics, suggesting complexity in assessing inter-rater reliability for this task. 

The common tag identification task revealed that our model achieved balanced precision and recall across two text pairs, with an accuracy of 0.87 and 0.76 respectively, which was higher than the baseline (0.69 and 0.60), slightly higher than the human-generated tags for the first pair (0.83), but slightly lower than human-generated tags for the second pair (0.86). Statistical tests, including Kruskal-Wallis and Dunn's post-hoc test, showed statistically significant differences between our model and the baseline for all metrics. While our model's tags were rated significantly lower in relevance compared to human-generated tags, there were no significant differences in generality, coverage, and shared coverage. Effect size analysis using Cliff's Delta confirmed large differences between the baseline and both our model and human-generated tags for relevance, coverage, and shared coverage. However, the effect sizes between our model and human-generated tags were small to negligible, indicating that our model's performance was comparable to human-generated tags.

For the large-scale automated evaluation, we evaluated the proposed model using an LLM, \textit{GPT-4-mini}. We found that the LLM managed to detect the majority of intruders in the dataset (88.3\%), indicating that the tags generated by the proposed model are cohesive. Tag quality assessment by the LLM yielded a mean relevance score of 4.11, a mean generality score of 3.29 (SD = 0.87), and a mean coverage score of 3.72. The LLM found that keyword tags were the most relevant, and it found them to be slightly more general than regular tags. The automated evaluation also found a weakly positive (near-neutral) correlation between relevance and generality, which differed from the negative correlation found in the human evaluation. The discrepancy in correlation may be attributed to differences in how humans and LLMs perceive these qualities, or to the different dataset samples used in the two evaluations.

Our results demonstrate that the proposed model generates cohesive, relevant and diverse tags for OpenML dataset descriptions with a good coverage, outperforming the baseline model and approaching human-level performance in several aspects.