In this chapter, we present our proposed methodology, including the methods and techniques used to address our research questions. We also cover our data exploration and preprocessing steps, the hyperparameter tuning process, and the automated and human evaluation methods and metrics we have chosen.

\section{Data exploration and preprocessing}
\subsection{Exploratory data analysis}
First, an exploratory data analysis (EDA) will be conducted to understand the characteristics of the OpenML dataset descriptions. This analysis will be unstructured, discovering patterns in the data in an exploratory manner, using statistical and visual methods. The goal of the EDA is to gain insights into the dataset descriptions, such as their length, vocabulary, distribution of words, etc. This information will help us understand the nature of the data and identify any preprocessing steps that may be necessary to improve the quality of the descriptions.

\subsection{Data preprocessing}
After the EDA, the dataset descriptions will undergo preprocessing to prepare them for the topic modeling process. This preprocessing may involve steps such as stemming, lemmatization, stop-word removal, and tokenization. These steps are important to ensure that the descriptions are in a suitable format for the topic modeling algorithms.

\subsection{Data augmentation}
In addition to the original dataset descriptions, we will explore the possibility of augmenting the data with additional information. This could include metadata such as dataset name, tags, features (column names), scraping from the original dataset if available, etc. This additional information can provide context and background to the descriptions, which may help improve the quality of the topics extracted by the topic model.

\section{Tag generation}
In this subsection, we introduce our model, which is, in fact, a pipeline comprising multiple submodels and techniques. Steps 1-3 involve data preprocessing to prepare the data for the model. Steps 5-8 are referred to as the \textit{Base BERTopic model}. It includes dimensionality reduction, clustering, bag-of-words and c-TF-IDF. Finally, steps 9 and 10 represent our improvements to the base model, introducing an approach that, to the best of our knowledge, is novel within the literature. Additionally, we provide an explanation of which steps are computationally efficient and which are more expensive.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/tag_generation_pipeline.pdf}
    \caption{Tag generation pipeline}
    \label{fig:tag_generation_pipeline}
\end{figure}

To explain the pipeline illustrated in \cref{fig:tag_generation_pipeline} in more detail, we provide a step-by-step description of the process:

\begin{enumerate}
    \item \textbf{Original Descriptions}: The input to the pipeline is a set of original dataset descriptions. These come from the OpenML dataset and are used as the basis for generating tags.
    \item \textbf{Augmented Descriptions}: The OpenML dataset descriptions come with metadata such as dataset name, tags, features (column names) and some of them link to the original dataset, which can be used to scrape (extract) additional information. These are used to augment the dataset descriptions.
    \item \textbf{Prompt Descriptions Human-Readable}: Augmented descriptions are rewritten to be more human-readable via an LLM and prompt engineering. This is because the original descriptions are often in a technical format that is not easily interpretable by humans. Since LLMs are trained on large text corpora, they work best with human-readable (natural language) text.
    \item \textbf{Create Embeddings}: Embeddings for each description are created using a pre-trained embedding model.
    \item \textbf{Dimensionality Reduction}: The dimensionality of the embeddings is reduced to cure the curse of dimensionality.
    \item \textbf{Clustering}: The reduced embeddings are clustered to group similar descriptions together. The output of this step is clusters, which represent our topics. Each cluster contains a set of descriptions (which we now call \textit{representative documents}) that are similar to each other.
    \item \textbf{Bag-of-words}: The descriptions in each cluster are converted to a bag-of-words representation, ignoring common words such as "the", "and", etc.
    \item \textbf{c-TF-IDF}: The bag-of-words representation is used to calculate the c-TF-IDF score for each word in each cluster. This score is used to rank the words in each cluster.
    \item \textbf{Fine-tune to extract tags}: For each topic, we have representative documents (from the clustering step) and representative words (from the c-TF-IDF step). We prompt engineer a question that asks an LLM to generate tags for each cluster. As context, we feed the LLM with the top \textit{k} representative documents and the top \textit{m} representative words for each topic. This results in tags that are common among the representative documents and representative words, and hence are representative of the topic.
    \item \textbf{Zeroshot Classifier}: Each description can in reality be contained in multiple clusters (topics). In this step, we get the top \textit{n} most likely clusters for each description. Then, for each description, we get the tags for each of the top \textit{n} clusters in a set. We feed this set of tags to a zeroshot text classification model, which returns confidences from 0 to 1 for whether each tag describes the description. This step is crucial for filtering out irrelevant tags for each description. For instance, if a description about diabetes and a description about cancer are both contained in the same topic (which may be, for example, "medical conditions"), we want to ensure that the tags for the cancer description are not assigned to the diabetes description. Furthermore, the description about diabetes may be contained in another topic that cancer is not in, such as "nutrition". In this case, we want to ensure that the tags in "nutrition" are not assigned to the cancer description, but are assigned to the diabetes description.

\end{enumerate}


\section{Automated evaluation metrics and baselines}
In order to evaluate the quality of the extracted topics, we will use a combination of automated evaluation metrics and baselines. These metrics and baselines will help us assess the performance of our model and compare it to existing topic modeling techniques. The following sections provide an overview of the metrics and baselines we will use in our evaluation.

\subsection{Metrics}
\subsubsection{Topic coherence}
As explained in \cref{sec:topic_coherence}, topic coherence is a widely used metric for evaluating the quality of topics generated by topic models. It measures the semantic similarity between words in a topic and is based on the assumption that coherent topics contain words that are related to each other. We will use topic coherence to assess the quality of the topics extracted by our model.

\subsubsection{Topic diversity}
Topic diversity (\cref{sec:topic_diversity}) is another important metric for evaluating topic models. It measures the extent to which topics are distinct from each other and do not overlap in terms of the words they contain. A diverse set of topics ensures that the model captures a wide range of themes and concepts present in the data. We will use topic diversity to evaluate the diversity of topics generated by our model.

\subsubsection{Silhouette score}
The silhouette score (\cref{sec:silhouette_score}) is a metric used to evaluate the quality of clusters in unsupervised learning. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). A high silhouette score indicates that the clusters are well-separated and that the objects within each cluster are similar to each other. We will use the silhouette score to evaluate the quality of the clusters generated by our model.

\subsection{Baselines}
In addition to the proposed topic model, we will compare its performance against several baseline models. These baselines represent established or commonly used topic modeling techniques and will serve as a point of reference for evaluating the proposed model. The baselines we will consider include Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and Top2Vec (described in \cref{sec:latent_dirichlet_allocation,sec:non-negative_matrix_factorization,sec:top2vec}). These models are widely used in the field of topic modeling and will provide a benchmark for evaluating the performance of our model.

\todo{Write about baselines blabla}

\subsection{Automated evaluation pipeline}

\Cref{fig:data_pipeline} shows a flowchart of the automated evaluation pipeline, which contains the following sequential steps:

\begin{enumerate}

    \item \textbf{Data Fetching}: This is the first stage where dataset descriptions are downloaded from OpenML.

    \item \textbf{Data Preparation}: After fetching the data, the next step involves preparing it. This includes removing noise, correcting errors, augmenting it, and standardizing the format to prepare it for analysis. Data preparation ensures that the input to the topic model is of high quality, which is crucial for the success of the subsequent modeling steps. \\ The next step involves removing inadequate data points, such as excessively short descriptions and duplicates. Stop words are removed for models that require it (e.g., LDA). Additionally, the process includes stemming and lemmatization to normalize words to their base forms.

    \item \textbf{Topic Model}: In this step, the proposed topic model is applied to the prepared data. In this case, it will be BERTopic.

    \item \textbf{Benchmark Models}: Concurrently with the proposed topic model, benchmark models are run. These models represent established or baseline approaches to topic modeling against which the performance of the proposed topic model is compared. This will involve baseline models such as LDA, NMF and Top2Vec.

    \item \textbf{Topic Labels}: The output from both the topic model and the benchmark models are sets of topics, represented by a cluster of words that are characteristic of a particular topic.

    \item \textbf{Evaluation}: Finally, the performances of the proposed topic model and benchmark models are evaluated. This includes comparing the topic coherence and diversity.

\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/data_pipeline.pdf}
    \caption{Data pipeline}
    \label{fig:data_pipeline}
\end{figure}

\subsection{Hyperparameter tuning}
\todo{Bayesian optimization...Refer back to \cref{fig:octis}}
as these were optimized in the previous section on hyperparameter tuning and Bayesian optimization

\section{Human evaluation}
After designing our pipeline, we will perform a human evaluation to assess the quality of the tags produced by our model. As previously discussed, automated evaluation metrics offer only a limited perspective on the quality of the generated tags. Human evaluation is crucial for providing a more comprehensive assessment. To this end, we will carry out a user study in which participants will evaluate the quality of the tags generated by our model.

\subsection{Experimental design}
\subsubsection{Participants}
We will begin by recruiting participants for the user study. Given that participants will be selected based on accessibility, this will constitute a convenience sample. Colleagues, friends, and acquaintances will be invited to participate. Despite the use of a convenience sample, we will aim to recruit individuals whose backgrounds align with those of OpenML's target users. Specifically, we will seek participants with expertise in data science, computer science, or, at a minimum, individuals with a bachelor's degree and a high proficiency in English.

\subsubsection{Procedure}
\textbf{This section should describe how the experiment will be conducted. You could detail the steps participants will go through, such as how the tags will be presented to them, the instructions they will receive, and the criteria they are expected to evaluate.}

Participants will be presented with a set of items, each accompanied by tags generated by our model. They will be asked to evaluate the quality of the tags based on various criteria such as relevance, specificity, and clarity. Each tag will be rated on a Likert scale from 1 (poor quality) to 5 (excellent quality). Participants will also be given the option to provide qualitative feedback on the tags if they wish. The study will be conducted online, and participants will be given an estimated time to complete the evaluation.

\subsubsection{Materials}
\textbf{This section outlines the tools, software, or datasets used in the study. In your case, this could include the interface for evaluating the tags, the source data, and the tags generated by the model.}

The evaluation interface will be a web-based platform, designed to present participants with items and their corresponding tags. The tags are generated by our model trained on the OpenML dataset. Each participant will be provided with a set of 20 items, with each item showing the tags generated by the model. The study will also include a brief tutorial on how to use the interface and what aspects of tag quality to focus on during evaluation.

\subsubsection{Evaluation Criteria}
\textbf{This section defines the metrics or criteria that participants will use to evaluate the tags. This is crucial to ensure that the assessment is structured and focused on specific aspects of the tags' quality.}

Participants will evaluate the tags based on the following criteria:
\begin{itemize}
    \item \textbf{Relevance}: Do the tags accurately describe the content of the item?
    \item \textbf{Specificity}: Are the tags specific enough to distinguish between similar items?
    \item \textbf{Clarity}: Are the tags easy to understand and unambiguous?
    \item \textbf{Usefulness}: Would these tags help a user searching for similar content?
\end{itemize}
Each criterion will be rated on a 5-point Likert scale, with space for additional comments.

\subsubsection{Data Collection}
During the evaluation, participants' ratings and feedback will be automatically recorded through the web interface. All data will be anonymized and stored in a secure database. No personally identifiable information will be collected, and participants will be assigned a unique ID to keep their responses anonymous. The collected data will include numerical ratings for each criterion as well as any optional qualitative feedback provided by participants.

\subsubsection{Analysis Plan}
The quantitative data from the Likert-scale ratings will be analyzed using descriptive statistics, such as means and standard deviations for each criterion. Additionally, we will conduct statistical tests (e.g., a t-test or ANOVA) to determine if there are significant differences in tag quality across different items. Qualitative feedback will be analyzed using thematic analysis to identify common themes in participants' perceptions of tag quality. This will help us understand not only the numerical ratings but also the reasons behind participants' judgments.

\subsubsection{Ethical Considerations}
All participants will be required to provide informed consent before taking part in the study. They will be informed about the purpose of the study, what their participation entails, and how their data will be used. Participants will have the right to withdraw from the study at any time without any consequences. All data collected will be anonymized, and no personally identifiable information will be stored. The study protocol has been reviewed and approved by the appropriate ethics review board.

\subsubsection{Limitations}
\textbf{This section acknowledges any potential limitations of your experimental design, such as the use of a convenience sample or the limited generalizability of the results.}

One limitation of this study is the use of a convenience sample, which may not be fully representative of the target user base of OpenML. While we aim to recruit participants with relevant backgrounds, the results may still be subject to selection bias. Additionally, the subjective nature of human evaluation means that individual biases and preferences could affect the results. To mitigate this, we will aim to recruit a diverse group of participants and analyze the data for any potential biases.

\subsubsection{Hypotheses}
We hypothesize that the tags generated by our model will score highly on relevance and clarity but may need improvement in specificity. We also expect that participants with a stronger background in data science will provide more critical feedback compared to those with less technical expertise.

\section{Chapter summary}