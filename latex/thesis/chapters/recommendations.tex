\section{Recommendations}
1. Keep looking for improved submodels — embedding model and fine-tuning model and zeroshot model especially. These models can be tested with the automated evaluation metrics and with the GPT evaluation quickly, quick iterations.

2. Look into training a custom zeroshot model with a better base architecture — this could be a separate Master's project on its own. Look into Moritz Laurer's work on zeroshot learning. Or, more realistically, use a pre-trained LLM to do zeroshot text classification. It will not return scores, but it will still perform better than zeroshot text classification models, as zeroshot models are not trained very frequently, and are small.

3. Additionally, word intruder tests could be used to evaluate whether the proposed model performs better on popular datasets than the current state of the art. This could be a good way to evaluate the model's generalizability.

4. Keep track of which tags are created by the model and which are human-created. This would let you easily update the tags in the future, when the whole model is rerun.

5. Research iterative/dynamic clustering algorithms (HDScan drawbacks)


Throughout this research, we identified several promising directions for future work and improvement of the tag generation system. These recommendations range from immediate, practical enhancements to longer-term research opportunities.

First, the modular nature of our pipeline presents ongoing opportunities for improvement through the integration of newer, better-performing submodels. The rapid advancement in language models and embedding techniques means that regularly evaluating and incorporating new submodels could yield significant improvements. Particularly promising areas include embedding models achieving high scores on the MTEB benchmark \cite{muennighoff_mteb_2023}, newer LLMs for fine-tuning, and improved zeroshot classifiers. Our automated evaluation pipeline allows for quick testing of these components, making continuous improvement practical and efficient.

Second, while the current zeroshot classifier performs adequately, there is potential for improvement in this area. Training a custom zeroshot model with a better base architecture could be an entire Master's project in itself, given the complexity and resource requirements involved. The zeroshot classifier in this thesis is based on DeBERTa \cite{he_deberta_2021}, a relatively small and old model. A more practical short-term solution would be to use a pre-trained LLM for zeroshot text classification. Dedicated zeroshot models are usually smaller and trained less frequently because they are highly specialized for one task. As a result, there is less demand for them compared to general-purpose LLMs, which tend to perform better across a wider range of tasks, including text classification (for our experiments demonstrating this, refer to the \textit{preliminary\_model.ipynb} notebook in the \href{https://github.com/ivangermanov/openml-tags}{GitHub repository} \cite{germanov_topic_modeling_of_2024} ). Even though the LLM would not provide confidence scores, it would still likely outperform the current zeroshot model. For long-term development, following \citet{laurer_building_2024}'s work on training zeroshot text classifiers could provide a valuable starting point for training a custom model.

Third, to validate the broader applicability of our approach, we recommend conducting word intruder tests on popular datasets beyond OpenML. This evaluation would help assess the model's generalizability and identify potential areas for improvement when handling different types of data. Furthermore, comparing results against current state-of-the-art topic models would provide valuable insights into the strengths and limitations of our approach. To start this process, we recommend finding the relevant literature in which word intruder tests are used to evaluate topic models and adapting these methods to our context \cite{chang_reading_2009,lau_machine_2014,hoyle_is_2021,newman_evaluating_2010,mimno_optimizing_nodate,musil_exploring_2024,bhatia_automatic_2017,lim_large-scale_2023}.

Fourth, for the OpenML platform, we recommend keeping track of which tags are generated by the model and which are human-generated. This would allow for easy updates to the tags in the future when the entire model is rerun, without affecting the human-generated tags.

Finally, we recommend exploring iterative (online) dimensionality reduction and, especially, clustering algorithms to address the limitations of the current static approaches. To take the example of clustering, hierarchical clustering algorithms such as HDBSCAN have been shown to be effective in many applications, but they are static. Static clustering has limitations in that they are run once for a fixed set of data and do not adapt to size changes in the dataset. Recalculating the clustering from scratch for each new dataset is not computationally expensive for our dataset of ~5000 descriptions, but could become an issue as the dataset grows. For instance, HDBScan's time complexity is $O(n^2)$ for computing the distance matrix, which could become a bottleneck for larger datasets. Iterative clustering algorithms, on the other hand, can adapt to changes in the dataset without having to recalculate the clusters from scratch. Researching and implementing iterative dimensionality reduction algorithms such as Incremental PCA \cite{balsubramani_fast_2013,artac_incremental_2002,dagher_incremental_2010}, and iterative clustering algorithms \cite{montiel_online_2022} such as Mini Batch K-means \cite{bejar_alonso_k-means_2013,hicks_mbkmeans_2021} or BIRCH \cite{zhang_birch_1996} could provide a more scalable and efficient solution for the long-term.